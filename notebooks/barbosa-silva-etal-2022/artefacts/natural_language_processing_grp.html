<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.8.3.min.js"></script>                <div id="36d7752d-d09d-4cbf-8feb-aaba7beefea1" class="plotly-graph-div" style="height:700.0px; width:1500px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("36d7752d-d09d-4cbf-8feb-aaba7beefea1")) {                    Plotly.newPlot(                        "36d7752d-d09d-4cbf-8feb-aaba7beefea1",                        [{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Computer code processing","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Computer code processing","orientation":"v","showlegend":true,"x":["2017-08","2018-03","2018-04","2018-10","2019-10"],"xaxis":"x","y":["Computer code processing","Computer code processing","Computer code processing","Computer code processing","Computer code processing"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Dialog process","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Dialog process","orientation":"v","showlegend":true,"x":["2017-09","2017-11","2018-05","2018-09","2018-10","2019-02","2019-04"],"xaxis":"x","y":["Dialog process","Dialog process","Dialog process","Dialog process","Dialog process","Dialog process","Dialog process"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Inference and reasoning","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Inference and reasoning","orientation":"v","showlegend":true,"x":["2014-08","2015-08","2016-09","2017-02","2017-09","2017-11","2017-12","2018-04","2018-05","2018-06","2018-09","2018-10","2019-01","2019-02","2019-06","2019-07","2019-09"],"xaxis":"x","y":["Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Information extraction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Information extraction","orientation":"v","showlegend":true,"x":["2016-01","2017-07","2017-09","2018-07","2018-08","2018-09","2018-10","2018-12","2019-01","2019-02","2019-03","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2019-11","2020-03","2020-04"],"xaxis":"x","y":["Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Information retrieval","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Information retrieval","orientation":"v","showlegend":true,"x":["2018-03","2019-01","2019-04","2019-11"],"xaxis":"x","y":["Information retrieval","Information retrieval","Information retrieval","Information retrieval"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Machine translation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Machine translation","orientation":"v","showlegend":true,"x":["2018-10","2019-01","2019-02","2019-05"],"xaxis":"x","y":["Machine translation","Machine translation","Machine translation","Machine translation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Natural language generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Natural language generation","orientation":"v","showlegend":true,"x":["2014-09","2014-10","2014-12","2015-09","2016-02","2016-03","2016-06","2016-07","2016-08","2016-09","2016-10","2016-11","2016-12","2017-01","2017-05","2017-06","2017-09","2017-11","2018-02","2018-03","2018-06","2018-07","2018-08","2018-09","2018-10","2019-01","2019-03","2019-04","2019-05","2019-06","2019-08","2019-09","2019-10"],"xaxis":"x","y":["Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Text-to-image Generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Text-to-image Generation","orientation":"v","showlegend":true,"x":["2016-12","2017-10","2017-11","2019-01","2019-03","2019-04","2019-09"],"xaxis":"x","y":["Text-to-image Generation","Text-to-image Generation","Text-to-image Generation","Text-to-image Generation","Text-to-image Generation","Text-to-image Generation","Text-to-image Generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pragmatics analysis","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pragmatics analysis","orientation":"v","showlegend":true,"x":["2013-10","2014-06","2014-08","2015-02","2015-06","2015-11","2016-02","2016-06","2016-07","2016-09","2017-02","2017-04","2017-07","2017-08","2017-09","2017-12","2018-01","2018-02","2018-04","2018-05","2018-06","2018-07","2018-10","2018-11","2019-01","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2019-12"],"xaxis":"x","y":["Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Question answering","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Question answering","orientation":"v","showlegend":true,"x":["2014-06","2014-12","2015-06","2015-11","2016-02","2016-03","2016-05","2016-06","2016-08","2016-09","2016-10","2016-11","2016-12","2017-03","2017-04","2017-05","2017-06","2017-07","2017-08","2017-10","2017-12","2018-01","2018-03","2018-04","2018-05","2018-06","2018-07","2018-08","2018-09","2018-10","2018-11","2019-01","2019-02","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2020-02"],"xaxis":"x","y":["Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic analysis","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic analysis","orientation":"v","showlegend":true,"x":["2016-03","2016-06","2017-04","2017-05","2017-09","2017-12","2018-02","2018-03","2018-05","2018-10","2018-11","2019-05","2019-06","2019-07","2019-09"],"xaxis":"x","y":["Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Sentence embedding","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Sentence embedding","orientation":"v","showlegend":true,"x":["2017-07","2018-07"],"xaxis":"x","y":["Sentence embedding","Sentence embedding"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Syntactic analysis","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Syntactic analysis","orientation":"v","showlegend":true,"x":["2016-03","2016-07","2016-11","2017-04","2017-07","2018-05","2018-07","2018-08","2018-10","2018-11","2019-03","2019-04","2019-06","2019-09"],"xaxis":"x","y":["Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Text classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Text classification","orientation":"v","showlegend":true,"x":["2015-11","2016-03","2016-06","2016-09","2016-11","2017-02","2017-10","2018-01","2018-02","2018-03","2018-05","2018-07","2018-08","2018-09","2018-10","2019-01","2019-02","2019-03","2019-04","2019-05","2019-06","2019-08","2019-09","2020-02"],"xaxis":"x","y":["Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":["<BR>task: Computer code processing<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: Django - Code Generation benchmarking - Accuracy<BR>","<BR>task: Computer code processing<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: 100 sleep nights of 8 caregivers - Code Generation benchmarking - 14 gestures accuracy<BR>","<BR>task: Computer code processing<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Computer code processing<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: Android Repos - Code Generation benchmarking - Perplexity<BR>","<BR>task: Computer code processing<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: CoNaLa - Code Generation benchmarking - BLEU<BR>  Code Generation: CoNaLa-Ext - Code Generation benchmarking - BLEU<BR>","<BR>task: Dialog process<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>","<BR>task: Dialog process<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - Mean Rank<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Dialog process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Area<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Food<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Price<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Request<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Request<BR>","<BR>task: Dialog process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - NDCG (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Inference and reasoning<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Train Accuracy<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - Parameters<BR>","<BR>task: Inference and reasoning<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>","<BR>task: Inference and reasoning<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Event2Mind dev - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>  Common Sense Reasoning: Event2Mind test - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>","<BR>task: Inference and reasoning<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Winograd Schema Challenge - Common Sense Reasoning benchmarking - Score<BR>  Natural Language Inference: V-SNLI - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Dev<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>","<BR>task: Inference and reasoning<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Visual Dialog v0.9 - Common Sense Reasoning benchmarking - 1 in 10 R-at-5<BR>  Natural Language Inference: XNLI French - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: ReCoRD - Common Sense Reasoning benchmarking - EM<BR>  Common Sense Reasoning: ReCoRD - Common Sense Reasoning benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: Quora Question Pairs - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: XNLI Chinese - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese Dev - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A1<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A2<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A3<BR>  Natural Language Inference: WNLI - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Visual Dialog  v0.9 - Common Sense Reasoning benchmarking - 1 in 10 R-at-5<BR>  Common Sense Reasoning: Visual Dialog  v0.9 - Common Sense Reasoning benchmarking - Recall-at-10<BR>","<BR>task: Information extraction<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Information extraction<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Information extraction<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-10%<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-30%<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1 (surface form)<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: Wikipedia-Wikidata relations - Relation Extraction benchmarking - Error rate<BR>","<BR>task: Information extraction<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Macro F1<BR>","<BR>task: Information extraction<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: OntoNotes 4 - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Resume NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Weibo NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: CoNLL 2000 - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>","<BR>task: Information extraction<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: SighanNER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: NCBI-disease - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: JNLPBA - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ChemProt - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: JNLPBA - Relation Extraction benchmarking - F1<BR>  Relation Extraction: SciERC - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA Dev - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: WLPC - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: WetLab - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: WLPC - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - F1<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - Precision<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - Recall<BR>","<BR>task: Information extraction<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>","<BR>task: Information extraction<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: LINNAEUS - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: Species-800 - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: Code-Switching English-Spanish NER - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ontontoes chinese v5 - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT24 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT29 - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - Precision<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - Recall<BR>","<BR>task: Information retrieval<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Information retrieval<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Information retrieval<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R-at-10<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R-at-1<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R@50<BR>","<BR>task: Information retrieval<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI AmazonQA - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI OpenSubtitles - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Machine translation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Machine translation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - PPL<BR>","<BR>task: Natural language generation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: 20NEWS - Machine Translation benchmarking - Accuracy<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2015 English-Russian - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Natural language generation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>  Language Modelling: Text8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>","<BR>task: Natural language generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 Thai-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Czech-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Czech - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Russian - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Romanian-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Russian-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: Hutter Prize - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Language Modelling: enwik8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: Chinese Poems - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-3<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-4<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>","<BR>task: Natural language generation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Test perplexity<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Validation perplexity<BR>","<BR>task: Natural language generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Test perplexity<BR>","<BR>task: Natural language generation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - KL<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - NLL<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>","<BR>task: Natural language generation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>  Text Summarization: Pubmed - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: arXiv - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Natural language generation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>","<BR>task: Natural language generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - PPL<BR>  Machine Translation: 20NEWS - Machine Translation benchmarking - 1-of-100 Accuracy<BR>  Machine Translation: WMT 2017 Latvian-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT 2017 English-Chinese - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: ACCURAT balanced test corpus for under resourced languages Estonian-Russian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: ACCURAT balanced test corpus for under resourced languages Russian-Estonian - Machine Translation benchmarking - BLEU<BR>  Text Generation: LDC2016E25 - Text Generation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: LAMBADA - Language Modelling benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - SacreBLEU<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - SacreBLEU<BR>  Question Generation: Visual Question Generation - Question Generation benchmarking - BLEU-1<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-1<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-2<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-3<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-4<BR>","<BR>task: Natural language generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT2014 French-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT 2017 English-Latvian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 English-Estonian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 English-Finnish - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 Estonian-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 Finnish-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-Czech - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: The Pile - Language Modelling benchmarking - Bits per byte<BR>","<BR>task: Natural language generation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2016 Finnish-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2017 Finnish-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2019 Finnish-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2019 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2019 English-German - Machine Translation benchmarking - SacreBLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-3<BR>","<BR>task: Natural language generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2017 Arabic-English - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 English-Arabic - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 English-French - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 French-English - Machine Translation benchmarking - Cased sacreBLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: PTB - Language Modelling benchmarking - PPL<BR>  Machine Translation: IWSLT2015 Chinese-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Text-to-image Generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Text-to-image Generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Text-to-image Generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - FID<BR>","<BR>task: Text-to-image Generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - SOA-C<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Acc<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - LPIPS<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Real<BR>","<BR>task: Pragmatics analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Average<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Yelp Binary classification - Sentiment Analysis benchmarking - Error<BR>  Sentiment Analysis: Yelp Fine-grained classification - Sentiment Analysis benchmarking - Error<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Amazon Review Full - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Amazon Review Polarity - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Sogou News - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: SemEval - Sentiment Analysis benchmarking - F1-score<BR>  Sentiment Analysis: SemEval 2017 Task 4-A - Sentiment Analysis benchmarking - Average Recall<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Arousal)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Discuss)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Unrelated)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Weighted Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: CR - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: 2017_test set - Paraphrase Identification benchmarking - 10 fold Cross validation<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Intent Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Slot F1 Score<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Twitter - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - F1<BR>  Sentiment Analysis: ChnSentiCorp - Sentiment Analysis benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Bias (F/M)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Feminine F1 (F)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Masculine F1 (M)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Overall F1<BR>  Sentiment Analysis: ASTD - Sentiment Analysis benchmarking - Average Recall<BR>  Sentiment Analysis: ArSAS - Sentiment Analysis benchmarking - Average Recall<BR>  Sentiment Analysis: FiQA - Sentiment Analysis benchmarking - MSE<BR>  Sentiment Analysis: FiQA - Sentiment Analysis benchmarking - R^2<BR>  Sentiment Analysis: Financial PhraseBank - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Financial PhraseBank - Sentiment Analysis benchmarking - F1 score<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: DailyDialog - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>  Emotion Recognition in Conversation: EmoryNLP - Emotion Recognition in Conversation benchmarking - Weighted Macro-F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ASOS.com user intent - Intent Detection benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Reverb - Question Answering benchmarking - Accuracy<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>","<BR>task: Question answering<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>","<BR>task: Question answering<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 10k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 1k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Mean Error Rate<BR>","<BR>task: Question answering<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: SimpleQuestions - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>","<BR>task: Question answering<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Question Answering: MCTest-160 - Question Answering benchmarking - Accuracy<BR>  Question Answering: MCTest-500 - Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Story Cloze Test - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>","<BR>task: Question answering<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>","<BR>task: Question answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Question answering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>  Question Answering: Quasart-T - Question Answering benchmarking - EM<BR>","<BR>task: Question answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: COMPLEXQUESTIONS - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>","<BR>task: Question answering<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: AI2 Kaggle Dataset - Question Answering benchmarking - P-at-1<BR>","<BR>task: Question answering<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>","<BR>task: Question answering<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>","<BR>task: Question answering<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Quora Question Pairs - Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: CLEVR - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Question answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: QuAC - Question Answering benchmarking - F1<BR>  Question Answering: QuAC - Question Answering benchmarking - HEQD<BR>  Question Answering: QuAC - Question Answering benchmarking - HEQQ<BR>  Visual Question Answering: 100 sleep nights of 8 caregivers - Visual Question Answering benchmarking - 14 gestures accuracy<BR>  Visual Question Answering: HowmanyQA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TallyQA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: JD Product Question Answer - Question Answering benchmarking - BLEU<BR>  Question Answering: Natural Questions - Question Answering benchmarking - F1 (Long)<BR>  Question Answering: Natural Questions - Question Answering benchmarking - F1 (Short)<BR>","<BR>task: Question answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TDIUC - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CODAH - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>","<BR>task: Question answering<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: HotpotQA - Question Answering benchmarking - JOINT-F1<BR>","<BR>task: Question answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: NaturalQA - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - number<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - other<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - unanswerable<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - yes/no<BR>","<BR>task: Question answering<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SCDE - Question Answering benchmarking - BA<BR>  Question Answering: SCDE - Question Answering benchmarking - DE<BR>  Question Answering: SCDE - Question Answering benchmarking - PA<BR>","<BR>task: Semantic analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: TAC2010 - Entity Disambiguation benchmarking - Micro Precision<BR>","<BR>task: Semantic analysis<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: ACE2004 - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: MSNBC - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-CWEB - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-WIKI - Entity Disambiguation benchmarking - Micro-F1<BR>","<BR>task: Semantic analysis<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-E<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-R<BR>","<BR>task: Semantic analysis<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>","<BR>task: Semantic analysis<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - All<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>","<BR>task: Semantic analysis<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>","<BR>task: Semantic analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>","<BR>task: Semantic analysis<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Semantic Role Labeling: CoNLL 2005 - Semantic Role Labeling benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: Geo - Semantic Parsing benchmarking - Accuracy<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: spider - Semantic Parsing benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: AQUAINT - Entity Disambiguation benchmarking - Micro-F1<BR>","<BR>task: Semantic analysis<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: WikiSQL - Semantic Parsing benchmarking - Accuracy<BR>","<BR>task: Sentence embedding<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - CR<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>","<BR>task: Syntactic analysis<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>","<BR>task: Syntactic analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Grammatical Error Detection: CoNLL-2014 A1 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Syntactic analysis<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>","<BR>task: Syntactic analysis<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Dependency Parsing: CoNLL-2009 - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: CoNLL-2009 - Dependency Parsing benchmarking - UAS<BR>","<BR>task: Syntactic analysis<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Chunking: CoNLL 2000 - Chunking benchmarking - Exact Span F1<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>","<BR>task: Syntactic analysis<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>  Dependency Parsing: GENIA - LAS - Dependency Parsing benchmarking - F1<BR>  Dependency Parsing: GENIA - UAS - Dependency Parsing benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ10)<BR>","<BR>task: Syntactic analysis<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Grammatical Error Detection: JFLEG - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Syntactic analysis<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>","<BR>task: Text classification<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Reuters De-En - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters En-De - Document Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>","<BR>task: Text classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: AG News - Text Classification benchmarking - Error<BR>  Text Classification: DBpedia - Text Classification benchmarking - Error<BR>","<BR>task: Text classification<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: RCV1 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: TREC-50 - Text Classification benchmarking - Error<BR>","<BR>task: Text classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: WOS-11967 - Document Classification benchmarking - Accuracy<BR>  Document Classification: WOS-46985 - Document Classification benchmarking - Accuracy<BR>  Document Classification: WOS-5736 - Document Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Sentence Classification: ACL-ARC - Sentence Classification benchmarking - F1<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Accuracy<BR>  Text Classification: LOCAL DATASET - Text Classification benchmarking - Accuracy (%)<BR>","<BR>task: Text classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - F-measure<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - F-measure<BR>","<BR>task: Text classification<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Sentence Classification: PubMed 20k RCT - Sentence Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Sogou News - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yelp-5 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Citation Intent Classification: SciCite - Citation Intent Classification benchmarking - F1<BR>  Sentence Classification: Paper Field - Sentence Classification benchmarking - F1<BR>  Sentence Classification: ScienceCite - Sentence Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: AAPD - Document Classification benchmarking - F1<BR>  Document Classification: Amazon - Document Classification benchmarking - Accuracy<BR>  Document Classification: BBCSport - Document Classification benchmarking - Accuracy<BR>  Document Classification: Classic - Document Classification benchmarking - Accuracy<BR>  Document Classification: Recipe - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - Accuracy<BR>  Document Classification: Twitter - Document Classification benchmarking - Accuracy<BR>  Document Classification: Yelp-14 - Document Classification benchmarking - Accuracy<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (10 classes)<BR>","<BR>task: Text classification<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yelp-2 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: IMDb-M - Document Classification benchmarking - Accuracy<BR>  Text Classification: Amazon-2 - Text Classification benchmarking - Error<BR>  Text Classification: Amazon-5 - Text Classification benchmarking - Error<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-1<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-3<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-5<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-1<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-3<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-5<BR>","<BR>task: Text classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: MPQA - Document Classification benchmarking - Accuracy<BR>  Text Classification: RCV1 - Text Classification benchmarking - Macro F1<BR>  Text Classification: RCV1 - Text Classification benchmarking - Micro F1<BR>","<BR>task: Text classification<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Precision<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Recall<BR>"],"marker":{"line":{"width":1,"color":"black"},"size":21,"symbol":42},"mode":"markers","x":["2016-03","2017-05","2017-08","2018-05","2018-10","2016-03","2016-05","2016-06","2017-04","2014-04","2015-08","2017-12","2018-03","2018-05","2018-06","2018-08","2018-09","2018-10","2018-11","2018-12","2019-04","2019-05","2019-06","2019-09","2014-06","2014-09","2014-10","2016-08","2017-06","2017-09","2018-04","2018-05","2018-06","2018-08","2018-09","2018-10","2019-01","2019-03","2019-04","2019-05","2019-06","2019-08","2019-09","2019-11","2020-03","2018-02","2018-12","2019-01","2019-04","2018-04","2019-01","2013-12","2014-06","2014-09","2015-08","2015-09","2015-12","2016-02","2016-06","2016-07","2016-09","2016-11","2016-12","2017-02","2017-04","2017-05","2017-09","2017-11","2018-03","2018-05","2018-07","2018-08","2018-09","2018-10","2019-01","2019-02","2019-06","2019-07","2019-08","2019-10","2019-11","2016-10","2016-12","2017-10","2017-11","2013-10","2014-12","2015-05","2015-09","2016-04","2016-07","2017-02","2017-04","2017-07","2017-12","2018-03","2018-06","2018-12","2019-01","2019-02","2019-03","2019-06","2019-08","2019-09","2019-12","2014-04","2014-05","2014-12","2015-03","2015-06","2015-11","2016-02","2016-03","2016-06","2016-08","2016-11","2016-12","2017-03","2017-04","2017-05","2017-07","2017-08","2017-10","2017-11","2018-03","2018-05","2018-08","2018-10","2019-01","2019-02","2019-04","2019-05","2019-07","2019-08","2020-04","2013-10","2014-11","2015-05","2016-01","2016-03","2017-04","2017-05","2017-07","2017-09","2018-01","2018-02","2018-03","2018-04","2018-05","2018-09","2019-09","2019-10","2015-09","2014-12","2015-06","2016-07","2016-08","2016-11","2017-11","2018-08","2018-10","2018-11","2019-01","2013-06","2014-03","2014-05","2014-10","2015-04","2015-09","2016-02","2016-07","2016-12","2017-07","2017-09","2018-01","2018-05","2018-06","2018-08","2018-09","2018-10","2019-01","2019-02","2019-03","2019-04","2019-05","2019-06","2019-08","2019-11"],"y":["Computer code processing","Computer code processing","Computer code processing","Computer code processing","Computer code processing","Dialog process","Dialog process","Dialog process","Dialog process","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information retrieval","Information retrieval","Information retrieval","Information retrieval","Machine translation","Machine translation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Text-to-image Generation","Text-to-image Generation","Text-to-image Generation","Text-to-image Generation","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Sentence embedding","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification"],"type":"scatter","line":{"color":"black","width":0}},{"hovertemplate":["<BR>task: Pragmatics analysis<BR>date: 2013-10<BR>ratio: 0.5551<BR>benchmarks:<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2014-06<BR>ratio: 0.3305<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2014-06<BR>ratio: 0.76<BR>benchmarks:<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2014-08<BR>ratio: 0.2549<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2014-08<BR>ratio: 0.2308<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2014-09<BR>ratio: 0.0886<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2014-10<BR>ratio: 0.0904<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2014-12<BR>ratio: 0.3032<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>","<BR>task: Natural language generation<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - PPL<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-02<BR>ratio: 0.1186<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-06<BR>ratio: 0.0427<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2015-06<BR>ratio: 0.232<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2015-08<BR>ratio: 0.5<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Train Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2015-09<BR>ratio: 0.1435<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-11<BR>ratio: 0.4025<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>","<BR>task: Text classification<BR>date: 2015-11<BR>ratio: 0.25<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>","<BR>task: Question answering<BR>date: 2015-11<BR>ratio: 0.2159<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>","<BR>task: Information extraction<BR>date: 2016-01<BR>ratio: 0.483<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-02<BR>ratio: 0.3491<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2016-02<BR>ratio: 0.3173<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Question answering<BR>date: 2016-02<BR>ratio: 0.0718<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>","<BR>task: Text classification<BR>date: 2016-03<BR>ratio: 0.5215<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Syntactic analysis<BR>date: 2016-03<BR>ratio: 0.1966<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>","<BR>task: Natural language generation<BR>date: 2016-03<BR>ratio: 0.2<BR>benchmarks:<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Semantic analysis<BR>date: 2016-03<BR>ratio: 0.1323<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2016-03<BR>ratio: 0.40459999999999996<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: MCTest-500 - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2016-06<BR>ratio: 0.0678<BR>benchmarks:<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2016-06<BR>ratio: 0.30955<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 German-English - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-06<BR>ratio: 0.0702<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2016-06<BR>ratio: 0.7941<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2016-06<BR>ratio: 0.3097<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 10k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 1k)<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Syntactic analysis<BR>date: 2016-07<BR>ratio: 0.9481<BR>benchmarks:<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-07<BR>ratio: 0.094<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2016-07<BR>ratio: 0.218<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2016-08<BR>ratio: 0.3589<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2016-08<BR>ratio: 0.1882<BR>benchmarks:<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2016-09<BR>ratio: 0.36160000000000003<BR>benchmarks:<BR>  Language Modelling: enwik8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-09<BR>ratio: 0.0286<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2016-09<BR>ratio: 0.3558<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2016-09<BR>ratio: 0.101<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2016-09<BR>ratio: 0.098<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Question answering<BR>date: 2016-10<BR>ratio: 0.6069<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: bAbi - Question Answering benchmarking - Mean Error Rate<BR>","<BR>task: Natural language generation<BR>date: 2016-10<BR>ratio: 0.8<BR>benchmarks:<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2016-11<BR>ratio: 0.29685<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>","<BR>task: Syntactic analysis<BR>date: 2016-11<BR>ratio: 0.38507499999999995<BR>benchmarks:<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Natural language generation<BR>date: 2016-11<BR>ratio: 0.0632<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Text classification<BR>date: 2016-11<BR>ratio: 0.0123<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2016-12<BR>ratio: 0.6667<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Test perplexity<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Test perplexity<BR>","<BR>task: Text-to-image Generation<BR>date: 2016-12<BR>ratio: 0.0708<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Question answering<BR>date: 2016-12<BR>ratio: 0.7801<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Natural language generation<BR>date: 2017-01<BR>ratio: 0.0597<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Text classification<BR>date: 2017-02<BR>ratio: 0.2857<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>","<BR>task: Inference and reasoning<BR>date: 2017-02<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-02<BR>ratio: 0.1937<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>","<BR>task: Question answering<BR>date: 2017-03<BR>ratio: 0.033<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2017-04<BR>ratio: 0.6016<BR>benchmarks:<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Semantic analysis<BR>date: 2017-04<BR>ratio: 0.55<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-04<BR>ratio: 0.14375<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2017-04<BR>ratio: 0.06<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2017-05<BR>ratio: 0.921<BR>benchmarks:<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: TAC2010 - Entity Disambiguation benchmarking - Micro Precision<BR>","<BR>task: Natural language generation<BR>date: 2017-05<BR>ratio: 0.2783333333333333<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: Chinese Poems - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>","<BR>task: Question answering<BR>date: 2017-05<BR>ratio: 0.2379<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Natural language generation<BR>date: 2017-06<BR>ratio: 0.42015<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>","<BR>task: Question answering<BR>date: 2017-06<BR>ratio: 0.1524<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2017-07<BR>ratio: 0.30775<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>  Grammatical Error Detection: CoNLL-2014 A1 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Question answering<BR>date: 2017-07<BR>ratio: 0.22825<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Information extraction<BR>date: 2017-07<BR>ratio: 0.212<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>","<BR>task: Sentence embedding<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - CR<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-07<BR>ratio: 0.57325<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: Amazon Review Full - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Amazon Review Polarity - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2017-08<BR>ratio: 0.4228<BR>benchmarks:<BR>  Question Answering: AI2 Kaggle Dataset - Question Answering benchmarking - P-at-1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-08<BR>ratio: 0.2288<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Computer code processing<BR>date: 2017-08<BR>ratio: 0.4269<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2017-09<BR>ratio: 0.0407<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-09<BR>ratio: 0.3099<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>","<BR>task: Dialog process<BR>date: 2017-09<BR>ratio: 0.56715<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Inference and reasoning<BR>date: 2017-09<BR>ratio: 0.0196<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2017-09<BR>ratio: 0.6226<BR>benchmarks:<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-3<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-4<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>","<BR>task: Information extraction<BR>date: 2017-09<BR>ratio: 0.4385<BR>benchmarks:<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Text-to-image Generation<BR>date: 2017-10<BR>ratio: 0.7021<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Text classification<BR>date: 2017-10<BR>ratio: 0.0798<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2017-10<BR>ratio: 0.6272<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>","<BR>task: Text-to-image Generation<BR>date: 2017-11<BR>ratio: 0.6346<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Dialog process<BR>date: 2017-11<BR>ratio: 0.25695<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Inference and reasoning<BR>date: 2017-11<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2017-11<BR>ratio: 0.0459<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2017-12<BR>ratio: 0.0571<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2017-12<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2017-12<BR>ratio: 0.1887<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-12<BR>ratio: 0.54575<BR>benchmarks:<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Discuss)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Unrelated)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Weighted Accuracy<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-01<BR>ratio: 0.0809<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2018-01<BR>ratio: 0.4672<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2018-02<BR>ratio: 0.19115<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>","<BR>task: Text classification<BR>date: 2018-02<BR>ratio: 0.1176<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-02<BR>ratio: 0.2873<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2018-02<BR>ratio: 0.6357<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>","<BR>task: Question answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2018-03<BR>ratio: 0.5076<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Semantic analysis<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-E<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-R<BR>","<BR>task: Information retrieval<BR>date: 2018-03<BR>ratio: 0.541<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-03<BR>ratio: 0.0357<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>","<BR>task: Computer code processing<BR>date: 2018-03<BR>ratio: 0.2009<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Question answering<BR>date: 2018-04<BR>ratio: 0.5921<BR>benchmarks:<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>","<BR>task: Inference and reasoning<BR>date: 2018-04<BR>ratio: 0.0412<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-04<BR>ratio: 0.2342<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>","<BR>task: Computer code processing<BR>date: 2018-04<BR>ratio: 0.137<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-05<BR>ratio: 0.2896<BR>benchmarks:<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Syntactic analysis<BR>date: 2018-05<BR>ratio: 0.1343<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>","<BR>task: Dialog process<BR>date: 2018-05<BR>ratio: 0.8365<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Request<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Request<BR>","<BR>task: Inference and reasoning<BR>date: 2018-05<BR>ratio: 0.57845<BR>benchmarks:<BR>  Common Sense Reasoning: Event2Mind test - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2018-05<BR>ratio: 0.11170000000000001<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>","<BR>task: Question answering<BR>date: 2018-05<BR>ratio: 0.36539999999999995<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Text classification<BR>date: 2018-05<BR>ratio: 0.3299<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>","<BR>task: Question answering<BR>date: 2018-06<BR>ratio: 0.3862<BR>benchmarks:<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>  Question Answering: Story Cloze Test - Question Answering benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-06<BR>ratio: 0.6132<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: V-SNLI - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2018-06<BR>ratio: 0.11065<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>","<BR>task: Natural language generation<BR>date: 2018-07<BR>ratio: 0.0553<BR>benchmarks:<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>","<BR>task: Text classification<BR>date: 2018-07<BR>ratio: 0.5132<BR>benchmarks:<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2018-07<BR>ratio: 0.3122<BR>benchmarks:<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-07<BR>ratio: 0.0282<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>","<BR>task: Sentence embedding<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Question Answering: Quasart-T - Question Answering benchmarking - EM<BR>","<BR>task: Syntactic analysis<BR>date: 2018-07<BR>ratio: 0.791<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>","<BR>task: Question answering<BR>date: 2018-08<BR>ratio: 0.2718<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2018-08<BR>ratio: 0.72315<BR>benchmarks:<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>","<BR>task: Natural language generation<BR>date: 2018-08<BR>ratio: 0.3277<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - PPL<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Syntactic analysis<BR>date: 2018-08<BR>ratio: 0.4922<BR>benchmarks:<BR>  Chunking: CoNLL 2000 - Chunking benchmarking - Exact Span F1<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>","<BR>task: Text classification<BR>date: 2018-08<BR>ratio: 0.5092<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>  Text Classification: AG News - Text Classification benchmarking - Error<BR>","<BR>task: Text classification<BR>date: 2018-09<BR>ratio: 0.652<BR>benchmarks:<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Dialog process<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Inference and reasoning<BR>date: 2018-09<BR>ratio: 0.2353<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2018-09<BR>ratio: 0.53495<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>","<BR>task: Information extraction<BR>date: 2018-09<BR>ratio: 0.3185<BR>benchmarks:<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2018-09<BR>ratio: 0.2419<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>","<BR>task: Question answering<BR>date: 2018-10<BR>ratio: 0.196<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2018-10<BR>ratio: 0.43885<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>  Semantic Role Labeling: CoNLL 2005 - Semantic Role Labeling benchmarking - F1<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2018-10<BR>ratio: 0.2123<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>","<BR>task: Inference and reasoning<BR>date: 2018-10<BR>ratio: 0.48510000000000003<BR>benchmarks:<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Dev<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Dialog process<BR>date: 2018-10<BR>ratio: 0.327<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>","<BR>task: Natural language generation<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Machine Translation: WMT 2017 Latvian-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Information extraction<BR>date: 2018-10<BR>ratio: 0.20085<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Machine translation<BR>date: 2018-10<BR>ratio: 0.15<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Text classification<BR>date: 2018-10<BR>ratio: 0.9057<BR>benchmarks:<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>","<BR>task: Computer code processing<BR>date: 2018-10<BR>ratio: 0.5481<BR>benchmarks:<BR>  Code Generation: Django - Code Generation benchmarking - Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-10<BR>ratio: 0.33775<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2018-11<BR>ratio: 0.2719<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>","<BR>task: Syntactic analysis<BR>date: 2018-11<BR>ratio: 0.2698<BR>benchmarks:<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Question answering<BR>date: 2018-11<BR>ratio: 0.1449<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-11<BR>ratio: 0.35325<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>","<BR>task: Information extraction<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-10%<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-30%<BR>","<BR>task: Machine translation<BR>date: 2019-01<BR>ratio: 0.4127<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-01<BR>ratio: 0.47159999999999996<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT 2017 English-Chinese - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-Czech - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Romanian-English - Machine Translation benchmarking - BLEU score<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>","<BR>task: Information retrieval<BR>date: 2019-01<BR>ratio: 0.3558<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-01<BR>ratio: 0.6137<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>","<BR>task: Text-to-image Generation<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - FID<BR>","<BR>task: Question answering<BR>date: 2019-01<BR>ratio: 0.2041<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>","<BR>task: Inference and reasoning<BR>date: 2019-01<BR>ratio: 0.3456<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI French - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Named Entity Recognition: NCBI-disease - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-01<BR>ratio: 0.37765000000000004<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Yelp Fine-grained classification - Sentiment Analysis benchmarking - Error<BR>","<BR>task: Question answering<BR>date: 2019-02<BR>ratio: 0.6327<BR>benchmarks:<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Machine translation<BR>date: 2019-02<BR>ratio: 0.397<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Inference and reasoning<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Common Sense Reasoning: Winograd Schema Challenge - Common Sense Reasoning benchmarking - Score<BR>","<BR>task: Information extraction<BR>date: 2019-02<BR>ratio: 0.3333<BR>benchmarks:<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2019-02<BR>ratio: 0.3546<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Accuracy<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Dialog process<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Natural language generation<BR>date: 2019-03<BR>ratio: 0.3031<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>","<BR>task: Text-to-image Generation<BR>date: 2019-03<BR>ratio: 0.1016<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Text classification<BR>date: 2019-03<BR>ratio: 0.6981<BR>benchmarks:<BR>  Sentence Classification: ACL-ARC - Sentence Classification benchmarking - F1<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>  Sentence Classification: ScienceCite - Sentence Classification benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-03<BR>ratio: 0.833<BR>benchmarks:<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ChemProt - Relation Extraction benchmarking - F1<BR>  Relation Extraction: SciERC - Relation Extraction benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2019-03<BR>ratio: 0.1343<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>","<BR>task: Information retrieval<BR>date: 2019-04<BR>ratio: 0.2915<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Dialog process<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Natural language generation<BR>date: 2019-04<BR>ratio: 0.5763<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>","<BR>task: Question answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-04<BR>ratio: 0.4341666666666666<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>","<BR>task: Text-to-image Generation<BR>date: 2019-04<BR>ratio: 0.5876<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - SOA-C<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Acc<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>","<BR>task: Syntactic analysis<BR>date: 2019-04<BR>ratio: 0.1967<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>","<BR>task: Text classification<BR>date: 2019-04<BR>ratio: 0.0123<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-05<BR>ratio: 0.4549<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>  Text Classification: Sogou News - Text Classification benchmarking - Accuracy<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>","<BR>task: Machine translation<BR>date: 2019-05<BR>ratio: 0.3157<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Question answering<BR>date: 2019-05<BR>ratio: 0.45885<BR>benchmarks:<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Semantic analysis<BR>date: 2019-05<BR>ratio: 0.575<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>","<BR>task: Natural language generation<BR>date: 2019-05<BR>ratio: 0.40315<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-05<BR>ratio: 0.3432<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-05<BR>ratio: 0.2659<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2019-06<BR>ratio: 0.4636<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-06<BR>ratio: 0.2085<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2019-06<BR>ratio: 0.5752<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT 2018 Finnish-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Syntactic analysis<BR>date: 2019-06<BR>ratio: 0.7168<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ10)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-06<BR>ratio: 0.5772666666666666<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Slot F1 Score<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2019-06<BR>ratio: 0.4252<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>","<BR>task: Question answering<BR>date: 2019-06<BR>ratio: 0.2683<BR>benchmarks:<BR>  Question Answering: Quora Question Pairs - Question Answering benchmarking - Accuracy<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Text classification<BR>date: 2019-06<BR>ratio: 0.5346<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: Yelp-2 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-07<BR>ratio: 0.3592<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A1<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese Dev - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-07<BR>ratio: 0.46240000000000003<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-07<BR>ratio: 0.2631<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-07<BR>ratio: 0.5883499999999999<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: MSRA Dev - Chinese Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2019-07<BR>ratio: 0.0577<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-08<BR>ratio: 0.18335<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>","<BR>task: Information extraction<BR>date: 2019-08<BR>ratio: 0.4151<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2019-08<BR>ratio: 0.1081<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>","<BR>task: Question answering<BR>date: 2019-08<BR>ratio: 0.23105<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>","<BR>task: Text classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Document Classification: BBCSport - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - Accuracy<BR>","<BR>task: Syntactic analysis<BR>date: 2019-09<BR>ratio: 0.1429<BR>benchmarks:<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2019-09<BR>ratio: 0.2846<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Information extraction<BR>date: 2019-09<BR>ratio: 0.60275<BR>benchmarks:<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-09<BR>ratio: 0.02725<BR>benchmarks:<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Text-to-image Generation<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - FID<BR>","<BR>task: Question answering<BR>date: 2019-09<BR>ratio: 0.09475<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Text classification<BR>date: 2019-09<BR>ratio: 0.8833<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - F-measure<BR>  Text Classification: Amazon-2 - Text Classification benchmarking - Error<BR>  Text Classification: Amazon-5 - Text Classification benchmarking - Error<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - F-measure<BR>","<BR>task: Inference and reasoning<BR>date: 2019-09<BR>ratio: 0.12815000000000001<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2019-09<BR>ratio: 0.5651333333333334<BR>benchmarks:<BR>  Entity Disambiguation: ACE2004 - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: AQUAINT - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: MSNBC - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-CWEB - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-WIKI - Entity Disambiguation benchmarking - Micro-F1<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2019-10<BR>ratio: 0.438<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-3<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>","<BR>task: Computer code processing<BR>date: 2019-10<BR>ratio: 0.1871<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-11<BR>ratio: 0.32506666666666667<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Resume NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>","<BR>task: Information retrieval<BR>date: 2019-11<BR>ratio: 0.4096<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI AmazonQA - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Intent Detection: ASOS.com user intent - Intent Detection benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2020-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - F1<BR>  Text Classification: RCV1 - Text Classification benchmarking - Micro F1<BR>","<BR>task: Information extraction<BR>date: 2020-03<BR>ratio: 0.3066<BR>benchmarks:<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2020-04<BR>ratio: 0.2667<BR>benchmarks:<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>"],"marker":{"color":[0.5551,0.3305,0.76,0.2549,0.2308,0.0886,0.0904,0.3032,1.0,0.1186,0.0427,0.232,0.5,0.1435,0.4025,0.25,0.2159,0.483,0.3491,0.3173,0.0718,0.5215,0.1966,0.2,0.1323,0.40459999999999996,0.2762,0.0678,0.30955,0.0702,0.7941,0.3097,0.9481,0.094,0.218,0.3589,0.1882,0.36160000000000003,0.0286,0.3558,0.101,0.098,0.6069,0.8,0.29685,0.38507499999999995,0.0632,0.0123,0.6667,0.0708,0.7801,0.0597,0.2857,0.0392,0.1937,0.033,0.6016,0.55,0.14375,0.06,0.921,0.2783333333333333,0.2379,0.42015,0.1524,0.30775,0.22825,0.212,1.0,0.57325,0.4228,0.2288,0.4269,0.0407,0.3099,0.56715,0.0196,0.6226,0.4385,0.7021,0.0798,0.6272,0.6346,0.25695,0.0392,0.0459,0.0571,0.0392,0.1887,0.54575,0.0809,0.0882,0.4672,0.19115,0.1176,0.2873,0.6357,0.1587,0.5076,1.0,0.541,0.0357,0.2009,0.5921,0.0412,0.2342,0.137,0.2896,0.1343,0.8365,0.57845,0.11170000000000001,0.36539999999999995,0.3299,0.7567,0.3862,0.6132,0.11065,0.0553,0.5132,0.3122,0.0282,1.0,1.0,0.791,0.2718,0.72315,0.3277,0.4922,0.5092,0.652,0.1442,0.2353,0.53495,0.3185,0.2419,0.196,0.43885,0.2123,0.48510000000000003,0.327,1.0,0.20085,0.15,0.9057,0.5481,0.33775,0.2719,0.2698,0.1449,0.35325,1.0,0.4127,0.47159999999999996,0.3558,0.6137,1.0,0.2041,0.3456,1.0,0.37765000000000004,0.6327,0.397,1.0,0.3333,0.3546,0.1426,0.3031,0.1016,0.6981,0.833,0.1343,0.2915,0.4213,0.5763,0.3147,0.4341666666666666,0.5876,1.0,0.1967,0.0123,0.4549,0.3157,0.45885,0.575,0.40315,0.3432,0.2659,0.4636,0.2085,0.5752,0.7168,0.5772666666666666,0.4252,0.2683,0.5346,0.3592,0.46240000000000003,0.2631,0.5883499999999999,0.0577,0.18335,0.4151,0.1081,0.23105,1.0,0.1429,0.2846,0.60275,0.02725,1.0,0.09475,0.8833,0.12815000000000001,0.5651333333333334,0.438,0.1871,0.32506666666666667,0.4096,1.0,0.5266,1.0,0.3066,0.2667],"colorbar":{"len":500,"lenmode":"pixels","thickness":10,"title":{"text":"ratio"}},"colorscale":[[0.0,"rgb(255,255,229)"],[0.125,"rgb(247,252,185)"],[0.25,"rgb(217,240,163)"],[0.375,"rgb(173,221,142)"],[0.5,"rgb(120,198,121)"],[0.625,"rgb(65,171,93)"],[0.75,"rgb(35,132,67)"],[0.875,"rgb(0,104,55)"],[1.0,"rgb(0,69,41)"]],"opacity":0.7,"showscale":true,"size":20,"symbol":"circle","line":{"color":"black","width":1}},"mode":"markers","x":["2013-10","2014-06","2014-06","2014-08","2014-08","2014-09","2014-10","2014-12","2014-12","2015-02","2015-06","2015-06","2015-08","2015-09","2015-11","2015-11","2015-11","2016-01","2016-02","2016-02","2016-02","2016-03","2016-03","2016-03","2016-03","2016-03","2016-05","2016-06","2016-06","2016-06","2016-06","2016-06","2016-07","2016-07","2016-07","2016-08","2016-08","2016-09","2016-09","2016-09","2016-09","2016-09","2016-10","2016-10","2016-11","2016-11","2016-11","2016-11","2016-12","2016-12","2016-12","2017-01","2017-02","2017-02","2017-02","2017-03","2017-04","2017-04","2017-04","2017-04","2017-05","2017-05","2017-05","2017-06","2017-06","2017-07","2017-07","2017-07","2017-07","2017-07","2017-08","2017-08","2017-08","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-10","2017-10","2017-10","2017-11","2017-11","2017-11","2017-11","2017-12","2017-12","2017-12","2017-12","2018-01","2018-01","2018-01","2018-02","2018-02","2018-02","2018-02","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-04","2018-04","2018-04","2018-04","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-06","2018-06","2018-06","2018-06","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-08","2018-08","2018-08","2018-08","2018-08","2018-09","2018-09","2018-09","2018-09","2018-09","2018-09","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-11","2018-11","2018-11","2018-11","2018-12","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-03","2019-03","2019-03","2019-03","2019-03","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-07","2019-07","2019-07","2019-07","2019-07","2019-08","2019-08","2019-08","2019-08","2019-08","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-10","2019-10","2019-11","2019-11","2019-12","2020-02","2020-02","2020-03","2020-04"],"y":["Pragmatics analysis","Pragmatics analysis","Question answering","Inference and reasoning","Pragmatics analysis","Natural language generation","Natural language generation","Question answering","Natural language generation","Pragmatics analysis","Pragmatics analysis","Question answering","Inference and reasoning","Natural language generation","Pragmatics analysis","Text classification","Question answering","Information extraction","Pragmatics analysis","Natural language generation","Question answering","Text classification","Syntactic analysis","Natural language generation","Semantic analysis","Question answering","Question answering","Semantic analysis","Natural language generation","Pragmatics analysis","Text classification","Question answering","Syntactic analysis","Pragmatics analysis","Natural language generation","Question answering","Natural language generation","Natural language generation","Pragmatics analysis","Text classification","Question answering","Inference and reasoning","Question answering","Natural language generation","Question answering","Syntactic analysis","Natural language generation","Text classification","Natural language generation","Text-to-image Generation","Question answering","Natural language generation","Text classification","Inference and reasoning","Pragmatics analysis","Question answering","Syntactic analysis","Semantic analysis","Pragmatics analysis","Question answering","Semantic analysis","Natural language generation","Question answering","Natural language generation","Question answering","Syntactic analysis","Question answering","Information extraction","Sentence embedding","Pragmatics analysis","Question answering","Pragmatics analysis","Computer code processing","Semantic analysis","Pragmatics analysis","Dialog process","Inference and reasoning","Natural language generation","Information extraction","Text-to-image Generation","Text classification","Question answering","Text-to-image Generation","Dialog process","Inference and reasoning","Natural language generation","Question answering","Inference and reasoning","Semantic analysis","Pragmatics analysis","Pragmatics analysis","Text classification","Question answering","Semantic analysis","Text classification","Pragmatics analysis","Natural language generation","Question answering","Natural language generation","Semantic analysis","Information retrieval","Text classification","Computer code processing","Question answering","Inference and reasoning","Pragmatics analysis","Computer code processing","Pragmatics analysis","Syntactic analysis","Dialog process","Inference and reasoning","Semantic analysis","Question answering","Text classification","Pragmatics analysis","Question answering","Inference and reasoning","Natural language generation","Natural language generation","Text classification","Information extraction","Pragmatics analysis","Sentence embedding","Question answering","Syntactic analysis","Question answering","Information extraction","Natural language generation","Syntactic analysis","Text classification","Text classification","Dialog process","Inference and reasoning","Natural language generation","Information extraction","Question answering","Question answering","Semantic analysis","Syntactic analysis","Inference and reasoning","Dialog process","Natural language generation","Information extraction","Machine translation","Text classification","Computer code processing","Pragmatics analysis","Semantic analysis","Syntactic analysis","Question answering","Pragmatics analysis","Information extraction","Machine translation","Natural language generation","Information retrieval","Text classification","Text-to-image Generation","Question answering","Inference and reasoning","Information extraction","Pragmatics analysis","Question answering","Machine translation","Inference and reasoning","Information extraction","Text classification","Dialog process","Natural language generation","Text-to-image Generation","Text classification","Information extraction","Syntactic analysis","Information retrieval","Dialog process","Natural language generation","Question answering","Information extraction","Text-to-image Generation","Pragmatics analysis","Syntactic analysis","Text classification","Text classification","Machine translation","Question answering","Semantic analysis","Natural language generation","Pragmatics analysis","Information extraction","Inference and reasoning","Information extraction","Natural language generation","Syntactic analysis","Pragmatics analysis","Semantic analysis","Question answering","Text classification","Inference and reasoning","Question answering","Pragmatics analysis","Information extraction","Semantic analysis","Pragmatics analysis","Information extraction","Natural language generation","Question answering","Text classification","Syntactic analysis","Natural language generation","Information extraction","Pragmatics analysis","Text-to-image Generation","Question answering","Text classification","Inference and reasoning","Semantic analysis","Natural language generation","Computer code processing","Information extraction","Information retrieval","Pragmatics analysis","Question answering","Text classification","Information extraction","Information extraction"],"type":"scatter","line":{"color":"black","width":0}}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"},"showgrid":true,"gridcolor":"lightBlue","tickmode":"auto"},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{},"categoryorder":"array","categoryarray":["Text classification","Syntactic analysis","Sentence embedding","Semantic analysis","Question answering","Pragmatics analysis","Text-to-image Generation","Natural language generation","Machine translation","Information retrieval","Information extraction","Inference and reasoning","Dialog process","Computer code processing"],"showgrid":true,"gridcolor":"lightBlue","side":"left"},"legend":{"title":{"text":"task"},"tracegroupgap":0},"margin":{"t":60},"title":{"text":"Natural Language Processing","y":0.995},"font":{"size":21},"showlegend":false,"plot_bgcolor":"white","height":700.0,"width":1500},                        {"responsive": true}                    )                };                            </script>        </div>
</body>
</html>