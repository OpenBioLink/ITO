{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94261446",
   "metadata": {},
   "source": [
    "## Check metric polatiry\n",
    "\n",
    "This notebook acesses the evaluation-tables.json file and compares the rank1 and rank 2 values for all the metrics.\n",
    "\n",
    "This outputs a file called df_metric_all.csv used for the calculation of the polarity, comparing the values of the ranks 1 and 2 per metric and dataset.\n",
    "\n",
    "Make sure the file evaluation-tables.json is available in the working dir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c9a6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Adjust display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a84d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "from SPARQLWrapper import SPARQLWrapper, N3, JSON\n",
    "from rdflib import Graph\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "\n",
    "#setup some pandas display modes\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e882f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read evaluation table\n",
    "df = pd.DataFrame(pd.read_json(\"evaluation-tables.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d5818a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'categories': [],\n",
       "  'datasets': [{'dataset': 'SYSU 3D',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sysu-3d'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'microsoft/SGN',\n",
       "         'url': 'https://github.com/microsoft/SGN'}],\n",
       "       'metrics': {'Accuracy': '86.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SGN',\n",
       "       'paper_date': '2019-04-02',\n",
       "       'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1904.01189v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "         'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy': '86.7%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VA-fusion (aug.)',\n",
       "       'paper_date': '2018-04-20',\n",
       "       'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '85.7%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EleAtt-GRU (aug.)',\n",
       "       'paper_date': '2019-09-03',\n",
       "       'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/1909.01939v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '83.14%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Local+LGN',\n",
       "       'paper_date': '2019-09-02',\n",
       "       'paper_title': 'Learning Latent Global Network for Skeleton-based Action Prediction',\n",
       "       'paper_url': 'https://doi.org/10.1109/TIP.2019.2937757',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '77.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Complete GR-GCN',\n",
       "       'paper_date': '2018-11-29',\n",
       "       'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "       'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '77.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VA-LSTM',\n",
       "       'paper_date': '2017-03-24',\n",
       "       'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data',\n",
       "       'paper_url': 'http://arxiv.org/abs/1703.08274v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '76.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DPRL',\n",
       "       'paper_date': '2018-06-01',\n",
       "       'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '75.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Dynamic Skeletons',\n",
       "       'paper_date': '2016-12-15',\n",
       "       'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '73.4%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-LSTM (Tree)',\n",
       "       'paper_date': '2017-06-26',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates',\n",
       "       'paper_url': 'http://arxiv.org/abs/1706.08276v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'UT-Kinect',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ut'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "         'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "       'metrics': {'Accuracy': '99.50%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Temporal Subspace Clustering',\n",
       "       'paper_date': '2020-06-21',\n",
       "       'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "       'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '98.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Complete GR-GCN',\n",
       "       'paper_date': '2018-11-29',\n",
       "       'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "       'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '98.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DPRL',\n",
       "       'paper_date': '2018-06-01',\n",
       "       'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'fdsa1860/skeletal',\n",
       "         'url': 'https://github.com/fdsa1860/skeletal'}],\n",
       "       'metrics': {'Accuracy': '97.1%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Lie Group',\n",
       "       'paper_date': '2014-06-23',\n",
       "       'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group',\n",
       "       'paper_url': 'https://doi.org/10.1109/CVPR.2014.82',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '96%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GFT',\n",
       "       'paper_date': '2019-08-26',\n",
       "       'paper_title': 'Graph Based Skeleton Modeling for Human Activity Analysis',\n",
       "       'paper_url': 'https://doi.org/10.1109/ICIP.2019.8803186',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'N-UCLA',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-n-ucla'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [],\n",
       "       'metrics': {'Accuracy': '93.99'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Hierarchical Action Classification (RGB + Pose)',\n",
       "       'paper_date': '2020-07-30',\n",
       "       'paper_title': 'Hierarchical Action Classification with Network Pruning',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.15244v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'srijandas07/VPN',\n",
       "         'url': 'https://github.com/srijandas07/VPN'}],\n",
       "       'metrics': {'Accuracy': '93.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VPN (RGB + Pose)',\n",
       "       'paper_date': '2020-07-06',\n",
       "       'paper_title': 'VPN: Learning Video-Pose Embedding for Activities of Daily Living',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.03056v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'srijandas07/vpnplusplus',\n",
       "         'url': 'https://github.com/srijandas07/vpnplusplus'}],\n",
       "       'metrics': {'Accuracy': '93.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VPN++ (RGB + Pose)',\n",
       "       'paper_date': '2021-05-17',\n",
       "       'paper_title': 'VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living',\n",
       "       'paper_url': 'https://arxiv.org/abs/2105.08141v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'microsoft/SGN',\n",
       "         'url': 'https://github.com/microsoft/SGN'}],\n",
       "       'metrics': {'Accuracy': '92.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SGN',\n",
       "       'paper_date': '2019-04-02',\n",
       "       'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1904.01189v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '92.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Action Machine',\n",
       "       'paper_date': '2018-12-14',\n",
       "       'paper_title': 'Action Machine: Rethinking Action Recognition in Trimmed Videos',\n",
       "       'paper_url': 'http://arxiv.org/abs/1812.05770v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '90.7%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EleAtt-GRU (aug.)',\n",
       "       'paper_date': '2019-09-03',\n",
       "       'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/1909.01939v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "         'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy': '88.1%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VA-fusion (aug.)',\n",
       "       'paper_date': '2018-04-20',\n",
       "       'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'fabienbaradel/glimpse_clouds',\n",
       "         'url': 'https://github.com/fabienbaradel/glimpse_clouds'}],\n",
       "       'metrics': {'Accuracy': '87.6%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Glimpse Clouds',\n",
       "       'paper_date': '2018-02-22',\n",
       "       'paper_title': 'Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points',\n",
       "       'paper_url': 'http://arxiv.org/abs/1802.07898v4',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'SBU',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sbu'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'Sy-Zhang/Geometric-Feature-Release',\n",
       "         'url': 'https://github.com/Sy-Zhang/Geometric-Feature-Release'}],\n",
       "       'metrics': {'Accuracy': '99.02%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Joint Line Distance',\n",
       "       'paper_date': '2017-03-01',\n",
       "       'paper_title': 'On Geometric Features for Skeleton-Based Action Recognition using Multilayer LSTM Networks',\n",
       "       'paper_url': 'https://doi.org/10.1109/WACV.2017.24',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '98.60%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MLGCN',\n",
       "       'paper_date': '2019-09-11',\n",
       "       'paper_title': 'MLGCN: Multi-Laplacian Graph Convolutional Networks for Human Action Recognition',\n",
       "       'paper_url': 'https://bmvc2019.org/wp-content/uploads/papers/1103-paper.pdf',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "         'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy': '98.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VA-fusion (aug.)',\n",
       "       'paper_date': '2018-04-20',\n",
       "       'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'mdeff/cnn_graph',\n",
       "         'url': 'https://github.com/mdeff/cnn_graph'},\n",
       "        {'title': 'xbresson/spectral_graph_convnets',\n",
       "         'url': 'https://github.com/xbresson/spectral_graph_convnets'},\n",
       "        {'title': 'jasonseu/cnn_graph.pytorch',\n",
       "         'url': 'https://github.com/jasonseu/cnn_graph.pytorch'},\n",
       "        {'title': 'andrejmiscic/gcn-pytorch',\n",
       "         'url': 'https://github.com/andrejmiscic/gcn-pytorch'},\n",
       "        {'title': 'mdeff/paper-cnn-graph-nips2016',\n",
       "         'url': 'https://github.com/mdeff/paper-cnn-graph-nips2016'},\n",
       "        {'title': 'hazdzz/ChebyNet',\n",
       "         'url': 'https://github.com/hazdzz/ChebyNet'},\n",
       "        {'title': 'stsfaroz/Graph-Convolutional-Networks-with-Cora-Dataset',\n",
       "         'url': 'https://github.com/stsfaroz/Graph-Convolutional-Networks-with-Cora-Dataset'}],\n",
       "       'metrics': {'Accuracy': '96.00%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ChebyNet',\n",
       "       'paper_date': '2016-06-30',\n",
       "       'paper_title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering',\n",
       "       'paper_url': 'http://arxiv.org/abs/1606.09375v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '96.00%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ArmaConv',\n",
       "       'paper_date': '2019-01-05',\n",
       "       'paper_title': 'Graph Neural Networks with convolutional ARMA filters',\n",
       "       'paper_url': 'https://arxiv.org/abs/1901.01343v7',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'Maghoumi/DeepGRU',\n",
       "         'url': 'https://github.com/Maghoumi/DeepGRU'}],\n",
       "       'metrics': {'Accuracy': '95.7%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DeepGRU',\n",
       "       'paper_date': '2018-10-30',\n",
       "       'paper_title': 'DeepGRU: Deep Gesture Recognition Utility',\n",
       "       'paper_url': 'https://arxiv.org/abs/1810.12514v4',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'Tiiiger/SGC',\n",
       "         'url': 'https://github.com/Tiiiger/SGC'},\n",
       "        {'title': 'pulkit1joshi/SGC',\n",
       "         'url': 'https://github.com/pulkit1joshi/SGC'},\n",
       "        {'title': 'hazdzz/SGC', 'url': 'https://github.com/hazdzz/SGC'}],\n",
       "       'metrics': {'Accuracy': '94.0%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SGCConv',\n",
       "       'paper_date': '2019-02-19',\n",
       "       'paper_title': 'Simplifying Graph Convolutional Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/1902.07153v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '93.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-LSTM + Trust Gate',\n",
       "       'paper_date': '2016-07-24',\n",
       "       'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'tkipf/gcn',\n",
       "         'url': 'https://github.com/tkipf/gcn'},\n",
       "        {'title': 'tkipf/pygcn', 'url': 'https://github.com/tkipf/pygcn'},\n",
       "        {'title': 'tkipf/keras-gcn',\n",
       "         'url': 'https://github.com/tkipf/keras-gcn'},\n",
       "        {'title': 'LeeWooJung/GCN_reproduce',\n",
       "         'url': 'https://github.com/LeeWooJung/GCN_reproduce'},\n",
       "        {'title': 'hazdzz/GCN', 'url': 'https://github.com/hazdzz/GCN'},\n",
       "        {'title': 'giuseppefutia/link-prediction-code',\n",
       "         'url': 'https://github.com/giuseppefutia/link-prediction-code'},\n",
       "        {'title': 'andrejmiscic/gcn-pytorch',\n",
       "         'url': 'https://github.com/andrejmiscic/gcn-pytorch'},\n",
       "        {'title': 'switiz/gnn-gcn-gat',\n",
       "         'url': 'https://github.com/switiz/gnn-gcn-gat'},\n",
       "        {'title': 'dtriepke/Graph_Convolutional_Network',\n",
       "         'url': 'https://github.com/dtriepke/Graph_Convolutional_Network'},\n",
       "        {'title': 'bcsrn/gcn', 'url': 'https://github.com/bcsrn/gcn'},\n",
       "        {'title': 'Anieca/GCN', 'url': 'https://github.com/Anieca/GCN'},\n",
       "        {'title': 'HoganZhang/pygcn_python3',\n",
       "         'url': 'https://github.com/HoganZhang/pygcn_python3'},\n",
       "        {'title': 'lipingcoding/pygcn',\n",
       "         'url': 'https://github.com/lipingcoding/pygcn'},\n",
       "        {'title': 'KimMeen/GCN', 'url': 'https://github.com/KimMeen/GCN'},\n",
       "        {'title': 'darnbi/pygcn', 'url': 'https://github.com/darnbi/pygcn'},\n",
       "        {'title': 'thanhtrunghuynh93/pygcn',\n",
       "         'url': 'https://github.com/thanhtrunghuynh93/pygcn'},\n",
       "        {'title': 'ChengSashankh/gcn-graph-classification',\n",
       "         'url': 'https://github.com/ChengSashankh/gcn-graph-classification'},\n",
       "        {'title': 'LouisDumont/GCN---re-implementation',\n",
       "         'url': 'https://github.com/LouisDumont/GCN---re-implementation'}],\n",
       "       'metrics': {'Accuracy': '90.00%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GCNConv',\n",
       "       'paper_date': '2016-09-09',\n",
       "       'paper_title': 'Semi-Supervised Classification with Graph Convolutional Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1609.02907v4',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'JHMDB Pose Tracking',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-jhmdb'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['PCK@0.1',\n",
       "      'PCK@0.2',\n",
       "      'PCK@0.3',\n",
       "      'PCK@0.4',\n",
       "      'PCK@0.5'],\n",
       "     'rows': [{'code_links': [{'title': 'aimerykong/predictive-filter-flow',\n",
       "         'url': 'https://github.com/aimerykong/predictive-filter-flow'},\n",
       "        {'title': 'bestaar/predictiveFilterFlow',\n",
       "         'url': 'https://github.com/bestaar/predictiveFilterFlow'}],\n",
       "       'metrics': {'PCK@0.1': '58.4',\n",
       "        'PCK@0.2': '78.1',\n",
       "        'PCK@0.3': '85.9',\n",
       "        'PCK@0.4': '89.8',\n",
       "        'PCK@0.5': '92.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'mgPFF+ft 1st',\n",
       "       'paper_date': '2019-04-02',\n",
       "       'paper_title': 'Multigrid Predictive Filter Flow for Unsupervised Learning on Videos',\n",
       "       'paper_url': 'http://arxiv.org/abs/1904.01693v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'hyperparameters/tracking_via_colorization',\n",
       "         'url': 'https://github.com/hyperparameters/tracking_via_colorization'}],\n",
       "       'metrics': {'PCK@0.1': '45.2',\n",
       "        'PCK@0.2': '69.6',\n",
       "        'PCK@0.3': '80.8',\n",
       "        'PCK@0.4': '87.5',\n",
       "        'PCK@0.5': '91.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ColorPointer',\n",
       "       'paper_date': '2018-06-25',\n",
       "       'paper_title': 'Tracking Emerges by Colorizing Videos',\n",
       "       'paper_url': 'http://arxiv.org/abs/1806.09594v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'NVIDIA/flownet2-pytorch',\n",
       "         'url': 'https://github.com/NVIDIA/flownet2-pytorch'},\n",
       "        {'title': 'philferriere/tfoptflow',\n",
       "         'url': 'https://github.com/philferriere/tfoptflow'},\n",
       "        {'title': 'ElliotHYLee/VisualOdometry3D',\n",
       "         'url': 'https://github.com/ElliotHYLee/VisualOdometry3D'},\n",
       "        {'title': 'mcgridles/LENS',\n",
       "         'url': 'https://github.com/mcgridles/LENS'},\n",
       "        {'title': 'rickyHong/tfoptflow-repl',\n",
       "         'url': 'https://github.com/rickyHong/tfoptflow-repl'}],\n",
       "       'metrics': {'PCK@0.1': '45.2',\n",
       "        'PCK@0.2': '62.9',\n",
       "        'PCK@0.3': '73.5',\n",
       "        'PCK@0.4': '80.6',\n",
       "        'PCK@0.5': '85.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'FlowNet2',\n",
       "       'paper_date': '2016-12-06',\n",
       "       'paper_title': 'FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1612.01925v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'NTU RGB+D 120',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu-rgbd-1'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy (Cross-Subject)', 'Accuracy (Cross-Setup)'],\n",
       "     'rows': [{'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '89.1',\n",
       "        'Accuracy (Cross-Subject)': '88.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EfficientGCN-B4',\n",
       "       'paper_date': '2021-06-29',\n",
       "       'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '89.2%',\n",
       "        'Accuracy (Cross-Subject)': '88.2%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'AngNet-JA + BA + JBA + VJBA',\n",
       "       'paper_date': '2021-05-04',\n",
       "       'paper_title': 'Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2105.01563v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '88.0',\n",
       "        'Accuracy (Cross-Subject)': '87.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EfficientGCN-B2',\n",
       "       'paper_date': '2021-06-29',\n",
       "       'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/ResGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/ResGCNv1'},\n",
       "        {'title': 'yfsong0709/EfficientGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '88.3%',\n",
       "        'Accuracy (Cross-Subject)': '87.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PA-ResGCN-B19',\n",
       "       'paper_date': '2020-10-20',\n",
       "       'paper_title': 'Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2010.09978v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'skelemoa/quovadis',\n",
       "         'url': 'https://github.com/skelemoa/quovadis'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '88.8%',\n",
       "        'Accuracy (Cross-Subject)': '87.22%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Ensemble-top5 (MS-G3D Net + 4s Shift-GCN + VA-CNN (ResNeXt101) + 2s SDGCN + GCN-NAS (retrained))',\n",
       "       'paper_date': '2020-07-04',\n",
       "       'paper_title': 'Quo Vadis, Skeleton Action Recognition ?',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.02072v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'open-mmlab/mmaction2',\n",
       "         'url': 'https://github.com/open-mmlab/mmaction2'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '90.3',\n",
       "        'Accuracy (Cross-Subject)': '86.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PoseC3D (w. HRNet 2D Skeleton)',\n",
       "       'paper_date': '2021-04-28',\n",
       "       'paper_title': 'Revisiting Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2104.13586v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kenziyuliu/ms-g3d',\n",
       "         'url': 'https://github.com/kenziyuliu/ms-g3d'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '88.4%',\n",
       "        'Accuracy (Cross-Subject)': '86.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MS-G3D Net',\n",
       "       'paper_date': '2020-03-31',\n",
       "       'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.14111v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'lshiwjx/DSTA-Net',\n",
       "         'url': 'https://github.com/lshiwjx/DSTA-Net'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '89.0 %',\n",
       "        'Accuracy (Cross-Subject)': '86.6%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DSTA-Net',\n",
       "       'paper_date': '2020-07-07',\n",
       "       'paper_title': 'Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.03263v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'srijandas07/VPN',\n",
       "         'url': 'https://github.com/srijandas07/VPN'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '87.8',\n",
       "        'Accuracy (Cross-Subject)': '86.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VPN',\n",
       "       'paper_date': '2020-07-06',\n",
       "       'paper_title': 'VPN: Learning Video-Pose Embedding for Activities of Daily Living',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.03056v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kchengiva/Shift-GCN',\n",
       "         'url': 'https://github.com/kchengiva/Shift-GCN'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '87.6%',\n",
       "        'Accuracy (Cross-Subject)': '85.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': '4s Shift-GCN',\n",
       "       'paper_date': '2020-06-01',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '84.3',\n",
       "        'Accuracy (Cross-Subject)': '85.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EfficientGCN-B0',\n",
       "       'paper_date': '2021-06-29',\n",
       "       'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '87.4%',\n",
       "        'Accuracy (Cross-Subject)': '85.4%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'FGCN ',\n",
       "       'paper_date': '2020-03-17',\n",
       "       'paper_title': 'Feedback Graph Convolutional Network for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.07564v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '86.90',\n",
       "        'Accuracy (Cross-Subject)': '84.88'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VA-CNN (ResNeXt-101)',\n",
       "       'paper_date': None,\n",
       "       'paper_title': '',\n",
       "       'paper_url': '',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'Chiaraplizz/ST-TR',\n",
       "         'url': 'https://github.com/Chiaraplizz/ST-TR'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '84.7%',\n",
       "        'Accuracy (Cross-Subject)': '82.7%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-TR-agcn',\n",
       "       'paper_date': '2020-08-17',\n",
       "       'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/2008.07404v4',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/RA-GCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/RA-GCNv1'},\n",
       "        {'title': 'yfsong0709/RA-GCNv2',\n",
       "         'url': 'https://github.com/yfsong0709/RA-GCNv2'},\n",
       "        {'title': 'peter-yys-yoon/pegcnv2',\n",
       "         'url': 'https://github.com/peter-yys-yoon/pegcnv2'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '82.7%',\n",
       "        'Accuracy (Cross-Subject)': '81.1%'},\n",
       "       'model_links': [],\n",
       "       'model_name': '3s RA-GCN',\n",
       "       'paper_date': '2020-08-09',\n",
       "       'paper_title': 'Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2008.03791v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '83.2%',\n",
       "        'Accuracy (Cross-Subject)': '80.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Mix-Dimension',\n",
       "       'paper_date': '2020-07-30',\n",
       "       'paper_title': 'Mix Dimension in Poincar√© Geometry for 3D Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.15678v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '79.8%',\n",
       "        'Accuracy (Cross-Subject)': '78.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GVFE + AS-GCN with DH-TCN',\n",
       "       'paper_date': '2019-12-20',\n",
       "       'paper_title': 'Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.09745v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'airglow/gimme_signals_action_recognition',\n",
       "         'url': 'https://github.com/airglow/gimme_signals_action_recognition'},\n",
       "        {'title': 'raphaelmemmesheimer/gimme_signals_action_recognition',\n",
       "         'url': 'https://github.com/raphaelmemmesheimer/gimme_signals_action_recognition'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '71.6%',\n",
       "        'Accuracy (Cross-Subject)': '70.8%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Gimme Signals (Skeleton, AIS)',\n",
       "       'paper_date': '2020-03-13',\n",
       "       'paper_title': 'Gimme Signals: Discriminative signal encoding for multimodal activity recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.06156v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '67.2%',\n",
       "        'Accuracy (Cross-Subject)': '68.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Logsig-RNN',\n",
       "       'paper_date': '2019-08-22',\n",
       "       'paper_title': 'Learning stochastic differential equations using RNN with log signature features',\n",
       "       'paper_url': 'https://arxiv.org/abs/1908.08286v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "         'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '62.8%',\n",
       "        'Accuracy (Cross-Subject)': '67.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TSRJI (Late Fusion) + HCN',\n",
       "       'paper_date': '2019-09-11',\n",
       "       'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints',\n",
       "       'paper_url': 'https://arxiv.org/abs/1909.05704v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "         'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '66.9%',\n",
       "        'Accuracy (Cross-Subject)': '67.7%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SkeleMotion + Yang et al. (2018)',\n",
       "       'paper_date': '2019-07-30',\n",
       "       'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1907.13025v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "         'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '59.7%',\n",
       "        'Accuracy (Cross-Subject)': '65.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TSRJI (Late Fusion)',\n",
       "       'paper_date': '2019-09-11',\n",
       "       'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints',\n",
       "       'paper_url': 'https://arxiv.org/abs/1909.05704v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '66.9%',\n",
       "        'Accuracy (Cross-Subject)': '64.6%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Body Pose Evolution Map',\n",
       "       'paper_date': '2018-06-01',\n",
       "       'paper_title': 'Recognizing Human Actions as the Evolution of Pose Estimation Maps',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Recognizing_Human_Actions_CVPR_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "         'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '63.0%',\n",
       "        'Accuracy (Cross-Subject)': '62.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SkeleMotion [Magnitude-Orientation (TSA)]',\n",
       "       'paper_date': '2019-07-30',\n",
       "       'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1907.13025v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '61.8%',\n",
       "        'Accuracy (Cross-Subject)': '62.2%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Multi-Task CNN with RotClips',\n",
       "       'paper_date': '2018-03-05',\n",
       "       'paper_title': 'Learning clip representations for skeleton-based 3d action recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/TIP.2018.2812099',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '63.3%',\n",
       "        'Accuracy (Cross-Subject)': '61.2%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Two-Stream Attention LSTM',\n",
       "       'paper_date': '2017-07-18',\n",
       "       'paper_title': 'Skeleton-Based Human Action Recognition with Global Context-Aware Attention LSTM Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1707.05740v5',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '63.2%',\n",
       "        'Accuracy (Cross-Subject)': '60.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Skeleton Visualization (Single Stream)',\n",
       "       'paper_date': '2017-08-01',\n",
       "       'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition',\n",
       "       'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '62.4%',\n",
       "        'Accuracy (Cross-Subject)': '59.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'FSNet',\n",
       "       'paper_date': '2019-02-08',\n",
       "       'paper_title': 'Skeleton-Based Online Action Prediction Using Scale Selection Network',\n",
       "       'paper_url': 'http://arxiv.org/abs/1902.03084v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '57.9%',\n",
       "        'Accuracy (Cross-Subject)': '58.4%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Multi-Task Learning Network',\n",
       "       'paper_date': '2017-03-09',\n",
       "       'paper_title': 'A New Representation of Skeleton Sequences for 3D Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1703.03492v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '59.2%',\n",
       "        'Accuracy (Cross-Subject)': '58.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GCA-LSTM',\n",
       "       'paper_date': '2017-07-01',\n",
       "       'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '60.9%',\n",
       "        'Accuracy (Cross-Subject)': '58.2%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Internal Feature Fusion',\n",
       "       'paper_date': '2017-06-26',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates',\n",
       "       'paper_url': 'http://arxiv.org/abs/1706.08276v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '57.9%',\n",
       "        'Accuracy (Cross-Subject)': '55.7%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Spatio-Temporal LSTM',\n",
       "       'paper_date': '2016-07-24',\n",
       "       'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '54.7%',\n",
       "        'Accuracy (Cross-Subject)': '50.8%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Dynamic Skeletons',\n",
       "       'paper_date': '2016-12-15',\n",
       "       'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '44.9%',\n",
       "        'Accuracy (Cross-Subject)': '36.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Soft RNN',\n",
       "       'paper_date': '2018-08-06',\n",
       "       'paper_title': 'Early action prediction by soft regression',\n",
       "       'paper_url': 'https://doi.org/10.1109/TPAMI.2018.2863279',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "         'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "       'metrics': {'Accuracy (Cross-Setup)': '26.3%',\n",
       "        'Accuracy (Cross-Subject)': '25.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Part-Aware LSTM',\n",
       "       'paper_date': '2016-04-11',\n",
       "       'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "       'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'HDM05',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-hdm05'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "         'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "       'metrics': {'Accuracy': '89.80%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Temporal Subspace Clustering',\n",
       "       'paper_date': '2020-06-21',\n",
       "       'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "       'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'Gaming 3D (G3D)',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-gaming'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [],\n",
       "       'metrics': {'Accuracy': '96.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'CNN',\n",
       "       'paper_date': '2016-12-30',\n",
       "       'paper_title': 'Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1612.09401v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "         'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "       'metrics': {'Accuracy': '92.91%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Temporal K-Means Clustering + Temporal Covariance Subspace Clustering',\n",
       "       'paper_date': '2020-06-21',\n",
       "       'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "       'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'rort1989/HDM',\n",
       "         'url': 'https://github.com/rort1989/HDM'}],\n",
       "       'metrics': {'Accuracy': '92.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'HDM-BG',\n",
       "       'paper_date': '2019-06-01',\n",
       "       'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '90.94'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Rolling Rotations (FTP)',\n",
       "       'paper_date': '2016-06-27',\n",
       "       'paper_title': 'Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data',\n",
       "       'paper_url': 'https://doi.org/10.1109/CVPR.2016.484',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'UWA3D',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-uwa3d'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "         'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy': '81.4%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VA-fusion (aug.)',\n",
       "       'paper_date': '2018-04-20',\n",
       "       'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '73.8%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ESV (Synthesized + Pre-trained)',\n",
       "       'paper_date': '2017-08-01',\n",
       "       'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition',\n",
       "       'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '17.7%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'HOJ3D',\n",
       "       'paper_date': '2012-07-16',\n",
       "       'paper_title': 'View invariant human action recognition using histograms of 3D joints',\n",
       "       'paper_url': 'https://doi.org/10.1109/CVPRW.2012.6239233',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'MSRC-12',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msrc-12'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "         'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "       'metrics': {'Accuracy': '99.08%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Temporal Subspace Clustering',\n",
       "       'paper_date': '2020-06-21',\n",
       "       'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "       'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'Kinetics-Skeleton dataset',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-kinetics'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'open-mmlab/mmaction2',\n",
       "         'url': 'https://github.com/open-mmlab/mmaction2'}],\n",
       "       'metrics': {'Accuracy': '47.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PoseC3D (w. HRNet 2D skeleton)',\n",
       "       'paper_date': '2021-04-28',\n",
       "       'paper_title': 'Revisiting Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2104.13586v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '38.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': '2s-AGCN+TEM',\n",
       "       'paper_date': '2020-03-19',\n",
       "       'paper_title': 'Temporal Extension Module for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.08951v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kenziyuliu/ms-g3d',\n",
       "         'url': 'https://github.com/kenziyuliu/ms-g3d'}],\n",
       "       'metrics': {'Accuracy': '38.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MS-G3D',\n",
       "       'paper_date': '2020-03-31',\n",
       "       'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.14111v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'open-mmlab/mmaction2',\n",
       "         'url': 'https://github.com/open-mmlab/mmaction2'}],\n",
       "       'metrics': {'Accuracy': '38.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PoseC3D',\n",
       "       'paper_date': '2021-04-28',\n",
       "       'paper_title': 'Revisiting Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2104.13586v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '37.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Dynamic GCN',\n",
       "       'paper_date': '2020-07-29',\n",
       "       'paper_title': 'Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.14690v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "         'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy': '37.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MS-AAGCN',\n",
       "       'paper_date': '2019-12-15',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.06971v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '37.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'CGCN',\n",
       "       'paper_date': '2020-03-06',\n",
       "       'paper_title': 'Centrality Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.03007v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "         'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy': '37.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'JB-AAGCN',\n",
       "       'paper_date': '2019-12-15',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.06971v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'Chiaraplizz/ST-TR',\n",
       "         'url': 'https://github.com/Chiaraplizz/ST-TR'}],\n",
       "       'metrics': {'Accuracy': '37.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-TR-agcn',\n",
       "       'paper_date': '2020-08-17',\n",
       "       'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/2008.07404v4',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'xiaoiker/GCN-NAS',\n",
       "         'url': 'https://github.com/xiaoiker/GCN-NAS'}],\n",
       "       'metrics': {'Accuracy': '37.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GCN-NAS',\n",
       "       'paper_date': '2019-11-11',\n",
       "       'paper_title': 'Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching',\n",
       "       'paper_url': 'https://arxiv.org/abs/1911.04131v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch',\n",
       "         'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}],\n",
       "       'metrics': {'Accuracy': '36.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DGNN',\n",
       "       'paper_date': '2019-06-01',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '36.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SLnL-rFA',\n",
       "       'paper_date': '2018-11-10',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention',\n",
       "       'paper_url': 'https://arxiv.org/abs/1811.04237v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal',\n",
       "         'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'},\n",
       "        {'title': 'lshiwjx/2s-AGCN',\n",
       "         'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy': '36.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': '2s-AGCN',\n",
       "       'paper_date': '2018-05-20',\n",
       "       'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1805.07694v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'limaosen0/AS-GCN',\n",
       "         'url': 'https://github.com/limaosen0/AS-GCN'}],\n",
       "       'metrics': {'Accuracy': '34.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'AS-GCN',\n",
       "       'paper_date': '2019-04-26',\n",
       "       'paper_title': 'Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1904.12659v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'andreYoo/PeGCNs',\n",
       "         'url': 'https://github.com/andreYoo/PeGCNs'}],\n",
       "       'metrics': {'Accuracy': '34.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PeGCN',\n",
       "       'paper_date': '2020-03-17',\n",
       "       'paper_title': 'Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.07514v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '33.7 '},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PR-GCN',\n",
       "       'paper_date': '2020-10-14',\n",
       "       'paper_title': 'Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2010.07367v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '33.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-GR',\n",
       "       'paper_date': '2019-07-17',\n",
       "       'paper_title': 'Spatiotemporal graph routing for skeleton-based action recognition',\n",
       "       'paper_url': 'https://doi.org/10.1609/aaai.v33i01.33018561',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '33.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'AR-GCN',\n",
       "       'paper_date': '2019-11-27',\n",
       "       'paper_title': 'An Attention-Enhanced Recurrent Graph Convolutional Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://doi.org/10.1145/3372806.3372814',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'raymondyeh07/chirality_nets',\n",
       "         'url': 'https://github.com/raymondyeh07/chirality_nets'}],\n",
       "       'metrics': {'Accuracy': '30.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Ours-Conv-Chiral',\n",
       "       'paper_date': '2019-10-31',\n",
       "       'paper_title': 'Chirality Nets for Human Pose Regression',\n",
       "       'paper_url': 'https://arxiv.org/abs/1911.00029v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'open-mmlab/mmskeleton',\n",
       "         'url': 'https://github.com/open-mmlab/mmskeleton'},\n",
       "        {'title': 'yysijie/st-gcn',\n",
       "         'url': 'https://github.com/yysijie/st-gcn'},\n",
       "        {'title': 'ericksiavichay/cs230-final-project',\n",
       "         'url': 'https://github.com/ericksiavichay/cs230-final-project'},\n",
       "        {'title': 'XinzeWu/st-GCN',\n",
       "         'url': 'https://github.com/XinzeWu/st-GCN'},\n",
       "        {'title': 'stillarrow/S2VT_ACT',\n",
       "         'url': 'https://github.com/stillarrow/S2VT_ACT'},\n",
       "        {'title': 'ZhangNYG/ST-GCN',\n",
       "         'url': 'https://github.com/ZhangNYG/ST-GCN'},\n",
       "        {'title': 'DixinFan/st-gcn',\n",
       "         'url': 'https://github.com/DixinFan/st-gcn'},\n",
       "        {'title': 'ken724049/action-recognition',\n",
       "         'url': 'https://github.com/ken724049/action-recognition'},\n",
       "        {'title': 'antoniolq/st-gcn',\n",
       "         'url': 'https://github.com/antoniolq/st-gcn'},\n",
       "        {'title': 'github-zbx/ST-GCN',\n",
       "         'url': 'https://github.com/github-zbx/ST-GCN'},\n",
       "        {'title': 'GeyuanZhang/st-gcn-master',\n",
       "         'url': 'https://github.com/GeyuanZhang/st-gcn-master'},\n",
       "        {'title': 'KrisLee512/ST-GCN',\n",
       "         'url': 'https://github.com/KrisLee512/ST-GCN'},\n",
       "        {'title': 'AbiterVX/ST-GCN',\n",
       "         'url': 'https://github.com/AbiterVX/ST-GCN'}],\n",
       "       'metrics': {'Accuracy': '30.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-GCN',\n",
       "       'paper_date': '2018-01-23',\n",
       "       'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1801.07455v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition',\n",
       "         'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}],\n",
       "       'metrics': {'Accuracy': '20.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Res-TCN',\n",
       "       'paper_date': '2017-04-14',\n",
       "       'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.04516v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'J-HMBD Early Action',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-j-hmbd'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['10%'],\n",
       "     'rows': [{'code_links': [{'title': 'ser-art/RAE-vs-AE',\n",
       "         'url': 'https://github.com/ser-art/RAE-vs-AE'},\n",
       "        {'title': 'rk68657/AutoEncoders',\n",
       "         'url': 'https://github.com/rk68657/AutoEncoders'}],\n",
       "       'metrics': {'10%': '60.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DR^2N',\n",
       "       'paper_date': '2018-02-09',\n",
       "       'paper_title': 'Relational Autoencoder for Feature Extraction',\n",
       "       'paper_url': 'http://arxiv.org/abs/1802.03145v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'labmlai/annotated_deep_learning_paper_implementations',\n",
       "         'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/graphs/gat'},\n",
       "        {'title': 'PetarV-/GAT', 'url': 'https://github.com/PetarV-/GAT'},\n",
       "        {'title': 'Diego999/pyGAT',\n",
       "         'url': 'https://github.com/Diego999/pyGAT'},\n",
       "        {'title': 'gordicaleksa/pytorch-GAT',\n",
       "         'url': 'https://github.com/gordicaleksa/pytorch-GAT'},\n",
       "        {'title': 'shenweichen/GraphNeuralNetwork',\n",
       "         'url': 'https://github.com/shenweichen/GraphNeuralNetwork'},\n",
       "        {'title': 'danielegrattarola/keras-gat',\n",
       "         'url': 'https://github.com/danielegrattarola/keras-gat'},\n",
       "        {'title': 'HazyResearch/hgcn',\n",
       "         'url': 'https://github.com/HazyResearch/hgcn'},\n",
       "        {'title': 'GraphSAINT/GraphSAINT',\n",
       "         'url': 'https://github.com/GraphSAINT/GraphSAINT'},\n",
       "        {'title': 'lukecavabarrett/pna',\n",
       "         'url': 'https://github.com/lukecavabarrett/pna'},\n",
       "        {'title': 'Kaimaoge/IGNNK',\n",
       "         'url': 'https://github.com/Kaimaoge/IGNNK'},\n",
       "        {'title': 'zhao-tong/GNNs-easy-to-use',\n",
       "         'url': 'https://github.com/zhao-tong/GNNs-easy-to-use'},\n",
       "        {'title': 'snowkylin/gnn', 'url': 'https://github.com/snowkylin/gnn'},\n",
       "        {'title': 'marblet/GNN_models_pytorch_geometric',\n",
       "         'url': 'https://github.com/marblet/GNN_models_pytorch_geometric'},\n",
       "        {'title': 'marble0117/GNN_models_pytorch',\n",
       "         'url': 'https://github.com/marble0117/GNN_models_pytorch'},\n",
       "        {'title': 'marble0117/GNN_models_pytorch_geometric',\n",
       "         'url': 'https://github.com/marble0117/GNN_models_pytorch_geometric'},\n",
       "        {'title': 'HeapHop30/graph-attention-nets',\n",
       "         'url': 'https://github.com/HeapHop30/graph-attention-nets'},\n",
       "        {'title': 'weiyangfb/PyTorchSparseGAT',\n",
       "         'url': 'https://github.com/weiyangfb/PyTorchSparseGAT'},\n",
       "        {'title': 'marblet/gat-pytorch',\n",
       "         'url': 'https://github.com/marblet/gat-pytorch'},\n",
       "        {'title': 'ds4dm/sparse-gcn',\n",
       "         'url': 'https://github.com/ds4dm/sparse-gcn'},\n",
       "        {'title': 'ds4dm/sGat', 'url': 'https://github.com/ds4dm/sGat'},\n",
       "        {'title': 'gcucurull/jax-gat',\n",
       "         'url': 'https://github.com/gcucurull/jax-gat'},\n",
       "        {'title': 'calciver/Graph-Attention-Networks',\n",
       "         'url': 'https://github.com/calciver/Graph-Attention-Networks'},\n",
       "        {'title': 'noahtren/Graph-Attention-Networks-TensorFlow-2',\n",
       "         'url': 'https://github.com/noahtren/Graph-Attention-Networks-TensorFlow-2'},\n",
       "        {'title': 'Yindong-Zhang/myGAT',\n",
       "         'url': 'https://github.com/Yindong-Zhang/myGAT'},\n",
       "        {'title': 'liu6zijian/simplified-gcn-model',\n",
       "         'url': 'https://github.com/liu6zijian/simplified-gcn-model'},\n",
       "        {'title': 'taishan1994/pytorch_gat',\n",
       "         'url': 'https://github.com/taishan1994/pytorch_gat'},\n",
       "        {'title': 'BIG-S2/keras-gnm',\n",
       "         'url': 'https://github.com/BIG-S2/keras-gnm'},\n",
       "        {'title': 'Aveek-Saha/Graph-Attention-Net',\n",
       "         'url': 'https://github.com/Aveek-Saha/Graph-Attention-Net'},\n",
       "        {'title': 'zxhhh97/ABot', 'url': 'https://github.com/zxhhh97/ABot'},\n",
       "        {'title': 'qema/orca-py', 'url': 'https://github.com/qema/orca-py'},\n",
       "        {'title': 'mitya8128/experiments_notes',\n",
       "         'url': 'https://github.com/mitya8128/experiments_notes'},\n",
       "        {'title': 'giuseppefutia/link-prediction-code',\n",
       "         'url': 'https://github.com/giuseppefutia/link-prediction-code'},\n",
       "        {'title': 'AngusMonroe/GAT-pytorch',\n",
       "         'url': 'https://github.com/AngusMonroe/GAT-pytorch'},\n",
       "        {'title': 'handasontam/GAT-with-edgewise-attention',\n",
       "         'url': 'https://github.com/handasontam/GAT-with-edgewise-attention'},\n",
       "        {'title': 'fongyk/graph-attention',\n",
       "         'url': 'https://github.com/fongyk/graph-attention'},\n",
       "        {'title': 'mlzxzhou/keras-gnm',\n",
       "         'url': 'https://github.com/mlzxzhou/keras-gnm'},\n",
       "        {'title': 'YunseobShin/wiki_GAT',\n",
       "         'url': 'https://github.com/YunseobShin/wiki_GAT'},\n",
       "        {'title': 'dzb1998/pyGAT', 'url': 'https://github.com/dzb1998/pyGAT'},\n",
       "        {'title': 'Anou9531/GAT', 'url': 'https://github.com/Anou9531/GAT'},\n",
       "        {'title': 'WantingZhao/my_GAT',\n",
       "         'url': 'https://github.com/WantingZhao/my_GAT'},\n",
       "        {'title': 'TyngJiunKuo/deep-learning-project',\n",
       "         'url': 'https://github.com/TyngJiunKuo/deep-learning-project'},\n",
       "        {'title': 'iwzy7071/graph_neural_network',\n",
       "         'url': 'https://github.com/iwzy7071/graph_neural_network'},\n",
       "        {'title': 'PumpkinYing/GAT',\n",
       "         'url': 'https://github.com/PumpkinYing/GAT'},\n",
       "        {'title': 'anish-lu-yihe/SVRT-by-GAT',\n",
       "         'url': 'https://github.com/anish-lu-yihe/SVRT-by-GAT'},\n",
       "        {'title': 'ChengSashankh/trying-GAT',\n",
       "         'url': 'https://github.com/ChengSashankh/trying-GAT'},\n",
       "        {'title': 'subercui/pyGConvAT',\n",
       "         'url': 'https://github.com/subercui/pyGConvAT'},\n",
       "        {'title': 'ChengyuSun/gat',\n",
       "         'url': 'https://github.com/ChengyuSun/gat'},\n",
       "        {'title': 'blueberryc/pyGAT',\n",
       "         'url': 'https://github.com/blueberryc/pyGAT'},\n",
       "        {'title': 'zhangbo2008/GAT_network',\n",
       "         'url': 'https://github.com/zhangbo2008/GAT_network'},\n",
       "        {'title': 'whut2962575697/gat_sementic_segmentation',\n",
       "         'url': 'https://github.com/whut2962575697/gat_sementic_segmentation'},\n",
       "        {'title': 'RJ12138/Multilevel',\n",
       "         'url': 'https://github.com/RJ12138/Multilevel'},\n",
       "        {'title': 'galkampel/HyperNetworks',\n",
       "         'url': 'https://github.com/galkampel/HyperNetworks'},\n",
       "        {'title': 'tlmakinen/GAT-noise',\n",
       "         'url': 'https://github.com/tlmakinen/GAT-noise'},\n",
       "        {'title': 'Anak2016/GAT', 'url': 'https://github.com/Anak2016/GAT'},\n",
       "        {'title': 'snownus/COOP', 'url': 'https://github.com/snownus/COOP'},\n",
       "        {'title': 'ColdenChan/GAT_D',\n",
       "         'url': 'https://github.com/ColdenChan/GAT_D'}],\n",
       "       'metrics': {'10%': '58.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GAT',\n",
       "       'paper_date': '2017-10-30',\n",
       "       'paper_title': 'Graph Attention Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1710.10903v3',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'Varying-view RGB-D Action-Skeleton',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-varying'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy (CS)',\n",
       "      'Accuracy (CV I)',\n",
       "      'Accuracy (CV II)',\n",
       "      'Accuracy (AV I)',\n",
       "      'Accuracy (AV II)'],\n",
       "     'rows': [{'code_links': [],\n",
       "       'metrics': {'Accuracy (AV I)': '57%',\n",
       "        'Accuracy (AV II)': '75%',\n",
       "        'Accuracy (CS)': '76%',\n",
       "        'Accuracy (CV I)': '29%',\n",
       "        'Accuracy (CV II)': '71%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VS-CNN',\n",
       "       'paper_date': '2019-04-24',\n",
       "       'paper_title': 'A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1904.10681v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'open-mmlab/mmskeleton',\n",
       "         'url': 'https://github.com/open-mmlab/mmskeleton'},\n",
       "        {'title': 'yysijie/st-gcn',\n",
       "         'url': 'https://github.com/yysijie/st-gcn'},\n",
       "        {'title': 'ericksiavichay/cs230-final-project',\n",
       "         'url': 'https://github.com/ericksiavichay/cs230-final-project'},\n",
       "        {'title': 'XinzeWu/st-GCN',\n",
       "         'url': 'https://github.com/XinzeWu/st-GCN'},\n",
       "        {'title': 'stillarrow/S2VT_ACT',\n",
       "         'url': 'https://github.com/stillarrow/S2VT_ACT'},\n",
       "        {'title': 'ZhangNYG/ST-GCN',\n",
       "         'url': 'https://github.com/ZhangNYG/ST-GCN'},\n",
       "        {'title': 'DixinFan/st-gcn',\n",
       "         'url': 'https://github.com/DixinFan/st-gcn'},\n",
       "        {'title': 'ken724049/action-recognition',\n",
       "         'url': 'https://github.com/ken724049/action-recognition'},\n",
       "        {'title': 'antoniolq/st-gcn',\n",
       "         'url': 'https://github.com/antoniolq/st-gcn'},\n",
       "        {'title': 'github-zbx/ST-GCN',\n",
       "         'url': 'https://github.com/github-zbx/ST-GCN'},\n",
       "        {'title': 'GeyuanZhang/st-gcn-master',\n",
       "         'url': 'https://github.com/GeyuanZhang/st-gcn-master'},\n",
       "        {'title': 'KrisLee512/ST-GCN',\n",
       "         'url': 'https://github.com/KrisLee512/ST-GCN'},\n",
       "        {'title': 'AbiterVX/ST-GCN',\n",
       "         'url': 'https://github.com/AbiterVX/ST-GCN'}],\n",
       "       'metrics': {'Accuracy (AV I)': '53%',\n",
       "        'Accuracy (AV II)': '43%',\n",
       "        'Accuracy (CS)': '71%',\n",
       "        'Accuracy (CV I)': '25%',\n",
       "        'Accuracy (CV II)': '56%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-GCN',\n",
       "       'paper_date': '2018-01-23',\n",
       "       'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1801.07455v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition',\n",
       "         'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}],\n",
       "       'metrics': {'Accuracy (AV I)': '48%',\n",
       "        'Accuracy (AV II)': '68%',\n",
       "        'Accuracy (CS)': '63%',\n",
       "        'Accuracy (CV I)': '14%',\n",
       "        'Accuracy (CV II)': '48%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Res-TCN',\n",
       "       'paper_date': '2017-04-14',\n",
       "       'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.04516v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "         'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "       'metrics': {'Accuracy (AV I)': '33%',\n",
       "        'Accuracy (AV II)': '50%',\n",
       "        'Accuracy (CS)': '60%',\n",
       "        'Accuracy (CV I)': '13%',\n",
       "        'Accuracy (CV II)': '33%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'P-LSTM',\n",
       "       'paper_date': '2016-04-11',\n",
       "       'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "       'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (AV I)': '43%',\n",
       "        'Accuracy (AV II)': '77%',\n",
       "        'Accuracy (CS)': '59%',\n",
       "        'Accuracy (CV I)': '26%',\n",
       "        'Accuracy (CV II)': '68%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SK-CNN',\n",
       "       'paper_date': '2017-08-01',\n",
       "       'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition',\n",
       "       'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'coderSkyChen/Action_Recognition_Zoo',\n",
       "         'url': 'https://github.com/coderSkyChen/Action_Recognition_Zoo'},\n",
       "        {'title': 'colincsl/TemporalConvolutionalNetworks',\n",
       "         'url': 'https://github.com/colincsl/TemporalConvolutionalNetworks'},\n",
       "        {'title': 'yz-cnsdqz/TemporalActionParsing-FineGrained',\n",
       "         'url': 'https://github.com/yz-cnsdqz/TemporalActionParsing-FineGrained'},\n",
       "        {'title': 'sadari1/TumorDetectionDeepLearning',\n",
       "         'url': 'https://github.com/sadari1/TumorDetectionDeepLearning'},\n",
       "        {'title': 'BehnooshParsa/HumanActionRecognition_with_ErgonomicRisk',\n",
       "         'url': 'https://github.com/BehnooshParsa/HumanActionRecognition_with_ErgonomicRisk'}],\n",
       "       'metrics': {'Accuracy (AV I)': '43%',\n",
       "        'Accuracy (AV II)': '64%',\n",
       "        'Accuracy (CS)': '56%',\n",
       "        'Accuracy (CV I)': '16%',\n",
       "        'Accuracy (CV II)': '43%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TCN',\n",
       "       'paper_date': '2016-11-16',\n",
       "       'paper_title': 'Temporal Convolutional Networks for Action Segmentation and Detection',\n",
       "       'paper_url': 'http://arxiv.org/abs/1611.05267v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "         'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "       'metrics': {'Accuracy (AV I)': '31%',\n",
       "        'Accuracy (AV II)': '68%',\n",
       "        'Accuracy (CS)': '56%',\n",
       "        'Accuracy (CV I)': '16%',\n",
       "        'Accuracy (CV II)': '31%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'LSTM',\n",
       "       'paper_date': '2016-04-11',\n",
       "       'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "       'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'Florence 3D',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-florence'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [],\n",
       "       'metrics': {'Accuracy': '99.1%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Deep STGC_K',\n",
       "       'paper_date': '2018-02-27',\n",
       "       'paper_title': 'Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1802.09834v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '98.4%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Complete GR-GCN',\n",
       "       'paper_date': '2018-11-29',\n",
       "       'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "       'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "         'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "       'metrics': {'Accuracy': '95.81%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Temporal Spectral Clustering + Temporal Subspace Clustering',\n",
       "       'paper_date': '2020-06-21',\n",
       "       'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "       'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '91.40%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Rolling Rotations (FTP)',\n",
       "       'paper_date': '2016-06-27',\n",
       "       'paper_title': 'Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data',\n",
       "       'paper_url': 'https://doi.org/10.1109/CVPR.2016.484',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'fdsa1860/skeletal',\n",
       "         'url': 'https://github.com/fdsa1860/skeletal'}],\n",
       "       'metrics': {'Accuracy': '90.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Lie Group',\n",
       "       'paper_date': '2014-06-23',\n",
       "       'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group',\n",
       "       'paper_url': 'https://doi.org/10.1109/CVPR.2014.82',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'NTU60-X',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu60-x'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy (Body + Fingers joints)',\n",
       "      'Accuracy (Body joints)',\n",
       "      'Accuracy (Body + Fingers + Face joints)'],\n",
       "     'rows': [{'code_links': [{'title': 'skelemoa/ntu-x',\n",
       "         'url': 'https://github.com/skelemoa/ntu-x'}],\n",
       "       'metrics': {'Accuracy (Body + Fingers + Face joints)': '89.64',\n",
       "        'Accuracy (Body + Fingers joints)': '91.78',\n",
       "        'Accuracy (Body joints)': '89.56'},\n",
       "       'model_links': [],\n",
       "       'model_name': '4s-ShiftGCN',\n",
       "       'paper_date': '2021-01-27',\n",
       "       'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions',\n",
       "       'paper_url': 'https://arxiv.org/abs/2101.11529v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'skelemoa/ntu-x',\n",
       "         'url': 'https://github.com/skelemoa/ntu-x'}],\n",
       "       'metrics': {'Accuracy (Body + Fingers + Face joints)': '91.12',\n",
       "        'Accuracy (Body + Fingers joints)': '91.76',\n",
       "        'Accuracy (Body joints)': '91.26'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MS-G3D',\n",
       "       'paper_date': '2021-01-27',\n",
       "       'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions',\n",
       "       'paper_url': 'https://arxiv.org/abs/2101.11529v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'skelemoa/ntu-x',\n",
       "         'url': 'https://github.com/skelemoa/ntu-x'}],\n",
       "       'metrics': {'Accuracy (Body + Fingers + Face joints)': '89.79',\n",
       "        'Accuracy (Body + Fingers joints)': '91.64',\n",
       "        'Accuracy (Body joints)': '89.98'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PA-ResGCN',\n",
       "       'paper_date': '2021-01-27',\n",
       "       'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions',\n",
       "       'paper_url': 'https://arxiv.org/abs/2101.11529v2',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'TCG-dataset',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-tcg'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Acc'],\n",
       "     'rows': [{'code_links': [{'title': 'againerju/tcg_recognition',\n",
       "         'url': 'https://github.com/againerju/tcg_recognition'}],\n",
       "       'metrics': {'Acc': '87.24'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Bidirectional LSTM',\n",
       "       'paper_date': '2020-07-31',\n",
       "       'paper_title': 'Traffic Control Gesture Recognition for Autonomous Vehicles',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.16072v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'SHREC 2017 track on 3D Hand Gesture Recognition',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-shrec'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['28 gestures accuracy',\n",
       "      'Accuracy',\n",
       "      '14 gestures accuracy',\n",
       "      'No. parameters',\n",
       "      'Speed  (FPS)'],\n",
       "     'rows': [{'code_links': [{'title': 'fandulu/DD-Net',\n",
       "         'url': 'https://github.com/fandulu/DD-Net'},\n",
       "        {'title': 'BlurryLight/DD-Net-Pytorch',\n",
       "         'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}],\n",
       "       'metrics': {'14 gestures accuracy': '94.6',\n",
       "        '28 gestures accuracy': '91.9',\n",
       "        'Accuracy': '94.6 (14  gestures) , 91.9 (28 gestures )',\n",
       "        'No. parameters': '1.82M',\n",
       "        'Speed  (FPS)': '2,200'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DD-Net',\n",
       "       'paper_date': '2019-07-23',\n",
       "       'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better',\n",
       "       'paper_url': 'https://arxiv.org/abs/1907.09658v8',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition',\n",
       "         'url': 'https://github.com/AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition'}],\n",
       "       'metrics': {'14 gestures accuracy': '93.57',\n",
       "        '28 gestures accuracy': '91.43'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TCN-Summ',\n",
       "       'paper_date': '2021-03-03',\n",
       "       'paper_title': 'Domain and View-point Agnostic Hand Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2103.02303v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'14 gestures accuracy': '93.6',\n",
       "        '28 gestures accuracy': '90.7',\n",
       "        'Speed  (FPS)': '161'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'STA-Res-TCN',\n",
       "       'paper_date': '2019-01-23',\n",
       "       'paper_title': 'Spatial-Temporal Attention Res-TCN for Skeleton-Based Dynamic Hand Gesture Recognition',\n",
       "       'paper_url': 'https://link.springer.com/chapter/10.1007/978-3-030-11024-6_18',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'14 gestures accuracy': '91.3',\n",
       "        '28 gestures accuracy': '86.6',\n",
       "        'Speed  (FPS)': '361'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MFA-Net',\n",
       "       'paper_date': '2019-01-10',\n",
       "       'paper_title': 'Motion feature augmented network for dynamic hand gesture recognition from skeletal data',\n",
       "       'paper_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6359639/',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'NTU RGB+D',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu-rgbd'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy (CV)', 'Accuracy (CS)'],\n",
       "     'rows': [{'code_links': [{'title': 'open-mmlab/mmaction2',\n",
       "         'url': 'https://github.com/open-mmlab/mmaction2'}],\n",
       "       'metrics': {'Accuracy (CS)': '94.1', 'Accuracy (CV)': '97.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PoseC3D (w. HRNet 2D skeleton)',\n",
       "       'paper_date': '2021-04-28',\n",
       "       'paper_title': 'Revisiting Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2104.13586v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '91.0', 'Accuracy (CV)': '96.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MS-AAGCN+TEM',\n",
       "       'paper_date': '2020-03-19',\n",
       "       'paper_title': 'Temporal Extension Module for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.08951v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kchengiva/Shift-GCN',\n",
       "         'url': 'https://github.com/kchengiva/Shift-GCN'}],\n",
       "       'metrics': {'Accuracy (CS)': '90.7', 'Accuracy (CV)': '96.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': '4s Shift-GCN',\n",
       "       'paper_date': '2020-06-01',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '91.7', 'Accuracy (CV)': '96.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'AngNet-JA + BA + JBA + VJBA',\n",
       "       'paper_date': '2021-05-04',\n",
       "       'paper_title': 'Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2105.01563v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'lshiwjx/DSTA-Net',\n",
       "         'url': 'https://github.com/lshiwjx/DSTA-Net'}],\n",
       "       'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DSTA-Net',\n",
       "       'paper_date': '2020-07-07',\n",
       "       'paper_title': 'Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.03263v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '90.3', 'Accuracy (CV)': '96.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'CGCN',\n",
       "       'paper_date': '2020-03-06',\n",
       "       'paper_title': 'Centrality Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.03007v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '90.1', 'Accuracy (CV)': '96.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Sym-GNN',\n",
       "       'paper_date': '2019-10-05',\n",
       "       'paper_title': 'Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction',\n",
       "       'paper_url': 'https://arxiv.org/abs/1910.02212v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '90.3', 'Accuracy (CV)': '96.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'BAGCN',\n",
       "       'paper_date': '2019-12-24',\n",
       "       'paper_title': 'Focusing and Diffusion: Bidirectional Attentive Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.11521v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '90.2', 'Accuracy (CV)': '96.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'FGCN-spatial+FGCN-motion',\n",
       "       'paper_date': '2020-03-17',\n",
       "       'paper_title': 'Feedback Graph Convolutional Network for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.07564v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kenziyuliu/ms-g3d',\n",
       "         'url': 'https://github.com/kenziyuliu/ms-g3d'}],\n",
       "       'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MS-G3D Net',\n",
       "       'paper_date': '2020-03-31',\n",
       "       'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.14111v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "         'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy (CS)': '90.0', 'Accuracy (CV)': '96.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MS-AAGCN',\n",
       "       'paper_date': '2019-12-15',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.06971v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch',\n",
       "         'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}],\n",
       "       'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '96.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DGNN',\n",
       "       'paper_date': '2019-06-01',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'Chiaraplizz/ST-TR',\n",
       "         'url': 'https://github.com/Chiaraplizz/ST-TR'}],\n",
       "       'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '96.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-TR',\n",
       "       'paper_date': '2020-08-17',\n",
       "       'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/2008.07404v4',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Dynamic GCN',\n",
       "       'paper_date': '2020-07-29',\n",
       "       'paper_title': 'Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.14690v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/ResGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/ResGCNv1'},\n",
       "        {'title': 'yfsong0709/EfficientGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "       'metrics': {'Accuracy (CS)': '90.9', 'Accuracy (CV)': '96'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PA-ResGCN-B19',\n",
       "       'paper_date': '2020-10-20',\n",
       "       'paper_title': 'Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2010.09978v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '89.7', 'Accuracy (CV)': '96'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Mix-Dimension',\n",
       "       'paper_date': '2020-07-30',\n",
       "       'paper_title': 'Mix Dimension in Poincar√© Geometry for 3D Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.15678v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "         'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '96.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'JB-AAGCN',\n",
       "       'paper_date': '2019-12-15',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.06971v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '89.58', 'Accuracy (CV)': '95.74'},\n",
       "       'model_links': [],\n",
       "       'model_name': '2s-SDGCN',\n",
       "       'paper_date': '2019-10-27',\n",
       "       'paper_title': 'Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_ICCVW_2019/html/SGRL/Wu_Spatial_Residual_Layer_and_Dense_Connection_Block_Enhanced_Spatial_Temporal_ICCVW_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "       'metrics': {'Accuracy (CS)': '91.7', 'Accuracy (CV)': '95.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EfficientGCN-B4',\n",
       "       'paper_date': '2021-06-29',\n",
       "       'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'xiaoiker/GCN-NAS',\n",
       "         'url': 'https://github.com/xiaoiker/GCN-NAS'}],\n",
       "       'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '95.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GCN-NAS',\n",
       "       'paper_date': '2019-11-11',\n",
       "       'paper_title': 'Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching',\n",
       "       'paper_url': 'https://arxiv.org/abs/1911.04131v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "       'metrics': {'Accuracy (CS)': '90.9', 'Accuracy (CV)': '95.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EfficientGCN-B2',\n",
       "       'paper_date': '2021-06-29',\n",
       "       'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal',\n",
       "         'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'},\n",
       "        {'title': 'lshiwjx/2s-AGCN',\n",
       "         'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy (CS)': '88.5', 'Accuracy (CV)': '95.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': '2s-AGCN',\n",
       "       'paper_date': '2018-05-20',\n",
       "       'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1805.07694v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "         'url': 'https://github.com/lshiwjx/2s-AGCN'}],\n",
       "       'metrics': {'Accuracy (CS)': '88.5', 'Accuracy (CV)': '95.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': '2s-NLGCN',\n",
       "       'paper_date': '2019-07-04',\n",
       "       'paper_title': 'Non-Local Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1805.07694v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "         'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '95.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VA-fusion (aug.)',\n",
       "       'paper_date': '2018-04-20',\n",
       "       'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '89.2', 'Accuracy (CV)': '95.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'AGC-LSTM (Joint&Part)',\n",
       "       'paper_date': '2019-02-25',\n",
       "       'paper_title': 'An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1902.09130v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '89.1', 'Accuracy (CV)': '94.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SLnL-rFA',\n",
       "       'paper_date': '2018-11-10',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention',\n",
       "       'paper_url': 'https://arxiv.org/abs/1811.04237v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "       'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '94.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EfficientGCN-B0',\n",
       "       'paper_date': '2021-06-29',\n",
       "       'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '87.5', 'Accuracy (CV)': '94.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GR-GCN',\n",
       "       'paper_date': '2018-11-29',\n",
       "       'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "       'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'limaosen0/AS-GCN',\n",
       "         'url': 'https://github.com/limaosen0/AS-GCN'}],\n",
       "       'metrics': {'Accuracy (CS)': '86.8', 'Accuracy (CV)': '94.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'AS-GCN',\n",
       "       'paper_date': '2019-04-26',\n",
       "       'paper_title': 'Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1904.12659v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '86.2', 'Accuracy (CV)': '94.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Sem-GCN',\n",
       "       'paper_date': '2020-05-01',\n",
       "       'paper_title': 'A Semantics-Guided Graph Convolutional Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://doi.org/10.1145/3390557.3394129',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'Sunnydreamrain/IndRNN_pytorch',\n",
       "         'url': 'https://github.com/Sunnydreamrain/IndRNN_pytorch'}],\n",
       "       'metrics': {'Accuracy (CS)': '86.70', 'Accuracy (CV)': '93.97'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Dense IndRNN',\n",
       "       'paper_date': '2019-10-11',\n",
       "       'paper_title': 'Deep Independently Recurrent Neural Network (IndRNN)',\n",
       "       'paper_url': 'https://arxiv.org/abs/1910.06251v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '88.6', 'Accuracy (CV)': '93.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': '3SCNN',\n",
       "       'paper_date': '2019-06-16',\n",
       "       'paper_title': 'Three-Stream Convolutional Neural Network With Multi-Task and Ensemble Learning for 3D Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPRW_2019/html/PBVS/Liang_Three-Stream_Convolutional_Neural_Network_With_Multi-Task_and_Ensemble_Learning_for_CVPRW_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '88.0', 'Accuracy (CV)': '93.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PGCN-TCA',\n",
       "       'paper_date': '2020-01-06',\n",
       "       'paper_title': 'PGCN-TCA: Pseudo Graph Convolutional Network With Temporal and Channel-Wise Attention for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/ACCESS.2020.2964115',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/RA-GCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/RA-GCNv1'},\n",
       "        {'title': 'yfsong0709/RA-GCNv2',\n",
       "         'url': 'https://github.com/yfsong0709/RA-GCNv2'},\n",
       "        {'title': 'peter-yys-yoon/pegcnv2',\n",
       "         'url': 'https://github.com/peter-yys-yoon/pegcnv2'}],\n",
       "       'metrics': {'Accuracy (CS)': '87.3', 'Accuracy (CV)': '93.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': '3s RA-GCN',\n",
       "       'paper_date': '2020-08-09',\n",
       "       'paper_title': 'Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2008.03791v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/RA-GCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/RA-GCNv1'},\n",
       "        {'title': 'yfsong0709/RA-GCNv2',\n",
       "         'url': 'https://github.com/yfsong0709/RA-GCNv2'},\n",
       "        {'title': 'peter-yys-yoon/pegcnv2',\n",
       "         'url': 'https://github.com/peter-yys-yoon/pegcnv2'}],\n",
       "       'metrics': {'Accuracy (CS)': '85.9', 'Accuracy (CV)': '93.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': '3s RA-GCN',\n",
       "       'paper_date': '2019-05-16',\n",
       "       'paper_title': 'Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons',\n",
       "       'paper_url': 'https://arxiv.org/abs/1905.06774v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'microsoft/SGN',\n",
       "         'url': 'https://github.com/microsoft/SGN'}],\n",
       "       'metrics': {'Accuracy (CS)': '86.6', 'Accuracy (CV)': '93.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SGN',\n",
       "       'paper_date': '2019-04-02',\n",
       "       'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1904.01189v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'andreYoo/PeGCNs',\n",
       "         'url': 'https://github.com/andreYoo/PeGCNs'}],\n",
       "       'metrics': {'Accuracy (CS)': '85.6', 'Accuracy (CV)': '93.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PeGCN',\n",
       "       'paper_date': '2020-03-17',\n",
       "       'paper_title': 'Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2003.07514v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'memory-attention-networks/MANs',\n",
       "         'url': 'https://github.com/memory-attention-networks/MANs'}],\n",
       "       'metrics': {'Accuracy (CS)': '82.67', 'Accuracy (CV)': '93.22'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MANs (DenseNet-161)',\n",
       "       'paper_date': '2018-04-23',\n",
       "       'paper_title': 'Memory Attention Networks for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1804.08254v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kalpitthakkar/pb-gcn',\n",
       "         'url': 'https://github.com/kalpitthakkar/pb-gcn'}],\n",
       "       'metrics': {'Accuracy (CS)': '87.5', 'Accuracy (CV)': '93.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PB-GCN',\n",
       "       'paper_date': '2018-09-13',\n",
       "       'paper_title': 'Part-based Graph Convolutional Network for Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1809.04983v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '85.1', 'Accuracy (CV)': '93.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'AR-GCN',\n",
       "       'paper_date': '2019-11-27',\n",
       "       'paper_title': 'An Attention-Enhanced Recurrent Graph Convolutional Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://doi.org/10.1145/3372806.3372814',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'yfsong0709/RA-GCNv1',\n",
       "         'url': 'https://github.com/yfsong0709/RA-GCNv1'},\n",
       "        {'title': 'yfsong0709/RA-GCNv2',\n",
       "         'url': 'https://github.com/yfsong0709/RA-GCNv2'},\n",
       "        {'title': 'peter-yys-yoon/pegcnv2',\n",
       "         'url': 'https://github.com/peter-yys-yoon/pegcnv2'}],\n",
       "       'metrics': {'Accuracy (CS)': '85.8', 'Accuracy (CV)': '93.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': '2s RA-GCN',\n",
       "       'paper_date': '2019-05-16',\n",
       "       'paper_title': 'Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons',\n",
       "       'paper_url': 'https://arxiv.org/abs/1905.06774v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '85.3', 'Accuracy (CV)': '92.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GVFE+ AS-GCN with DH-TCN',\n",
       "       'paper_date': '2019-12-20',\n",
       "       'paper_title': 'Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.09745v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '87.2', 'Accuracy (CV)': '92.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TS-SAN',\n",
       "       'paper_date': '2019-12-18',\n",
       "       'paper_title': 'Self-Attention Network for Skeleton-based Human Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.08435v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '84.8', 'Accuracy (CV)': '92.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SR-TSL',\n",
       "       'paper_date': '2018-05-07',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning',\n",
       "       'paper_url': 'http://arxiv.org/abs/1805.02335v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '85.0', 'Accuracy (CV)': '92.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': '3scale ResNet152',\n",
       "       'paper_date': '2017-04-19',\n",
       "       'paper_title': 'Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.05645v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '85.2', 'Accuracy (CV)': '91.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PR-GCN',\n",
       "       'paper_date': '2020-10-14',\n",
       "       'paper_title': 'Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2010.07367v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '86.8', 'Accuracy (CV)': '91.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'RF-Action',\n",
       "       'paper_date': '2019-09-20',\n",
       "       'paper_title': 'Making the Invisible Visible: Action Recognition Through Walls and Occlusions',\n",
       "       'paper_url': 'https://arxiv.org/abs/1909.09300v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'huguyuehuhu/HCN-pytorch',\n",
       "         'url': 'https://github.com/huguyuehuhu/HCN-pytorch'},\n",
       "        {'title': 'fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition',\n",
       "         'url': 'https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition'},\n",
       "        {'title': 'maxstrobel/HCN-PrototypeLoss-PyTorch',\n",
       "         'url': 'https://github.com/maxstrobel/HCN-PrototypeLoss-PyTorch'},\n",
       "        {'title': 'hhe-distance/AIF-CNN',\n",
       "         'url': 'https://github.com/hhe-distance/AIF-CNN'},\n",
       "        {'title': 'natepuppy/HCN-pytorch',\n",
       "         'url': 'https://github.com/natepuppy/HCN-pytorch'}],\n",
       "       'metrics': {'Accuracy (CS)': '86.5', 'Accuracy (CV)': '91.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'HCN',\n",
       "       'paper_date': '2018-04-17',\n",
       "       'paper_title': 'Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation',\n",
       "       'paper_url': 'http://arxiv.org/abs/1804.06055v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '82.83', 'Accuracy (CV)': '90.05'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'FO-GASTM',\n",
       "       'paper_date': '2019-07-08',\n",
       "       'paper_title': 'Learning Shape-Motion Representations from Geometric Algebra Spatio-Temporal Model for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/ICME.2019.00187',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '83.5', 'Accuracy (CV)': '89.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DPRL',\n",
       "       'paper_date': '2018-06-01',\n",
       "       'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'magnux/DMNN',\n",
       "         'url': 'https://github.com/magnux/DMNN'}],\n",
       "       'metrics': {'Accuracy (CS)': '82.0', 'Accuracy (CV)': '89.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DM-3DCNN',\n",
       "       'paper_date': '2017-10-23',\n",
       "       'paper_title': '3D CNNs on Distance Matrices for Human Action Recognition',\n",
       "       'paper_url': 'https://doi.org/10.1145/3123266.3123299',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '83.2', 'Accuracy (CV)': '89.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'CNN+Motion+Trans',\n",
       "       'paper_date': '2017-04-25',\n",
       "       'paper_title': 'Skeleton-based Action Recognition with Convolutional Neural Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.07595v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '84.23', 'Accuracy (CV)': '89.27'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'RGB+Skeleton (cross-attention)',\n",
       "       'paper_date': '2020-01-20',\n",
       "       'paper_title': 'Context-Aware Cross-Attention for Skeleton-Based Human Action Recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/ACCESS.2020.2968054',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '81.8', 'Accuracy (CV)': '89.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Bayesian GC-LSTM',\n",
       "       'paper_date': '2019-10-01',\n",
       "       'paper_title': 'Bayesian Graph Convolution LSTM for Skeleton Based Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Bayesian_Graph_Convolution_LSTM_for_Skeleton_Based_Action_Recognition_ICCV_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '83.36', 'Accuracy (CV)': '88.84'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-GCN-jpd',\n",
       "       'paper_date': '2019-06-24',\n",
       "       'paper_title': 'A Comparative Review of Recent Kinect-based Action Recognition Algorithms',\n",
       "       'paper_url': 'https://arxiv.org/abs/1906.09955v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '80.7', 'Accuracy (CV)': '88.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ARRN-LSTM',\n",
       "       'paper_date': '2018-05-07',\n",
       "       'paper_title': 'Relational Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1805.02556v4',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '80.7', 'Accuracy (CV)': '88.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EleAtt-GRU (aug.)',\n",
       "       'paper_date': '2019-09-03',\n",
       "       'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks',\n",
       "       'paper_url': 'https://arxiv.org/abs/1909.01939v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'open-mmlab/mmskeleton',\n",
       "         'url': 'https://github.com/open-mmlab/mmskeleton'},\n",
       "        {'title': 'yysijie/st-gcn',\n",
       "         'url': 'https://github.com/yysijie/st-gcn'},\n",
       "        {'title': 'ericksiavichay/cs230-final-project',\n",
       "         'url': 'https://github.com/ericksiavichay/cs230-final-project'},\n",
       "        {'title': 'XinzeWu/st-GCN',\n",
       "         'url': 'https://github.com/XinzeWu/st-GCN'},\n",
       "        {'title': 'stillarrow/S2VT_ACT',\n",
       "         'url': 'https://github.com/stillarrow/S2VT_ACT'},\n",
       "        {'title': 'ZhangNYG/ST-GCN',\n",
       "         'url': 'https://github.com/ZhangNYG/ST-GCN'},\n",
       "        {'title': 'DixinFan/st-gcn',\n",
       "         'url': 'https://github.com/DixinFan/st-gcn'},\n",
       "        {'title': 'ken724049/action-recognition',\n",
       "         'url': 'https://github.com/ken724049/action-recognition'},\n",
       "        {'title': 'antoniolq/st-gcn',\n",
       "         'url': 'https://github.com/antoniolq/st-gcn'},\n",
       "        {'title': 'github-zbx/ST-GCN',\n",
       "         'url': 'https://github.com/github-zbx/ST-GCN'},\n",
       "        {'title': 'GeyuanZhang/st-gcn-master',\n",
       "         'url': 'https://github.com/GeyuanZhang/st-gcn-master'},\n",
       "        {'title': 'KrisLee512/ST-GCN',\n",
       "         'url': 'https://github.com/KrisLee512/ST-GCN'},\n",
       "        {'title': 'AbiterVX/ST-GCN',\n",
       "         'url': 'https://github.com/AbiterVX/ST-GCN'}],\n",
       "       'metrics': {'Accuracy (CS)': '81.5', 'Accuracy (CV)': '88.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-GCN',\n",
       "       'paper_date': '2018-01-23',\n",
       "       'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1801.07455v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'TobiasLee/Text-Classification',\n",
       "         'url': 'https://github.com/TobiasLee/Text-Classification'},\n",
       "        {'title': 'batzner/indrnn',\n",
       "         'url': 'https://github.com/batzner/indrnn'},\n",
       "        {'title': 'lmnt-com/haste',\n",
       "         'url': 'https://github.com/lmnt-com/haste'},\n",
       "        {'title': 'StefOe/indrnn-pytorch',\n",
       "         'url': 'https://github.com/StefOe/indrnn-pytorch'},\n",
       "        {'title': 'Sunnydreamrain/IndRNN_pytorch',\n",
       "         'url': 'https://github.com/Sunnydreamrain/IndRNN_pytorch'},\n",
       "        {'title': 'Sunnydreamrain/IndRNN_Theano_Lasagne',\n",
       "         'url': 'https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne'},\n",
       "        {'title': 'trevor-richardson/rnn_zoo',\n",
       "         'url': 'https://github.com/trevor-richardson/rnn_zoo'},\n",
       "        {'title': 'amcs1729/Predicting-cloud-CPU-usage-on-Azure-data',\n",
       "         'url': 'https://github.com/amcs1729/Predicting-cloud-CPU-usage-on-Azure-data'},\n",
       "        {'title': 'Sunnydreamrain/IndRNN',\n",
       "         'url': 'https://github.com/Sunnydreamrain/IndRNN'},\n",
       "        {'title': 'secretlyvogon/IndRNNTF',\n",
       "         'url': 'https://github.com/secretlyvogon/IndRNNTF'},\n",
       "        {'title': 'secretlyvogon/Neural-Network-Implementations',\n",
       "         'url': 'https://github.com/secretlyvogon/Neural-Network-Implementations'}],\n",
       "       'metrics': {'Accuracy (CS)': '81.8', 'Accuracy (CV)': '88.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Ind-RNN',\n",
       "       'paper_date': '2018-03-13',\n",
       "       'paper_title': 'Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN',\n",
       "       'paper_url': 'http://arxiv.org/abs/1803.04831v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '79.4', 'Accuracy (CV)': '87.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'VA-LSTM',\n",
       "       'paper_date': '2017-03-24',\n",
       "       'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data',\n",
       "       'paper_url': 'http://arxiv.org/abs/1703.08274v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '80', 'Accuracy (CV)': '87.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Synthesized CNN',\n",
       "       'paper_date': '2017-08-01',\n",
       "       'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition',\n",
       "       'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '79.8', 'Accuracy (CV)': '87.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EleAtt-GRU',\n",
       "       'paper_date': '2018-07-12',\n",
       "       'paper_title': 'Adding Attentiveness to the Neurons in Recurrent Neural Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1807.04445v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '80.9', 'Accuracy (CV)': '86.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'HPM_RGB+HPM_3D+Traj',\n",
       "       'paper_date': '2017-07-04',\n",
       "       'paper_title': 'Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1707.00823v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '79.6', 'Accuracy (CV)': '84.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Clips+CNN+MTLN',\n",
       "       'paper_date': '2017-03-09',\n",
       "       'paper_title': 'A New Representation of Skeleton Sequences for 3D Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1703.03492v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "         'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "       'metrics': {'Accuracy (CS)': '76.5', 'Accuracy (CV)': '84.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Skelemotion + Yang et al.',\n",
       "       'paper_date': '2019-07-30',\n",
       "       'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1907.13025v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '79.6', 'Accuracy (CV)': '84.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'F2CSkeleton',\n",
       "       'paper_date': '2018-05-30',\n",
       "       'paper_title': 'A Fine-to-Coarse Convolutional Neural Network for 3D Human Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1805.11790v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '76.10', 'Accuracy (CV)': '84.00'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GCA-LSTM',\n",
       "       'paper_date': '2017-07-01',\n",
       "       'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '74.6', 'Accuracy (CV)': '83.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'URNN-2L-T',\n",
       "       'paper_date': '2017-10-22',\n",
       "       'paper_title': 'Adaptive RNN Tree for Large-Scale Human Action Recognition',\n",
       "       'paper_url': 'https://ieeexplore.ieee.org/document/8237423',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition',\n",
       "         'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}],\n",
       "       'metrics': {'Accuracy (CS)': '74.3', 'Accuracy (CV)': '83.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TCN',\n",
       "       'paper_date': '2017-04-14',\n",
       "       'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.04516v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'dzwallkilled/IEforAR',\n",
       "         'url': 'https://github.com/dzwallkilled/IEforAR'}],\n",
       "       'metrics': {'Accuracy (CV)': '82.31'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Five Spatial Skeleton Features',\n",
       "       'paper_date': '2017-05-02',\n",
       "       'paper_title': 'Investigation of Different Skeleton Features for CNN-based 3D Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1705.00835v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '74.60', 'Accuracy (CV)': '81.25'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Ensemble TS-LSTM v2',\n",
       "       'paper_date': '2017-10-01',\n",
       "       'paper_title': 'Ensemble Deep Learning for Skeleton-Based Action Recognition Using Temporal Sliding LSTM Networks',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_iccv_2017/html/Lee_Ensemble_Deep_Learning_ICCV_2017_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '75.9', 'Accuracy (CV)': '81.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'SkeletonNet',\n",
       "       'paper_date': '2017-03-31',\n",
       "       'paper_title': 'Skeletonnet: Mining deep part features for 3-d action recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/LSP.2017.2690339',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '73.4', 'Accuracy (CV)': '81.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'STA-LSTM',\n",
       "       'paper_date': '2016-11-18',\n",
       "       'paper_title': 'An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data',\n",
       "       'paper_url': 'http://arxiv.org/abs/1611.06067v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "         'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "       'metrics': {'Accuracy (CS)': '73.3', 'Accuracy (CV)': '80.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TSRJI (Late Fusion)',\n",
       "       'paper_date': '2019-09-11',\n",
       "       'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints',\n",
       "       'paper_url': 'https://arxiv.org/abs/1909.05704v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '71.3', 'Accuracy (CV)': '79.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Two-Stream RNN',\n",
       "       'paper_date': '2017-04-09',\n",
       "       'paper_title': 'Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.02581v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '69.2', 'Accuracy (CV)': '77.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Trust Gate ST-LSTM',\n",
       "       'paper_date': '2016-07-24',\n",
       "       'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '61.70', 'Accuracy (CV)': '75.50'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-LSTM',\n",
       "       'paper_date': '2016-07-24',\n",
       "       'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '66.8', 'Accuracy (CV)': '72.6'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Two-Stream 3DCNN',\n",
       "       'paper_date': '2017-05-23',\n",
       "       'paper_title': 'Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1705.08106v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "         'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "       'metrics': {'Accuracy (CS)': '62.93', 'Accuracy (CV)': '70.27'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Part-aware LSTM',\n",
       "       'paper_date': '2016-04-11',\n",
       "       'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "       'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "         'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "       'metrics': {'Accuracy (CS)': '60.7', 'Accuracy (CV)': '67.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Deep LSTM',\n",
       "       'paper_date': '2016-04-11',\n",
       "       'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "       'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '60.2', 'Accuracy (CV)': '65.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Dynamic Skeletons',\n",
       "       'paper_date': '2016-12-15',\n",
       "       'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '59.1', 'Accuracy (CV)': '64.0'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'H-RNN',\n",
       "       'paper_date': '2015-06-07',\n",
       "       'paper_title': 'Hierarchical recurrent neural network for skeleton based action recognition',\n",
       "       'paper_url': 'https://doi.org/10.1109/CVPR.2015.7298714',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'fdsa1860/skeletal',\n",
       "         'url': 'https://github.com/fdsa1860/skeletal'}],\n",
       "       'metrics': {'Accuracy (CS)': '50.1', 'Accuracy (CV)': '52.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Lie Group',\n",
       "       'paper_date': '2014-06-23',\n",
       "       'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group',\n",
       "       'paper_url': 'https://doi.org/10.1109/CVPR.2014.82',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (CS)': '38.6', 'Accuracy (CV)': '41.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Skeleton Quads',\n",
       "       'paper_date': '2014-08-24',\n",
       "       'paper_title': 'Skeletal quads: Human action recognition using joint quadruples',\n",
       "       'paper_url': 'https://doi.org/10.1109/ICPR.2014.772',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'JHMDB (2D poses only)',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-jhmdb-2d'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Average accuracy of 3 splits',\n",
       "      'Accuracy',\n",
       "      'No. parameters'],\n",
       "     'rows': [{'code_links': [{'title': 'fandulu/DD-Net',\n",
       "         'url': 'https://github.com/fandulu/DD-Net'},\n",
       "        {'title': 'BlurryLight/DD-Net-Pytorch',\n",
       "         'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}],\n",
       "       'metrics': {'Accuracy': '78.0 (average of 3 split train/test)',\n",
       "        'Average accuracy of 3 splits': '77.2',\n",
       "        'No. parameters': '1.82 M'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DD-Net',\n",
       "       'paper_date': '2019-07-23',\n",
       "       'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better',\n",
       "       'paper_url': 'https://arxiv.org/abs/1907.09658v8',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Average accuracy of 3 splits': '67.9',\n",
       "        'No. parameters': '-'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PoTion',\n",
       "       'paper_date': '2018-06-01',\n",
       "       'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'noboevbo/ehpi_action_recognition',\n",
       "         'url': 'https://github.com/noboevbo/ehpi_action_recognition'}],\n",
       "       'metrics': {'Average accuracy of 3 splits': '65.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EHPI',\n",
       "       'paper_date': '2019-04-19',\n",
       "       'paper_title': 'Simple yet efficient real-time pose-based action recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1904.09140v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'mzolfaghari/chained-multistream-networks',\n",
       "         'url': 'https://github.com/mzolfaghari/chained-multistream-networks'}],\n",
       "       'metrics': {'Average accuracy of 3 splits': '56.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Chained',\n",
       "       'paper_date': '2017-04-03',\n",
       "       'paper_title': 'Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.00616v2',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'PKU-MMD',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-pku-mmd'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['mAP@0.50 (CV)', 'mAP@0.50 (CS)'],\n",
       "     'rows': [{'code_links': [],\n",
       "       'metrics': {'mAP@0.50 (CS)': '92.9', 'mAP@0.50 (CV)': '94.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'RF-Action',\n",
       "       'paper_date': '2019-09-20',\n",
       "       'paper_title': 'Making the Invisible Visible: Action Recognition Through Walls and Occlusions',\n",
       "       'paper_url': 'https://arxiv.org/abs/1909.09300v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'huguyuehuhu/HCN-pytorch',\n",
       "         'url': 'https://github.com/huguyuehuhu/HCN-pytorch'},\n",
       "        {'title': 'fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition',\n",
       "         'url': 'https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition'},\n",
       "        {'title': 'maxstrobel/HCN-PrototypeLoss-PyTorch',\n",
       "         'url': 'https://github.com/maxstrobel/HCN-PrototypeLoss-PyTorch'},\n",
       "        {'title': 'hhe-distance/AIF-CNN',\n",
       "         'url': 'https://github.com/hhe-distance/AIF-CNN'},\n",
       "        {'title': 'natepuppy/HCN-pytorch',\n",
       "         'url': 'https://github.com/natepuppy/HCN-pytorch'}],\n",
       "       'metrics': {'mAP@0.50 (CS)': '92.6', 'mAP@0.50 (CV)': '94.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'HCN',\n",
       "       'paper_date': '2018-04-17',\n",
       "       'paper_title': 'Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation',\n",
       "       'paper_url': 'http://arxiv.org/abs/1804.06055v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'mAP@0.50 (CS)': '90.4', 'mAP@0.50 (CV)': '93.7'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Li et al. [[Li et al.2017b]]',\n",
       "       'paper_date': '2017-04-25',\n",
       "       'paper_title': 'Skeleton-based Action Recognition with Convolutional Neural Networks',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.07595v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'Skeletics-152',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy (Cross-Subject)'],\n",
       "     'rows': [{'code_links': [{'title': 'skelemoa/quovadis',\n",
       "         'url': 'https://github.com/skelemoa/quovadis'}],\n",
       "       'metrics': {'Accuracy (Cross-Subject)': '57.01 %'},\n",
       "       'model_links': [],\n",
       "       'model_name': '4s-ShiftGCN',\n",
       "       'paper_date': '2020-07-04',\n",
       "       'paper_title': 'Quo Vadis, Skeleton Action Recognition ?',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.02072v2',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'Skeleton-Mimetics',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-skeleton'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy (%)'],\n",
       "     'rows': [{'code_links': [{'title': 'skelemoa/quovadis',\n",
       "         'url': 'https://github.com/skelemoa/quovadis'}],\n",
       "       'metrics': {'Accuracy (%)': '57.37 %'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MS-G3D',\n",
       "       'paper_date': '2020-07-04',\n",
       "       'paper_title': 'Quo Vadis, Skeleton Action Recognition ?',\n",
       "       'paper_url': 'https://arxiv.org/abs/2007.02072v2',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'MSR Action3D',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msr'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "         'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "       'metrics': {'Accuracy': '88.51%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Temporal K-Means Clustering + Temporal Subspace Clustering',\n",
       "       'paper_date': '2020-06-21',\n",
       "       'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "       'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'rort1989/HDM',\n",
       "         'url': 'https://github.com/rort1989/HDM'}],\n",
       "       'metrics': {'Accuracy': '86.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'HDM-BG',\n",
       "       'paper_date': '2019-06-01',\n",
       "       'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '74'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'GFT',\n",
       "       'paper_date': '2019-08-26',\n",
       "       'paper_title': 'Graph Based Skeleton Modeling for Human Activity Analysis',\n",
       "       'paper_url': 'https://doi.org/10.1109/ICIP.2019.8803186',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'MSR ActionPairs',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msr-1'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "         'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "       'metrics': {'Accuracy': '98.02%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Temporal Subspace Clustering',\n",
       "       'paper_date': '2020-06-21',\n",
       "       'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "       'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'CAD-120',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-cad-120'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [],\n",
       "       'metrics': {'Accuracy': '91.1%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'NGM (5-shot)',\n",
       "       'paper_date': '2018-09-01',\n",
       "       'paper_title': 'Neural Graph Matching Networks for Fewshot 3D Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '89.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'All Features (w ground truth)',\n",
       "       'paper_date': '2013-02-01',\n",
       "       'paper_title': 'Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation',\n",
       "       'paper_url': 'http://proceedings.mlr.press/v28/koppula13.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '86.0%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'KGS',\n",
       "       'paper_date': '2012-10-04',\n",
       "       'paper_title': 'Learning Human Activities and Object Affordances from RGB-D Videos',\n",
       "       'paper_url': 'http://arxiv.org/abs/1210.1207v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'asheshjain399/RNNexp',\n",
       "         'url': 'https://github.com/asheshjain399/RNNexp'},\n",
       "        {'title': 'zhaolongkzz/human_motion',\n",
       "         'url': 'https://github.com/zhaolongkzz/human_motion'}],\n",
       "       'metrics': {'Accuracy': '85.4%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'S-RNN (5-shot)',\n",
       "       'paper_date': '2015-11-17',\n",
       "       'paper_title': 'Structural-RNN: Deep Learning on Spatio-Temporal Graphs',\n",
       "       'paper_url': 'http://arxiv.org/abs/1511.05298v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '85.0%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'NGM w/o Edges  (5-shot)',\n",
       "       'paper_date': '2018-09-01',\n",
       "       'paper_title': 'Neural Graph Matching Networks for Fewshot 3D Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '70.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Our DP seg. + moves + heuristic seg.',\n",
       "       'paper_date': '2013-02-01',\n",
       "       'paper_title': 'Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation',\n",
       "       'paper_url': 'http://proceedings.mlr.press/v28/koppula13.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'charlesq34/pointnet',\n",
       "         'url': 'https://github.com/charlesq34/pointnet'},\n",
       "        {'title': 'fxia22/pointnet.pytorch',\n",
       "         'url': 'https://github.com/fxia22/pointnet.pytorch'},\n",
       "        {'title': 'vinits5/learning3d',\n",
       "         'url': 'https://github.com/vinits5/learning3d'},\n",
       "        {'title': 'maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification',\n",
       "         'url': 'https://github.com/maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification'},\n",
       "        {'title': 'DylanWusee/pointconv_pytorch',\n",
       "         'url': 'https://github.com/DylanWusee/pointconv_pytorch'},\n",
       "        {'title': 'nikitakaraevv/pointnet',\n",
       "         'url': 'https://github.com/nikitakaraevv/pointnet'},\n",
       "        {'title': 'princeton-vl/SimpleView',\n",
       "         'url': 'https://github.com/princeton-vl/SimpleView'},\n",
       "        {'title': 'ZhihaoZhu/PointNet-Implementation-Tensorflow',\n",
       "         'url': 'https://github.com/ZhihaoZhu/PointNet-Implementation-Tensorflow'},\n",
       "        {'title': 'sarthakTUM/roofn3d',\n",
       "         'url': 'https://github.com/sarthakTUM/roofn3d'},\n",
       "        {'title': 'ajhamdi/AdvPC', 'url': 'https://github.com/ajhamdi/AdvPC'},\n",
       "        {'title': 'romaintha/pytorch_pointnet',\n",
       "         'url': 'https://github.com/romaintha/pytorch_pointnet'},\n",
       "        {'title': 'AI-Guru/pointcloud_experiments',\n",
       "         'url': 'https://github.com/AI-Guru/pointcloud_experiments'},\n",
       "        {'title': 'YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti',\n",
       "         'url': 'https://github.com/YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti'},\n",
       "        {'title': 'YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024',\n",
       "         'url': 'https://github.com/YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024'},\n",
       "        {'title': 'saaries/PointNet',\n",
       "         'url': 'https://github.com/saaries/PointNet'},\n",
       "        {'title': 'y2kmz/pointnetv2',\n",
       "         'url': 'https://github.com/y2kmz/pointnetv2'},\n",
       "        {'title': 'Fragjacker/Pointcloud-grad-CAM',\n",
       "         'url': 'https://github.com/Fragjacker/Pointcloud-grad-CAM'},\n",
       "        {'title': 'abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds',\n",
       "         'url': 'https://github.com/abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds'},\n",
       "        {'title': 'donshen/pointnet.phasedetection',\n",
       "         'url': 'https://github.com/donshen/pointnet.phasedetection'},\n",
       "        {'title': 'Yuto0107/pointnet',\n",
       "         'url': 'https://github.com/Yuto0107/pointnet'},\n",
       "        {'title': 'opeco17/pointnet',\n",
       "         'url': 'https://github.com/opeco17/pointnet'},\n",
       "        {'title': 'YanWei123/Pointnet_encoder_Foldingnet_decoder_quantization',\n",
       "         'url': 'https://github.com/YanWei123/Pointnet_encoder_Foldingnet_decoder_quantization'},\n",
       "        {'title': 'yanxp/PointNet',\n",
       "         'url': 'https://github.com/yanxp/PointNet'},\n",
       "        {'title': 'amyllykoski/CycleGAN',\n",
       "         'url': 'https://github.com/amyllykoski/CycleGAN'},\n",
       "        {'title': 'ftdlyc/pointnet_pytorch',\n",
       "         'url': 'https://github.com/ftdlyc/pointnet_pytorch'},\n",
       "        {'title': 'alpemek/ais3d', 'url': 'https://github.com/alpemek/ais3d'},\n",
       "        {'title': 'Dir-b/PointNet',\n",
       "         'url': 'https://github.com/Dir-b/PointNet'},\n",
       "        {'title': 'PaParaZz1/PointNet',\n",
       "         'url': 'https://github.com/PaParaZz1/PointNet'},\n",
       "        {'title': 'witignite/Frustum-PointNet',\n",
       "         'url': 'https://github.com/witignite/Frustum-PointNet'},\n",
       "        {'title': 'sanantoniochili/PointCloud_KNN',\n",
       "         'url': 'https://github.com/sanantoniochili/PointCloud_KNN'},\n",
       "        {'title': 'zgx0534/pointnet_win',\n",
       "         'url': 'https://github.com/zgx0534/pointnet_win'},\n",
       "        {'title': 'KhusDM/PointNetTree',\n",
       "         'url': 'https://github.com/KhusDM/PointNetTree'},\n",
       "        {'title': 'timothylimyl/PointNet-Pytorch',\n",
       "         'url': 'https://github.com/timothylimyl/PointNet-Pytorch'},\n",
       "        {'title': 'minhncedutw/pointnet1_keras',\n",
       "         'url': 'https://github.com/minhncedutw/pointnet1_keras'},\n",
       "        {'title': 'kenakai16/pointconv_pytorch',\n",
       "         'url': 'https://github.com/kenakai16/pointconv_pytorch'},\n",
       "        {'title': 'xurui1217/pointnet.pytorch-master',\n",
       "         'url': 'https://github.com/xurui1217/pointnet.pytorch-master'},\n",
       "        {'title': 'GOD-GOD-Autonomous-Vehicle/self-pointnet',\n",
       "         'url': 'https://github.com/GOD-GOD-Autonomous-Vehicle/self-pointnet'},\n",
       "        {'title': 'Young98CN/pointconv_pytorch',\n",
       "         'url': 'https://github.com/Young98CN/pointconv_pytorch'},\n",
       "        {'title': 'm-117/PointNet-ein-Implementationsbeispiel-mit-Jupyter-Notebooks',\n",
       "         'url': 'https://github.com/m-117/PointNet-ein-Implementationsbeispiel-mit-Jupyter-Notebooks'},\n",
       "        {'title': 'hnVfly/pointnet.mxnet',\n",
       "         'url': 'https://github.com/hnVfly/pointnet.mxnet'},\n",
       "        {'title': 'LebronGG/PointNet',\n",
       "         'url': 'https://github.com/LebronGG/PointNet'},\n",
       "        {'title': 'ytng001/sensemaking',\n",
       "         'url': 'https://github.com/ytng001/sensemaking'},\n",
       "        {'title': 'lingzhang1/pointnet_tensorflow',\n",
       "         'url': 'https://github.com/lingzhang1/pointnet_tensorflow'},\n",
       "        {'title': 'coconutzs/PointNet_zs',\n",
       "         'url': 'https://github.com/coconutzs/PointNet_zs'},\n",
       "        {'title': 'Fnjn/UCSD-CSE-291I',\n",
       "         'url': 'https://github.com/Fnjn/UCSD-CSE-291I'},\n",
       "        {'title': 'aviros/roatationPointnet',\n",
       "         'url': 'https://github.com/aviros/roatationPointnet'},\n",
       "        {'title': 'ShadowShadowWong/update-pointnet-work',\n",
       "         'url': 'https://github.com/ShadowShadowWong/update-pointnet-work'},\n",
       "        {'title': 'Yang2446/pointnet',\n",
       "         'url': 'https://github.com/Yang2446/pointnet'},\n",
       "        {'title': 'dwtstore/sfm1', 'url': 'https://github.com/dwtstore/sfm1'},\n",
       "        {'title': 'ModelBunker/PointNet-TensorFlow',\n",
       "         'url': 'https://github.com/ModelBunker/PointNet-TensorFlow'},\n",
       "        {'title': 'Lw510107/pointnet-2018.6.27-',\n",
       "         'url': 'https://github.com/Lw510107/pointnet-2018.6.27-'},\n",
       "        {'title': 'freddieee/pn_6d_single',\n",
       "         'url': 'https://github.com/freddieee/pn_6d_single'},\n",
       "        {'title': 'mengxingshifen1218/pointnet.pytorch',\n",
       "         'url': 'https://github.com/mengxingshifen1218/pointnet.pytorch'},\n",
       "        {'title': 'LONG-9621/Extract_Point_3D',\n",
       "         'url': 'https://github.com/LONG-9621/Extract_Point_3D'},\n",
       "        {'title': 'lingzhang1/pointnet_pytorch',\n",
       "         'url': 'https://github.com/lingzhang1/pointnet_pytorch'},\n",
       "        {'title': 'THHHomas/mls', 'url': 'https://github.com/THHHomas/mls'},\n",
       "        {'title': 'ahmed-anas/thesis-pointnet',\n",
       "         'url': 'https://github.com/ahmed-anas/thesis-pointnet'},\n",
       "        {'title': 'liuch37/pointnet',\n",
       "         'url': 'https://github.com/liuch37/pointnet'},\n",
       "        {'title': 'CheesyB/cpointnet',\n",
       "         'url': 'https://github.com/CheesyB/cpointnet'},\n",
       "        {'title': 'aviros/pointnet_totations',\n",
       "         'url': 'https://github.com/aviros/pointnet_totations'},\n",
       "        {'title': 'Taeuk-Jang/pointcompletion',\n",
       "         'url': 'https://github.com/Taeuk-Jang/pointcompletion'},\n",
       "        {'title': 'bt77/pointnet', 'url': 'https://github.com/bt77/pointnet'},\n",
       "        {'title': 'yanx27/Pointnet',\n",
       "         'url': 'https://github.com/yanx27/Pointnet'},\n",
       "        {'title': 'monacv/pointnet',\n",
       "         'url': 'https://github.com/monacv/pointnet'},\n",
       "        {'title': 'merazlab/3D_Deep_Learning_Link',\n",
       "         'url': 'https://github.com/merazlab/3D_Deep_Learning_Link'},\n",
       "        {'title': 'ajertec/PointNetKeras',\n",
       "         'url': 'https://github.com/ajertec/PointNetKeras'},\n",
       "        {'title': 'AlfredoZermini/PointNet',\n",
       "         'url': 'https://github.com/AlfredoZermini/PointNet'},\n",
       "        {'title': 'YiruS/pointnet_adversarial',\n",
       "         'url': 'https://github.com/YiruS/pointnet_adversarial'},\n",
       "        {'title': 'wonderland-dsg/pointnet-grid',\n",
       "         'url': 'https://github.com/wonderland-dsg/pointnet-grid'},\n",
       "        {'title': 'KiranAkadas/My_Pointnet_v2',\n",
       "         'url': 'https://github.com/KiranAkadas/My_Pointnet_v2'},\n",
       "        {'title': 'Q-Qgao/pointnet',\n",
       "         'url': 'https://github.com/Q-Qgao/pointnet'},\n",
       "        {'title': 'LONG-9621/PointNet',\n",
       "         'url': 'https://github.com/LONG-9621/PointNet'},\n",
       "        {'title': 'wuryantoAji/POINTNET',\n",
       "         'url': 'https://github.com/wuryantoAji/POINTNET'},\n",
       "        {'title': 'WLK12580/12', 'url': 'https://github.com/WLK12580/12'},\n",
       "        {'title': 'SBPL-Cruz/perch_pose_sampler',\n",
       "         'url': 'https://github.com/SBPL-Cruz/perch_pose_sampler'},\n",
       "        {'title': 'Harut0726/my-pointnet-tensorflow',\n",
       "         'url': 'https://github.com/Harut0726/my-pointnet-tensorflow'},\n",
       "        {'title': 'BPMJG/annotated_pointnet',\n",
       "         'url': 'https://github.com/BPMJG/annotated_pointnet'},\n",
       "        {'title': 'SonuDileep/3-D-Object-Detection-using-PointNet',\n",
       "         'url': 'https://github.com/SonuDileep/3-D-Object-Detection-using-PointNet'},\n",
       "        {'title': 'prasadsawant5/PointNet',\n",
       "         'url': 'https://github.com/prasadsawant5/PointNet'},\n",
       "        {'title': 'zhijie-yang/pointnet.pytorch',\n",
       "         'url': 'https://github.com/zhijie-yang/pointnet.pytorch'},\n",
       "        {'title': 'KaidongLi/tf-3d-alpha',\n",
       "         'url': 'https://github.com/KaidongLi/tf-3d-alpha'},\n",
       "        {'title': 'hz-ants/pointnet.pytorch',\n",
       "         'url': 'https://github.com/hz-ants/pointnet.pytorch'},\n",
       "        {'title': 'AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation',\n",
       "         'url': 'https://github.com/AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation'},\n",
       "        {'title': 'VaanHUANG/CSCI5210HW1',\n",
       "         'url': 'https://github.com/VaanHUANG/CSCI5210HW1'}],\n",
       "       'metrics': {'Accuracy': '69.1%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PointNet (5-shot)',\n",
       "       'paper_date': '2016-12-02',\n",
       "       'paper_title': 'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation',\n",
       "       'paper_url': 'http://arxiv.org/abs/1612.00593v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "         'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "       'metrics': {'Accuracy': '68.1%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'P-LSTM (5-shot)',\n",
       "       'paper_date': '2016-04-11',\n",
       "       'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "       'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'First-Person Hand Action Benchmark',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-first'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['1:3 Accuracy',\n",
       "      '1:1 Accuracy',\n",
       "      '3:1 Accuracy',\n",
       "      'Cross-person Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition',\n",
       "         'url': 'https://github.com/AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition'}],\n",
       "       'metrics': {'1:1 Accuracy': '95.93',\n",
       "        '1:3 Accuracy': '92.9',\n",
       "        '3:1 Accuracy': '96.76',\n",
       "        'Cross-person Accuracy': '88.70'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TCN-Summ',\n",
       "       'paper_date': '2021-03-03',\n",
       "       'paper_title': 'Domain and View-point Agnostic Hand Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2103.02303v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'UAV-Human',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-uav'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Average Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'kchengiva/Shift-GCN',\n",
       "         'url': 'https://github.com/kchengiva/Shift-GCN'}],\n",
       "       'metrics': {'Average Accuracy': '37.98'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Shift-GCN',\n",
       "       'paper_date': '2020-06-01',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Average Accuracy': '36.97'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'HARD-Net',\n",
       "       'paper_date': None,\n",
       "       'paper_title': 'HARD-Net: Hardness-AwaRe Discrimination Network for 3D Early Activity Prediction',\n",
       "       'paper_url': 'https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1360_ECCV_2020_paper.php',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal',\n",
       "         'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'},\n",
       "        {'title': 'lshiwjx/2s-AGCN',\n",
       "         'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "        {'title': 'iamjeff7/j-va-aagcn',\n",
       "         'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "       'metrics': {'Average Accuracy': '34.84'},\n",
       "       'model_links': [],\n",
       "       'model_name': '2S-AGCN',\n",
       "       'paper_date': '2018-05-20',\n",
       "       'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/1805.07694v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'open-mmlab/mmskeleton',\n",
       "         'url': 'https://github.com/open-mmlab/mmskeleton'},\n",
       "        {'title': 'yysijie/st-gcn',\n",
       "         'url': 'https://github.com/yysijie/st-gcn'},\n",
       "        {'title': 'ericksiavichay/cs230-final-project',\n",
       "         'url': 'https://github.com/ericksiavichay/cs230-final-project'},\n",
       "        {'title': 'XinzeWu/st-GCN',\n",
       "         'url': 'https://github.com/XinzeWu/st-GCN'},\n",
       "        {'title': 'stillarrow/S2VT_ACT',\n",
       "         'url': 'https://github.com/stillarrow/S2VT_ACT'},\n",
       "        {'title': 'ZhangNYG/ST-GCN',\n",
       "         'url': 'https://github.com/ZhangNYG/ST-GCN'},\n",
       "        {'title': 'DixinFan/st-gcn',\n",
       "         'url': 'https://github.com/DixinFan/st-gcn'},\n",
       "        {'title': 'ken724049/action-recognition',\n",
       "         'url': 'https://github.com/ken724049/action-recognition'},\n",
       "        {'title': 'antoniolq/st-gcn',\n",
       "         'url': 'https://github.com/antoniolq/st-gcn'},\n",
       "        {'title': 'github-zbx/ST-GCN',\n",
       "         'url': 'https://github.com/github-zbx/ST-GCN'},\n",
       "        {'title': 'GeyuanZhang/st-gcn-master',\n",
       "         'url': 'https://github.com/GeyuanZhang/st-gcn-master'},\n",
       "        {'title': 'KrisLee512/ST-GCN',\n",
       "         'url': 'https://github.com/KrisLee512/ST-GCN'},\n",
       "        {'title': 'AbiterVX/ST-GCN',\n",
       "         'url': 'https://github.com/AbiterVX/ST-GCN'}],\n",
       "       'metrics': {'Average Accuracy': '30.25'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'ST-GCN',\n",
       "       'paper_date': '2018-01-23',\n",
       "       'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1801.07455v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch',\n",
       "         'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}],\n",
       "       'metrics': {'Average Accuracy': '29.90'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DGNN',\n",
       "       'paper_date': '2019-06-01',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'J-HMDB',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-j-hmdb'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy (RGB+pose)', 'Accuracy (pose)'],\n",
       "     'rows': [{'code_links': [],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '90.4', 'Accuracy (pose)': '67.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Potion',\n",
       "       'paper_date': '2018-06-01',\n",
       "       'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '86.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PA3D+RPAN',\n",
       "       'paper_date': '2019-06-01',\n",
       "       'paper_title': 'PA3D: Pose-Action 3D Machine for Video Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '85.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'I3D + Potion',\n",
       "       'paper_date': '2018-06-01',\n",
       "       'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'deepmind/kinetics-i3d',\n",
       "         'url': 'https://github.com/deepmind/kinetics-i3d'},\n",
       "        {'title': 'open-mmlab/mmaction2',\n",
       "         'url': 'https://github.com/open-mmlab/mmaction2'},\n",
       "        {'title': 'piergiaj/pytorch-i3d',\n",
       "         'url': 'https://github.com/piergiaj/pytorch-i3d'},\n",
       "        {'title': 'hassony2/kinetics_i3d_pytorch',\n",
       "         'url': 'https://github.com/hassony2/kinetics_i3d_pytorch'},\n",
       "        {'title': 'yaohungt/GSTEG_CVPR_2019',\n",
       "         'url': 'https://github.com/yaohungt/GSTEG_CVPR_2019'},\n",
       "        {'title': 'dlpbc/keras-kinetics-i3d',\n",
       "         'url': 'https://github.com/dlpbc/keras-kinetics-i3d'},\n",
       "        {'title': 'StanfordVL/RubiksNet',\n",
       "         'url': 'https://github.com/StanfordVL/RubiksNet'},\n",
       "        {'title': 'FrederikSchorr/sign-language',\n",
       "         'url': 'https://github.com/FrederikSchorr/sign-language'},\n",
       "        {'title': 'CMU-CREATE-Lab/deep-smoke-machine',\n",
       "         'url': 'https://github.com/CMU-CREATE-Lab/deep-smoke-machine'},\n",
       "        {'title': 'JeffCHEN2017/WSSTG',\n",
       "         'url': 'https://github.com/JeffCHEN2017/WSSTG'},\n",
       "        {'title': 'OanaIgnat/i3d_keras',\n",
       "         'url': 'https://github.com/OanaIgnat/i3d_keras'},\n",
       "        {'title': 'ahsaniqbal/Kinetics-FeatureExtractor',\n",
       "         'url': 'https://github.com/ahsaniqbal/Kinetics-FeatureExtractor'},\n",
       "        {'title': 'prinshul/GWSDR',\n",
       "         'url': 'https://github.com/prinshul/GWSDR'},\n",
       "        {'title': 'PPPrior/i3d-pytorch',\n",
       "         'url': 'https://github.com/PPPrior/i3d-pytorch'},\n",
       "        {'title': 'sebastiantiesmeyer/deeplabchop3d',\n",
       "         'url': 'https://github.com/sebastiantiesmeyer/deeplabchop3d'},\n",
       "        {'title': 'anonymous-p/Flickering_Adversarial_Video',\n",
       "         'url': 'https://github.com/anonymous-p/Flickering_Adversarial_Video'},\n",
       "        {'title': 'vijayvee/behavior-recognition',\n",
       "         'url': 'https://github.com/vijayvee/behavior-recognition'},\n",
       "        {'title': 'LukasHedegaard/co3d',\n",
       "         'url': 'https://github.com/LukasHedegaard/co3d'},\n",
       "        {'title': 'vijayvee/behavior_recognition',\n",
       "         'url': 'https://github.com/vijayvee/behavior_recognition'},\n",
       "        {'title': 'AbdurrahmanNadi/activity_recognition_web_service',\n",
       "         'url': 'https://github.com/AbdurrahmanNadi/activity_recognition_web_service'},\n",
       "        {'title': 'hjchoi-minds/i3dnia',\n",
       "         'url': 'https://github.com/hjchoi-minds/i3dnia'},\n",
       "        {'title': 'Alexyuda/action_recognition',\n",
       "         'url': 'https://github.com/Alexyuda/action_recognition'},\n",
       "        {'title': 'helloxy96/CS5242_Project2020',\n",
       "         'url': 'https://github.com/helloxy96/CS5242_Project2020'},\n",
       "        {'title': 'ShobhitMaheshwari/sign-language1',\n",
       "         'url': 'https://github.com/ShobhitMaheshwari/sign-language1'},\n",
       "        {'title': 'mHealthBuet/SegCodeNet',\n",
       "         'url': 'https://github.com/mHealthBuet/SegCodeNet'}],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '84.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'I3D',\n",
       "       'paper_date': '2017-05-22',\n",
       "       'paper_title': 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset',\n",
       "       'paper_url': 'http://arxiv.org/abs/1705.07750v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'agethen/RPAN',\n",
       "         'url': 'https://github.com/agethen/RPAN'}],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '83.9'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'RPAN',\n",
       "       'paper_date': '2017-10-22',\n",
       "       'paper_title': 'RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos',\n",
       "       'paper_url': 'https://doi.org/10.1109/ICCV.2017.402',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'mzolfaghari/chained-multistream-networks',\n",
       "         'url': 'https://github.com/mzolfaghari/chained-multistream-networks'}],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '76.1', 'Accuracy (pose)': '56.8'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Chained (RGB+Flow +Pose)',\n",
       "       'paper_date': '2017-04-03',\n",
       "       'paper_title': 'Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection',\n",
       "       'paper_url': 'http://arxiv.org/abs/1704.00616v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '71.1'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'MR Two-Sream R-CNN',\n",
       "       'paper_date': '2016-09-17',\n",
       "       'paper_title': 'Multi-region two-stream R-CNN for action detection',\n",
       "       'paper_url': 'https://doi.org/10.1007/978-3-319-46493-0_45',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '69.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'PA3D',\n",
       "       'paper_date': '2019-06-01',\n",
       "       'paper_title': 'PA3D: Pose-Action 3D Machine for Video Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '64.3'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'STAR-Net',\n",
       "       'paper_date': '2019-02-26',\n",
       "       'paper_title': 'STAR-Net: Action Recognition using Spatio-Temporal Activation Reprojection',\n",
       "       'paper_url': 'http://arxiv.org/abs/1902.10024v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'JeffCHEN2017/WSSTG',\n",
       "         'url': 'https://github.com/JeffCHEN2017/WSSTG'}],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '62.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Action Tubes',\n",
       "       'paper_date': '2014-11-21',\n",
       "       'paper_title': 'Finding Action Tubes',\n",
       "       'paper_url': 'http://arxiv.org/abs/1411.6031v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'fandulu/DD-Net',\n",
       "         'url': 'https://github.com/fandulu/DD-Net'},\n",
       "        {'title': 'BlurryLight/DD-Net-Pytorch',\n",
       "         'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '-', 'Accuracy (pose)': '77.2'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'DD-Net',\n",
       "       'paper_date': '2019-07-23',\n",
       "       'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better',\n",
       "       'paper_url': 'https://arxiv.org/abs/1907.09658v8',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'noboevbo/ehpi_action_recognition',\n",
       "         'url': 'https://github.com/noboevbo/ehpi_action_recognition'}],\n",
       "       'metrics': {'Accuracy (RGB+pose)': '-', 'Accuracy (pose)': '65.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'EHPI',\n",
       "       'paper_date': '2019-04-19',\n",
       "       'paper_title': 'Simple yet efficient real-time pose-based action recognition',\n",
       "       'paper_url': 'http://arxiv.org/abs/1904.09140v1',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []},\n",
       "   {'dataset': 'UPenn Action',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-upenn'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'google-research/google-research',\n",
       "         'url': 'https://github.com/google-research/google-research/tree/master/poem'}],\n",
       "       'metrics': {'Accuracy': '97.5'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Pr-VIPE',\n",
       "       'paper_date': '2019-12-02',\n",
       "       'paper_title': 'View-Invariant Probabilistic Embedding for Human Pose',\n",
       "       'paper_url': 'https://arxiv.org/abs/1912.01001v4',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'rort1989/HDM',\n",
       "         'url': 'https://github.com/rort1989/HDM'}],\n",
       "       'metrics': {'Accuracy': '93.4'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'HDM-BG',\n",
       "       'paper_date': '2019-06-01',\n",
       "       'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []}],\n",
       "  'description': '<span style=\"color:grey; opacity: 0.6\">( Image credit: [View Adaptive Neural Networks for High\\r\\nPerformance Skeleton-based Human Action\\r\\nRecognition](https://arxiv.org/pdf/1804.07453v3.pdf) )</span>',\n",
       "  'source_link': None,\n",
       "  'subtasks': [],\n",
       "  'synonyms': [],\n",
       "  'task': 'Skeleton Based Action Recognition'},\n",
       " {'categories': [],\n",
       "  'datasets': [{'dataset': 'NTU RGB+D 120',\n",
       "    'dataset_citations': [],\n",
       "    'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "      'url': 'https://paperswithcode.com/sota/one-shot-3d-action-recognition-on-ntu-rgbd'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['Accuracy'],\n",
       "     'rows': [{'code_links': [{'title': 'raphaelmemmesheimer/sl-dml',\n",
       "         'url': 'https://github.com/raphaelmemmesheimer/sl-dml'}],\n",
       "       'metrics': {'Accuracy': '49.6%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Deep Metric Learning (Triplet Loss, Signals)',\n",
       "       'paper_date': '2020-04-23',\n",
       "       'paper_title': 'SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action Recognition',\n",
       "       'paper_url': 'https://arxiv.org/abs/2004.11085v4',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'AlbertoSabater/Skeleton-based-One-shot-Action-Recognition',\n",
       "         'url': 'https://github.com/AlbertoSabater/Skeleton-based-One-shot-Action-Recognition'}],\n",
       "       'metrics': {'Accuracy': '46.5%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'TCN_OneShot',\n",
       "       'paper_date': '2021-02-17',\n",
       "       'paper_title': 'One-shot action recognition in challenging therapy scenarios',\n",
       "       'paper_url': 'https://arxiv.org/abs/2102.08997v3',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "         'url': 'https://github.com/shahroudy/NTURGB-D'},\n",
       "        {'title': 'LinguoLi/CrosSCLR',\n",
       "         'url': 'https://github.com/LinguoLi/CrosSCLR'}],\n",
       "       'metrics': {'Accuracy': '45.3%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'APSR',\n",
       "       'paper_date': '2019-05-12',\n",
       "       'paper_title': 'NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding',\n",
       "       'paper_url': 'https://arxiv.org/abs/1905.04757v2',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '42.9%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Average Pooling',\n",
       "       'paper_date': '2017-06-26',\n",
       "       'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates',\n",
       "       'paper_url': 'http://arxiv.org/abs/1706.08276v1',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '42.1%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Fully Connected',\n",
       "       'paper_date': '2017-07-01',\n",
       "       'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html',\n",
       "       'uses_additional_data': False},\n",
       "      {'code_links': [],\n",
       "       'metrics': {'Accuracy': '41.0%'},\n",
       "       'model_links': [],\n",
       "       'model_name': 'Attention Network',\n",
       "       'paper_date': '2017-07-01',\n",
       "       'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition',\n",
       "       'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html',\n",
       "       'uses_additional_data': False}]},\n",
       "    'subdatasets': []}],\n",
       "  'description': '',\n",
       "  'source_link': None,\n",
       "  'subtasks': [],\n",
       "  'synonyms': [],\n",
       "  'task': 'One-Shot 3D Action Recognition'},\n",
       " {'categories': [],\n",
       "  'datasets': [],\n",
       "  'description': 'Detect if two people are looking at each other',\n",
       "  'source_link': None,\n",
       "  'subtasks': [],\n",
       "  'synonyms': [],\n",
       "  'task': 'Mutual Gaze'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the json\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#this is once specific category (100)\n",
    "df.iloc[100].subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4eead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a sample (category) at index == 100\n",
    "sample= df[df.index==100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1eac14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>datasets</th>\n",
       "      <th>description</th>\n",
       "      <th>source_link</th>\n",
       "      <th>subtasks</th>\n",
       "      <th>synonyms</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[Computer Vision]</td>\n",
       "      <td>[{'dataset': 'NTU RGB+D 120', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-ntu-rgb-d-1'}], 'description': '', 'sota': {'metrics': ['Accuracy (Cross-Setup)', 'Accuracy (Cross-Subject)'], 'rows': [{'code_links': [{'title': 'mauriciolp/inter-rel-net', 'url': 'https://github.com/mauriciolp/inter-rel-net'}], 'metrics': {'Accuracy (Cross-Setup)': '79.6', 'Accuracy (Cross-Subject)': '77.7'}, 'model_links': [], 'model_name': \"LSTM-IRN'fc1inter+intra\", 'paper_date': '2019-10-11', 'paper_title': 'Interaction Relational Network for Mutual Action Recognition', 'paper_url': 'https://arxiv.org/abs/1910.04963v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'BIT', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-bit'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '94.03'}, 'model_links': [], 'model_name': 'H-LSTCM', 'paper_date': '2018-11-01', 'paper_title': 'Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition', 'paper_url': 'http://arxiv.org/abs/1811.00270v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '92.88'}, 'model_links': [], 'model_name': 'Co-LSTSM', 'paper_date': '2017-06-03', 'paper_title': 'Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition', 'paper_url': 'http://arxiv.org/abs/1706.00931v1', 'uses_additional_data': False}, {'code_links': [{'title': 'garythung/torch-lrcn', 'url': 'https://github.com/garythung/torch-lrcn'}, {'title': 'doronharitan/human_activity_recognition_LRCN', 'url': 'https://github.com/doronharitan/human_activity_recognition_LRCN'}, {'title': 'DJAlexJ/LRCN-for-Video-Regression', 'url': 'https://github.com/DJAlexJ/LRCN-for-Video-Regression'}, {'title': 'sujaygarlanka/tennis_stroke_recognition', 'url': 'https://github.com/sujaygarlanka/tennis_stroke_recognition'}, {'title': 'rlaengud123/CMC_LRCN', 'url': 'https://github.com/rlaengud123/CMC_LRCN'}, {'title': 'Deepu1992/VideoClassification', 'url': 'https://github.com/Deepu1992/VideoClassification'}, {'title': 'kahnchana/RNN', 'url': 'https://github.com/kahnchana/RNN'}], 'metrics': {'Accuracy': '80.13'}, 'model_links': [], 'model_name': 'Donahue et al.', 'paper_date': '2014-11-17', 'paper_title': 'Long-term Recurrent Convolutional Networks for Visual Recognition and Description', 'paper_url': 'http://arxiv.org/abs/1411.4389v4', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'SBU', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-sbu'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'mauriciolp/inter-rel-net', 'url': 'https://github.com/mauriciolp/inter-rel-net'}], 'metrics': {'Accuracy': '98.2'}, 'model_links': [], 'model_name': \"LSTM-IRN'fc1inter+intra\", 'paper_date': '2019-10-11', 'paper_title': 'Interaction Relational Network for Mutual Action Recognition', 'paper_url': 'https://arxiv.org/abs/1910.04963v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UT-Interaction', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-ut-1'}], 'description': '', 'sota': {'metrics': ['Accuracy (Set 1)', 'Accuracy (Set 2)'], 'rows': [{'code_links': [{'title': 'mauriciolp/inter-rel-net', 'url': 'https://github.com/mauriciolp/inter-rel-net'}], 'metrics': {'Accuracy (Set 1)': '98.3', 'Accuracy (Set 2)': '96.7'}, 'model_links': [], 'model_name': \"LSTM-IRN'fc1inter+intra\", 'paper_date': '2019-10-11', 'paper_title': 'Interaction Relational Network for Mutual Action Recognition', 'paper_url': 'https://arxiv.org/abs/1910.04963v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'NTU RGB+D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-ntu-rgb-d'}], 'description': '', 'sota': {'metrics': ['Accuracy (Cross-Subject)', 'Accuracy (Cross-View)'], 'rows': [{'code_links': [{'title': 'mauriciolp/inter-rel-net', 'url': 'https://github.com/mauriciolp/inter-rel-net'}], 'metrics': {'Accuracy (Cross-Subject)': '90.5', 'Accuracy (Cross-View)': '93.5'}, 'model_links': [], 'model_name': \"LSTM-IRN'fc1inter+intra\", 'paper_date': '2019-10-11', 'paper_title': 'Interaction Relational Network for Mutual Action Recognition', 'paper_url': 'https://arxiv.org/abs/1910.04963v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UT', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-ut'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '98.33'}, 'model_links': [], 'model_name': 'H-LSTCM', 'paper_date': '2018-11-01', 'paper_title': 'Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition', 'paper_url': 'http://arxiv.org/abs/1811.00270v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '95.00'}, 'model_links': [], 'model_name': 'Co-LSTSM', 'paper_date': '2017-06-03', 'paper_title': 'Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition', 'paper_url': 'http://arxiv.org/abs/1706.00931v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '93.30'}, 'model_links': [], 'model_name': 'Raptis et al.', 'paper_date': '2013-06-01', 'paper_title': 'Poselet Key-Framing: A Model for Human Activity Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2013/html/Raptis_Poselet_Key-Framing_A_2013_CVPR_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'garythung/torch-lrcn', 'url': 'https://github.com/garythung/torch-lrcn'}, {'title': 'doronharitan/human_activity_recognition_LRCN', 'url': 'https://github.com/doronharitan/human_activity_recognition_LRCN'}, {'title': 'DJAlexJ/LRCN-for-Video-Regression', 'url': 'https://github.com/DJAlexJ/LRCN-for-Video-Regression'}, {'title': 'sujaygarlanka/tennis_stroke_recognition', 'url': 'https://github.com/sujaygarlanka/tennis_stroke_recognition'}, {'title': 'rlaengud123/CMC_LRCN', 'url': 'https://github.com/rlaengud123/CMC_LRCN'}, {'title': 'Deepu1992/VideoClassification', 'url': 'https://github.com/Deepu1992/VideoClassification'}, {'title': 'kahnchana/RNN', 'url': 'https://github.com/kahnchana/RNN'}], 'metrics': {'Accuracy': '85.00'}, 'model_links': [], 'model_name': 'Donahue et al.', 'paper_date': '2014-11-17', 'paper_title': 'Long-term Recurrent Convolutional Networks for Visual Recognition and Description', 'paper_url': 'http://arxiv.org/abs/1411.4389v4', 'uses_additional_data': False}]}, 'subdatasets': []}]</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'categories': [], 'datasets': [{'dataset': 'SYSU 3D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sysu-3d'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'microsoft/SGN', 'url': 'https://github.com/microsoft/SGN'}], 'metrics': {'Accuracy': '86.9%'}, 'model_links': [], 'model_name': 'SGN', 'paper_date': '2019-04-02', 'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1904.01189v3', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '86.7%'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '85.7%'}, 'model_links': [], 'model_name': 'EleAtt-GRU (aug.)', 'paper_date': '2019-09-03', 'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks', 'paper_url': 'https://arxiv.org/abs/1909.01939v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '83.14%'}, 'model_links': [], 'model_name': 'Local+LGN', 'paper_date': '2019-09-02', 'paper_title': 'Learning Latent Global Network for Skeleton-based Action Prediction', 'paper_url': 'https://doi.org/10.1109/TIP.2019.2937757', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '77.9%'}, 'model_links': [], 'model_name': 'Complete GR-GCN', 'paper_date': '2018-11-29', 'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression', 'paper_url': 'http://arxiv.org/abs/1811.12013v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '77.5%'}, 'model_links': [], 'model_name': 'VA-LSTM', 'paper_date': '2017-03-24', 'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data', 'paper_url': 'http://arxiv.org/abs/1703.08274v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '76.9%'}, 'model_links': [], 'model_name': 'DPRL', 'paper_date': '2018-06-01', 'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '75.5%'}, 'model_links': [], 'model_name': 'Dynamic Skeletons', 'paper_date': '2016-12-15', 'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition', 'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '73.4%'}, 'model_links': [], 'model_name': 'ST-LSTM (Tree)', 'paper_date': '2017-06-26', 'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates', 'paper_url': 'http://arxiv.org/abs/1706.08276v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UT-Kinect', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ut'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '99.50%'}, 'model_links': [], 'model_name': 'Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '98.5%'}, 'model_links': [], 'model_name': 'Complete GR-GCN', 'paper_date': '2018-11-29', 'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression', 'paper_url': 'http://arxiv.org/abs/1811.12013v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '98.5%'}, 'model_links': [], 'model_name': 'DPRL', 'paper_date': '2018-06-01', 'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'fdsa1860/skeletal', 'url': 'https://github.com/fdsa1860/skeletal'}], 'metrics': {'Accuracy': '97.1%'}, 'model_links': [], 'model_name': 'Lie Group', 'paper_date': '2014-06-23', 'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group', 'paper_url': 'https://doi.org/10.1109/CVPR.2014.82', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '96%'}, 'model_links': [], 'model_name': 'GFT', 'paper_date': '2019-08-26', 'paper_title': 'Graph Based Skeleton Modeling for Human Activity Analysis', 'paper_url': 'https://doi.org/10.1109/ICIP.2019.8803186', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'N-UCLA', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-n-ucla'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '93.99'}, 'model_links': [], 'model_name': 'Hierarchical Action Classification (RGB + Pose)', 'paper_date': '2020-07-30', 'paper_title': 'Hierarchical Action Classification with Network Pruning', 'paper_url': 'https://arxiv.org/abs/2007.15244v1', 'uses_additional_data': False}, {'code_links': [{'title': 'srijandas07/VPN', 'url': 'https://github.com/srijandas07/VPN'}], 'metrics': {'Accuracy': '93.5'}, 'model_links': [], 'model_name': 'VPN (RGB + Pose)', 'paper_date': '2020-07-06', 'paper_title': 'VPN: Learning Video-Pose Embedding for Activities of Daily Living', 'paper_url': 'https://arxiv.org/abs/2007.03056v1', 'uses_additional_data': False}, {'code_links': [{'title': 'srijandas07/vpnplusplus', 'url': 'https://github.com/srijandas07/vpnplusplus'}], 'metrics': {'Accuracy': '93.5'}, 'model_links': [], 'model_name': 'VPN++ (RGB + Pose)', 'paper_date': '2021-05-17', 'paper_title': 'VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living', 'paper_url': 'https://arxiv.org/abs/2105.08141v1', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/SGN', 'url': 'https://github.com/microsoft/SGN'}], 'metrics': {'Accuracy': '92.5%'}, 'model_links': [], 'model_name': 'SGN', 'paper_date': '2019-04-02', 'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1904.01189v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '92.3%'}, 'model_links': [], 'model_name': 'Action Machine', 'paper_date': '2018-12-14', 'paper_title': 'Action Machine: Rethinking Action Recognition in Trimmed Videos', 'paper_url': 'http://arxiv.org/abs/1812.05770v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '90.7%'}, 'model_links': [], 'model_name': 'EleAtt-GRU (aug.)', 'paper_date': '2019-09-03', 'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks', 'paper_url': 'https://arxiv.org/abs/1909.01939v1', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '88.1%'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [{'title': 'fabienbaradel/glimpse_clouds', 'url': 'https://github.com/fabienbaradel/glimpse_clouds'}], 'metrics': {'Accuracy': '87.6%'}, 'model_links': [], 'model_name': 'Glimpse Clouds', 'paper_date': '2018-02-22', 'paper_title': 'Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points', 'paper_url': 'http://arxiv.org/abs/1802.07898v4', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'SBU', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sbu'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'Sy-Zhang/Geometric-Feature-Release', 'url': 'https://github.com/Sy-Zhang/Geometric-Feature-Release'}], 'metrics': {'Accuracy': '99.02%'}, 'model_links': [], 'model_name': 'Joint Line Distance', 'paper_date': '2017-03-01', 'paper_title': 'On Geometric Features for Skeleton-Based Action Recognition using Multilayer LSTM Networks', 'paper_url': 'https://doi.org/10.1109/WACV.2017.24', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '98.60%'}, 'model_links': [], 'model_name': 'MLGCN', 'paper_date': '2019-09-11', 'paper_title': 'MLGCN: Multi-Laplacian Graph Convolutional Networks for Human Action Recognition', 'paper_url': 'https://bmvc2019.org/wp-content/uploads/papers/1103-paper.pdf', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '98.3%'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [{'title': 'mdeff/cnn_graph', 'url': 'https://github.com/mdeff/cnn_graph'}, {'title': 'xbresson/spectral_graph_convnets', 'url': 'https://github.com/xbresson/spectral_graph_convnets'}, {'title': 'jasonseu/cnn_graph.pytorch', 'url': 'https://github.com/jasonseu/cnn_graph.pytorch'}, {'title': 'andrejmiscic/gcn-pytorch', 'url': 'https://github.com/andrejmiscic/gcn-pytorch'}, {'title': 'mdeff/paper-cnn-graph-nips2016', 'url': 'https://github.com/mdeff/paper-cnn-graph-nips2016'}, {'title': 'hazdzz/ChebyNet', 'url': 'https://github.com/hazdzz/ChebyNet'}, {'title': 'stsfaroz/Graph-Convolutional-Networks-with-Cora-Dataset', 'url': 'https://github.com/stsfaroz/Graph-Convolutional-Networks-with-Cora-Dataset'}], 'metrics': {'Accuracy': '96.00%'}, 'model_links': [], 'model_name': 'ChebyNet', 'paper_date': '2016-06-30', 'paper_title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering', 'paper_url': 'http://arxiv.org/abs/1606.09375v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '96.00%'}, 'model_links': [], 'model_name': 'ArmaConv', 'paper_date': '2019-01-05', 'paper_title': 'Graph Neural Networks with convolutional ARMA filters', 'paper_url': 'https://arxiv.org/abs/1901.01343v7', 'uses_additional_data': False}, {'code_links': [{'title': 'Maghoumi/DeepGRU', 'url': 'https://github.com/Maghoumi/DeepGRU'}], 'metrics': {'Accuracy': '95.7%'}, 'model_links': [], 'model_name': 'DeepGRU', 'paper_date': '2018-10-30', 'paper_title': 'DeepGRU: Deep Gesture Recognition Utility', 'paper_url': 'https://arxiv.org/abs/1810.12514v4', 'uses_additional_data': False}, {'code_links': [{'title': 'Tiiiger/SGC', 'url': 'https://github.com/Tiiiger/SGC'}, {'title': 'pulkit1joshi/SGC', 'url': 'https://github.com/pulkit1joshi/SGC'}, {'title': 'hazdzz/SGC', 'url': 'https://github.com/hazdzz/SGC'}], 'metrics': {'Accuracy': '94.0%'}, 'model_links': [], 'model_name': 'SGCConv', 'paper_date': '2019-02-19', 'paper_title': 'Simplifying Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1902.07153v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '93.3%'}, 'model_links': [], 'model_name': 'ST-LSTM + Trust Gate', 'paper_date': '2016-07-24', 'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1607.07043v1', 'uses_additional_data': False}, {'code_links': [{'title': 'tkipf/gcn', 'url': 'https://github.com/tkipf/gcn'}, {'title': 'tkipf/pygcn', 'url': 'https://github.com/tkipf/pygcn'}, {'title': 'tkipf/keras-gcn', 'url': 'https://github.com/tkipf/keras-gcn'}, {'title': 'LeeWooJung/GCN_reproduce', 'url': 'https://github.com/LeeWooJung/GCN_reproduce'}, {'title': 'hazdzz/GCN', 'url': 'https://github.com/hazdzz/GCN'}, {'title': 'giuseppefutia/link-prediction-code', 'url': 'https://github.com/giuseppefutia/link-prediction-code'}, {'title': 'andrejmiscic/gcn-pytorch', 'url': 'https://github.com/andrejmiscic/gcn-pytorch'}, {'title': 'switiz/gnn-gcn-gat', 'url': 'https://github.com/switiz/gnn-gcn-gat'}, {'title': 'dtriepke/Graph_Convolutional_Network', 'url': 'https://github.com/dtriepke/Graph_Convolutional_Network'}, {'title': 'bcsrn/gcn', 'url': 'https://github.com/bcsrn/gcn'}, {'title': 'Anieca/GCN', 'url': 'https://github.com/Anieca/GCN'}, {'title': 'HoganZhang/pygcn_python3', 'url': 'https://github.com/HoganZhang/pygcn_python3'}, {'title': 'lipingcoding/pygcn', 'url': 'https://github.com/lipingcoding/pygcn'}, {'title': 'KimMeen/GCN', 'url': 'https://github.com/KimMeen/GCN'}, {'title': 'darnbi/pygcn', 'url': 'https://github.com/darnbi/pygcn'}, {'title': 'thanhtrunghuynh93/pygcn', 'url': 'https://github.com/thanhtrunghuynh93/pygcn'}, {'title': 'ChengSashankh/gcn-graph-classification', 'url': 'https://github.com/ChengSashankh/gcn-graph-classification'}, {'title': 'LouisDumont/GCN---re-implementation', 'url': 'https://github.com/LouisDumont/GCN---re-implementation'}], 'metrics': {'Accuracy': '90.00%'}, 'model_links': [], 'model_name': 'GCNConv', 'paper_date': '2016-09-09', 'paper_title': 'Semi-Supervised Classification with Graph Convolutional Networks', 'paper_url': 'http://arxiv.org/abs/1609.02907v4', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'JHMDB Pose Tracking', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-jhmdb'}], 'description': '', 'sota': {'metrics': ['PCK@0.1', 'PCK@0.2', 'PCK@0.3', 'PCK@0.4', 'PCK@0.5'], 'rows': [{'code_links': [{'title': 'aimerykong/predictive-filter-flow', 'url': 'https://github.com/aimerykong/predictive-filter-flow'}, {'title': 'bestaar/predictiveFilterFlow', 'url': 'https://github.com/bestaar/predictiveFilterFlow'}], 'metrics': {'PCK@0.1': '58.4', 'PCK@0.2': '78.1', 'PCK@0.3': '85.9', 'PCK@0.4': '89.8', 'PCK@0.5': '92.4'}, 'model_links': [], 'model_name': 'mgPFF+ft 1st', 'paper_date': '2019-04-02', 'paper_title': 'Multigrid Predictive Filter Flow for Unsupervised Learning on Videos', 'paper_url': 'http://arxiv.org/abs/1904.01693v1', 'uses_additional_data': False}, {'code_links': [{'title': 'hyperparameters/tracking_via_colorization', 'url': 'https://github.com/hyperparameters/tracking_via_colorization'}], 'metrics': {'PCK@0.1': '45.2', 'PCK@0.2': '69.6', 'PCK@0.3': '80.8', 'PCK@0.4': '87.5', 'PCK@0.5': '91.4'}, 'model_links': [], 'model_name': 'ColorPointer', 'paper_date': '2018-06-25', 'paper_title': 'Tracking Emerges by Colorizing Videos', 'paper_url': 'http://arxiv.org/abs/1806.09594v2', 'uses_additional_data': False}, {'code_links': [{'title': 'NVIDIA/flownet2-pytorch', 'url': 'https://github.com/NVIDIA/flownet2-pytorch'}, {'title': 'philferriere/tfoptflow', 'url': 'https://github.com/philferriere/tfoptflow'}, {'title': 'ElliotHYLee/VisualOdometry3D', 'url': 'https://github.com/ElliotHYLee/VisualOdometry3D'}, {'title': 'mcgridles/LENS', 'url': 'https://github.com/mcgridles/LENS'}, {'title': 'rickyHong/tfoptflow-repl', 'url': 'https://github.com/rickyHong/tfoptflow-repl'}], 'metrics': {'PCK@0.1': '45.2', 'PCK@0.2': '62.9', 'PCK@0.3': '73.5', 'PCK@0.4': '80.6', 'PCK@0.5': '85.5'}, 'model_links': [], 'model_name': 'FlowNet2', 'paper_date': '2016-12-06', 'paper_title': 'FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks', 'paper_url': 'http://arxiv.org/abs/1612.01925v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'NTU RGB+D 120', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu-rgbd-1'}], 'description': '', 'sota': {'metrics': ['Accuracy (Cross-Subject)', 'Accuracy (Cross-Setup)'], 'rows': [{'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (Cross-Setup)': '89.1', 'Accuracy (Cross-Subject)': '88.3'}, 'model_links': [], 'model_name': 'EfficientGCN-B4', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '89.2%', 'Accuracy (Cross-Subject)': '88.2%'}, 'model_links': [], 'model_name': 'AngNet-JA + BA + JBA + VJBA', 'paper_date': '2021-05-04', 'paper_title': 'Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2105.01563v3', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (Cross-Setup)': '88.0', 'Accuracy (Cross-Subject)': '87.9'}, 'model_links': [], 'model_name': 'EfficientGCN-B2', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/ResGCNv1', 'url': 'https://github.com/yfsong0709/ResGCNv1'}, {'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (Cross-Setup)': '88.3%', 'Accuracy (Cross-Subject)': '87.3%'}, 'model_links': [], 'model_name': 'PA-ResGCN-B19', 'paper_date': '2020-10-20', 'paper_title': 'Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2010.09978v1', 'uses_additional_data': False}, {'code_links': [{'title': 'skelemoa/quovadis', 'url': 'https://github.com/skelemoa/quovadis'}], 'metrics': {'Accuracy (Cross-Setup)': '88.8%', 'Accuracy (Cross-Subject)': '87.22%'}, 'model_links': [], 'model_name': 'Ensemble-top5 (MS-G3D Net + 4s Shift-GCN + VA-CNN (ResNeXt101) + 2s SDGCN + GCN-NAS (retrained))', 'paper_date': '2020-07-04', 'paper_title': 'Quo Vadis, Skeleton Action Recognition ?', 'paper_url': 'https://arxiv.org/abs/2007.02072v2', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}], 'metrics': {'Accuracy (Cross-Setup)': '90.3', 'Accuracy (Cross-Subject)': '86.9'}, 'model_links': [], 'model_name': 'PoseC3D (w. HRNet 2D Skeleton)', 'paper_date': '2021-04-28', 'paper_title': 'Revisiting Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2104.13586v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/ms-g3d', 'url': 'https://github.com/kenziyuliu/ms-g3d'}], 'metrics': {'Accuracy (Cross-Setup)': '88.4%', 'Accuracy (Cross-Subject)': '86.9%'}, 'model_links': [], 'model_name': 'MS-G3D Net', 'paper_date': '2020-03-31', 'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.14111v2', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/DSTA-Net', 'url': 'https://github.com/lshiwjx/DSTA-Net'}], 'metrics': {'Accuracy (Cross-Setup)': '89.0 %', 'Accuracy (Cross-Subject)': '86.6%'}, 'model_links': [], 'model_name': 'DSTA-Net', 'paper_date': '2020-07-07', 'paper_title': 'Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.03263v1', 'uses_additional_data': False}, {'code_links': [{'title': 'srijandas07/VPN', 'url': 'https://github.com/srijandas07/VPN'}], 'metrics': {'Accuracy (Cross-Setup)': '87.8', 'Accuracy (Cross-Subject)': '86.3'}, 'model_links': [], 'model_name': 'VPN', 'paper_date': '2020-07-06', 'paper_title': 'VPN: Learning Video-Pose Embedding for Activities of Daily Living', 'paper_url': 'https://arxiv.org/abs/2007.03056v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kchengiva/Shift-GCN', 'url': 'https://github.com/kchengiva/Shift-GCN'}], 'metrics': {'Accuracy (Cross-Setup)': '87.6%', 'Accuracy (Cross-Subject)': '85.9%'}, 'model_links': [], 'model_name': '4s Shift-GCN', 'paper_date': '2020-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (Cross-Setup)': '84.3', 'Accuracy (Cross-Subject)': '85.9'}, 'model_links': [], 'model_name': 'EfficientGCN-B0', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '87.4%', 'Accuracy (Cross-Subject)': '85.4%'}, 'model_links': [], 'model_name': 'FGCN ', 'paper_date': '2020-03-17', 'paper_title': 'Feedback Graph Convolutional Network for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.07564v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '86.90', 'Accuracy (Cross-Subject)': '84.88'}, 'model_links': [], 'model_name': 'VA-CNN (ResNeXt-101)', 'paper_date': None, 'paper_title': '', 'paper_url': '', 'uses_additional_data': False}, {'code_links': [{'title': 'Chiaraplizz/ST-TR', 'url': 'https://github.com/Chiaraplizz/ST-TR'}], 'metrics': {'Accuracy (Cross-Setup)': '84.7%', 'Accuracy (Cross-Subject)': '82.7%'}, 'model_links': [], 'model_name': 'ST-TR-agcn', 'paper_date': '2020-08-17', 'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks', 'paper_url': 'https://arxiv.org/abs/2008.07404v4', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/RA-GCNv1', 'url': 'https://github.com/yfsong0709/RA-GCNv1'}, {'title': 'yfsong0709/RA-GCNv2', 'url': 'https://github.com/yfsong0709/RA-GCNv2'}, {'title': 'peter-yys-yoon/pegcnv2', 'url': 'https://github.com/peter-yys-yoon/pegcnv2'}], 'metrics': {'Accuracy (Cross-Setup)': '82.7%', 'Accuracy (Cross-Subject)': '81.1%'}, 'model_links': [], 'model_name': '3s RA-GCN', 'paper_date': '2020-08-09', 'paper_title': 'Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2008.03791v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '83.2%', 'Accuracy (Cross-Subject)': '80.5%'}, 'model_links': [], 'model_name': 'Mix-Dimension', 'paper_date': '2020-07-30', 'paper_title': 'Mix Dimension in Poincar√© Geometry for 3D Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.15678v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '79.8%', 'Accuracy (Cross-Subject)': '78.3%'}, 'model_links': [], 'model_name': 'GVFE + AS-GCN with DH-TCN', 'paper_date': '2019-12-20', 'paper_title': 'Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition', 'paper_url': 'https://arxiv.org/abs/1912.09745v1', 'uses_additional_data': False}, {'code_links': [{'title': 'airglow/gimme_signals_action_recognition', 'url': 'https://github.com/airglow/gimme_signals_action_recognition'}, {'title': 'raphaelmemmesheimer/gimme_signals_action_recognition', 'url': 'https://github.com/raphaelmemmesheimer/gimme_signals_action_recognition'}], 'metrics': {'Accuracy (Cross-Setup)': '71.6%', 'Accuracy (Cross-Subject)': '70.8%'}, 'model_links': [], 'model_name': 'Gimme Signals (Skeleton, AIS)', 'paper_date': '2020-03-13', 'paper_title': 'Gimme Signals: Discriminative signal encoding for multimodal activity recognition', 'paper_url': 'https://arxiv.org/abs/2003.06156v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '67.2%', 'Accuracy (Cross-Subject)': '68.3%'}, 'model_links': [], 'model_name': 'Logsig-RNN', 'paper_date': '2019-08-22', 'paper_title': 'Learning stochastic differential equations using RNN with log signature features', 'paper_url': 'https://arxiv.org/abs/1908.08286v2', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (Cross-Setup)': '62.8%', 'Accuracy (Cross-Subject)': '67.9%'}, 'model_links': [], 'model_name': 'TSRJI (Late Fusion) + HCN', 'paper_date': '2019-09-11', 'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints', 'paper_url': 'https://arxiv.org/abs/1909.05704v1', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (Cross-Setup)': '66.9%', 'Accuracy (Cross-Subject)': '67.7%'}, 'model_links': [], 'model_name': 'SkeleMotion + Yang et al. (2018)', 'paper_date': '2019-07-30', 'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition', 'paper_url': 'https://arxiv.org/abs/1907.13025v1', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (Cross-Setup)': '59.7%', 'Accuracy (Cross-Subject)': '65.5%'}, 'model_links': [], 'model_name': 'TSRJI (Late Fusion)', 'paper_date': '2019-09-11', 'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints', 'paper_url': 'https://arxiv.org/abs/1909.05704v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '66.9%', 'Accuracy (Cross-Subject)': '64.6%'}, 'model_links': [], 'model_name': 'Body Pose Evolution Map', 'paper_date': '2018-06-01', 'paper_title': 'Recognizing Human Actions as the Evolution of Pose Estimation Maps', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Recognizing_Human_Actions_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (Cross-Setup)': '63.0%', 'Accuracy (Cross-Subject)': '62.9%'}, 'model_links': [], 'model_name': 'SkeleMotion [Magnitude-Orientation (TSA)]', 'paper_date': '2019-07-30', 'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition', 'paper_url': 'https://arxiv.org/abs/1907.13025v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '61.8%', 'Accuracy (Cross-Subject)': '62.2%'}, 'model_links': [], 'model_name': 'Multi-Task CNN with RotClips', 'paper_date': '2018-03-05', 'paper_title': 'Learning clip representations for skeleton-based 3d action recognition', 'paper_url': 'https://doi.org/10.1109/TIP.2018.2812099', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '63.3%', 'Accuracy (Cross-Subject)': '61.2%'}, 'model_links': [], 'model_name': 'Two-Stream Attention LSTM', 'paper_date': '2017-07-18', 'paper_title': 'Skeleton-Based Human Action Recognition with Global Context-Aware Attention LSTM Networks', 'paper_url': 'http://arxiv.org/abs/1707.05740v5', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '63.2%', 'Accuracy (Cross-Subject)': '60.3%'}, 'model_links': [], 'model_name': 'Skeleton Visualization (Single Stream)', 'paper_date': '2017-08-01', 'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition', 'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '62.4%', 'Accuracy (Cross-Subject)': '59.9%'}, 'model_links': [], 'model_name': 'FSNet', 'paper_date': '2019-02-08', 'paper_title': 'Skeleton-Based Online Action Prediction Using Scale Selection Network', 'paper_url': 'http://arxiv.org/abs/1902.03084v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '57.9%', 'Accuracy (Cross-Subject)': '58.4%'}, 'model_links': [], 'model_name': 'Multi-Task Learning Network', 'paper_date': '2017-03-09', 'paper_title': 'A New Representation of Skeleton Sequences for 3D Action Recognition', 'paper_url': 'http://arxiv.org/abs/1703.03492v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '59.2%', 'Accuracy (Cross-Subject)': '58.3%'}, 'model_links': [], 'model_name': 'GCA-LSTM', 'paper_date': '2017-07-01', 'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '60.9%', 'Accuracy (Cross-Subject)': '58.2%'}, 'model_links': [], 'model_name': 'Internal Feature Fusion', 'paper_date': '2017-06-26', 'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates', 'paper_url': 'http://arxiv.org/abs/1706.08276v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '57.9%', 'Accuracy (Cross-Subject)': '55.7%'}, 'model_links': [], 'model_name': 'Spatio-Temporal LSTM', 'paper_date': '2016-07-24', 'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1607.07043v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '54.7%', 'Accuracy (Cross-Subject)': '50.8%'}, 'model_links': [], 'model_name': 'Dynamic Skeletons', 'paper_date': '2016-12-15', 'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition', 'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '44.9%', 'Accuracy (Cross-Subject)': '36.3%'}, 'model_links': [], 'model_name': 'Soft RNN', 'paper_date': '2018-08-06', 'paper_title': 'Early action prediction by soft regression', 'paper_url': 'https://doi.org/10.1109/TPAMI.2018.2863279', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (Cross-Setup)': '26.3%', 'Accuracy (Cross-Subject)': '25.5%'}, 'model_links': [], 'model_name': 'Part-Aware LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'HDM05', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-hdm05'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '89.80%'}, 'model_links': [], 'model_name': 'Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Gaming 3D (G3D)', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-gaming'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '96.0'}, 'model_links': [], 'model_name': 'CNN', 'paper_date': '2016-12-30', 'paper_title': 'Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks', 'paper_url': 'http://arxiv.org/abs/1612.09401v1', 'uses_additional_data': False}, {'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '92.91%'}, 'model_links': [], 'model_name': 'Temporal K-Means Clustering + Temporal Covariance Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}, {'code_links': [{'title': 'rort1989/HDM', 'url': 'https://github.com/rort1989/HDM'}], 'metrics': {'Accuracy': '92.0'}, 'model_links': [], 'model_name': 'HDM-BG', 'paper_date': '2019-06-01', 'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '90.94'}, 'model_links': [], 'model_name': 'Rolling Rotations (FTP)', 'paper_date': '2016-06-27', 'paper_title': 'Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data', 'paper_url': 'https://doi.org/10.1109/CVPR.2016.484', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UWA3D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-uwa3d'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '81.4%'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '73.8%'}, 'model_links': [], 'model_name': 'ESV (Synthesized + Pre-trained)', 'paper_date': '2017-08-01', 'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition', 'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '17.7%'}, 'model_links': [], 'model_name': 'HOJ3D', 'paper_date': '2012-07-16', 'paper_title': 'View invariant human action recognition using histograms of 3D joints', 'paper_url': 'https://doi.org/10.1109/CVPRW.2012.6239233', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'MSRC-12', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msrc-12'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '99.08%'}, 'model_links': [], 'model_name': 'Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Kinetics-Skeleton dataset', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-kinetics'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}], 'metrics': {'Accuracy': '47.7'}, 'model_links': [], 'model_name': 'PoseC3D (w. HRNet 2D skeleton)', 'paper_date': '2021-04-28', 'paper_title': 'Revisiting Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2104.13586v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '38.6'}, 'model_links': [], 'model_name': '2s-AGCN+TEM', 'paper_date': '2020-03-19', 'paper_title': 'Temporal Extension Module for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.08951v2', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/ms-g3d', 'url': 'https://github.com/kenziyuliu/ms-g3d'}], 'metrics': {'Accuracy': '38.0'}, 'model_links': [], 'model_name': 'MS-G3D', 'paper_date': '2020-03-31', 'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.14111v2', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}], 'metrics': {'Accuracy': '38.0'}, 'model_links': [], 'model_name': 'PoseC3D', 'paper_date': '2021-04-28', 'paper_title': 'Revisiting Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2104.13586v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '37.9'}, 'model_links': [], 'model_name': 'Dynamic GCN', 'paper_date': '2020-07-29', 'paper_title': 'Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.14690v1', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '37.8'}, 'model_links': [], 'model_name': 'MS-AAGCN', 'paper_date': '2019-12-15', 'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1912.06971v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '37.5'}, 'model_links': [], 'model_name': 'CGCN', 'paper_date': '2020-03-06', 'paper_title': 'Centrality Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.03007v1', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '37.4'}, 'model_links': [], 'model_name': 'JB-AAGCN', 'paper_date': '2019-12-15', 'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1912.06971v1', 'uses_additional_data': False}, {'code_links': [{'title': 'Chiaraplizz/ST-TR', 'url': 'https://github.com/Chiaraplizz/ST-TR'}], 'metrics': {'Accuracy': '37.4'}, 'model_links': [], 'model_name': 'ST-TR-agcn', 'paper_date': '2020-08-17', 'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks', 'paper_url': 'https://arxiv.org/abs/2008.07404v4', 'uses_additional_data': False}, {'code_links': [{'title': 'xiaoiker/GCN-NAS', 'url': 'https://github.com/xiaoiker/GCN-NAS'}], 'metrics': {'Accuracy': '37.1'}, 'model_links': [], 'model_name': 'GCN-NAS', 'paper_date': '2019-11-11', 'paper_title': 'Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching', 'paper_url': 'https://arxiv.org/abs/1911.04131v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch', 'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}], 'metrics': {'Accuracy': '36.9'}, 'model_links': [], 'model_name': 'DGNN', 'paper_date': '2019-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '36.6'}, 'model_links': [], 'model_name': 'SLnL-rFA', 'paper_date': '2018-11-10', 'paper_title': 'Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention', 'paper_url': 'https://arxiv.org/abs/1811.04237v3', 'uses_additional_data': False}, {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal', 'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'}, {'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '36.1'}, 'model_links': [], 'model_name': '2s-AGCN', 'paper_date': '2018-05-20', 'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1805.07694v3', 'uses_additional_data': False}, {'code_links': [{'title': 'limaosen0/AS-GCN', 'url': 'https://github.com/limaosen0/AS-GCN'}], 'metrics': {'Accuracy': '34.8'}, 'model_links': [], 'model_name': 'AS-GCN', 'paper_date': '2019-04-26', 'paper_title': 'Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1904.12659v1', 'uses_additional_data': False}, {'code_links': [{'title': 'andreYoo/PeGCNs', 'url': 'https://github.com/andreYoo/PeGCNs'}], 'metrics': {'Accuracy': '34.8'}, 'model_links': [], 'model_name': 'PeGCN', 'paper_date': '2020-03-17', 'paper_title': 'Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.07514v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '33.7 '}, 'model_links': [], 'model_name': 'PR-GCN', 'paper_date': '2020-10-14', 'paper_title': 'Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2010.07367v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '33.6'}, 'model_links': [], 'model_name': 'ST-GR', 'paper_date': '2019-07-17', 'paper_title': 'Spatiotemporal graph routing for skeleton-based action recognition', 'paper_url': 'https://doi.org/10.1609/aaai.v33i01.33018561', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '33.5'}, 'model_links': [], 'model_name': 'AR-GCN', 'paper_date': '2019-11-27', 'paper_title': 'An Attention-Enhanced Recurrent Graph Convolutional Network for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1145/3372806.3372814', 'uses_additional_data': False}, {'code_links': [{'title': 'raymondyeh07/chirality_nets', 'url': 'https://github.com/raymondyeh07/chirality_nets'}], 'metrics': {'Accuracy': '30.9'}, 'model_links': [], 'model_name': 'Ours-Conv-Chiral', 'paper_date': '2019-10-31', 'paper_title': 'Chirality Nets for Human Pose Regression', 'paper_url': 'https://arxiv.org/abs/1911.00029v1', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmskeleton', 'url': 'https://github.com/open-mmlab/mmskeleton'}, {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'}, {'title': 'ericksiavichay/cs230-final-project', 'url': 'https://github.com/ericksiavichay/cs230-final-project'}, {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'}, {'title': 'stillarrow/S2VT_ACT', 'url': 'https://github.com/stillarrow/S2VT_ACT'}, {'title': 'ZhangNYG/ST-GCN', 'url': 'https://github.com/ZhangNYG/ST-GCN'}, {'title': 'DixinFan/st-gcn', 'url': 'https://github.com/DixinFan/st-gcn'}, {'title': 'ken724049/action-recognition', 'url': 'https://github.com/ken724049/action-recognition'}, {'title': 'antoniolq/st-gcn', 'url': 'https://github.com/antoniolq/st-gcn'}, {'title': 'github-zbx/ST-GCN', 'url': 'https://github.com/github-zbx/ST-GCN'}, {'title': 'GeyuanZhang/st-gcn-master', 'url': 'https://github.com/GeyuanZhang/st-gcn-master'}, {'title': 'KrisLee512/ST-GCN', 'url': 'https://github.com/KrisLee512/ST-GCN'}, {'title': 'AbiterVX/ST-GCN', 'url': 'https://github.com/AbiterVX/ST-GCN'}], 'metrics': {'Accuracy': '30.7'}, 'model_links': [], 'model_name': 'ST-GCN', 'paper_date': '2018-01-23', 'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1801.07455v2', 'uses_additional_data': False}, {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition', 'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}], 'metrics': {'Accuracy': '20.3'}, 'model_links': [], 'model_name': 'Res-TCN', 'paper_date': '2017-04-14', 'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks', 'paper_url': 'http://arxiv.org/abs/1704.04516v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'J-HMBD Early Action', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-j-hmbd'}], 'description': '', 'sota': {'metrics': ['10%'], 'rows': [{'code_links': [{'title': 'ser-art/RAE-vs-AE', 'url': 'https://github.com/ser-art/RAE-vs-AE'}, {'title': 'rk68657/AutoEncoders', 'url': 'https://github.com/rk68657/AutoEncoders'}], 'metrics': {'10%': '60.6'}, 'model_links': [], 'model_name': 'DR^2N', 'paper_date': '2018-02-09', 'paper_title': 'Relational Autoencoder for Feature Extraction', 'paper_url': 'http://arxiv.org/abs/1802.03145v1', 'uses_additional_data': False}, {'code_links': [{'title': 'labmlai/annotated_deep_learning_paper_implementations', 'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/graphs/gat'}, {'title': 'PetarV-/GAT', 'url': 'https://github.com/PetarV-/GAT'}, {'title': 'Diego999/pyGAT', 'url': 'https://github.com/Diego999/pyGAT'}, {'title': 'gordicaleksa/pytorch-GAT', 'url': 'https://github.com/gordicaleksa/pytorch-GAT'}, {'title': 'shenweichen/GraphNeuralNetwork', 'url': 'https://github.com/shenweichen/GraphNeuralNetwork'}, {'title': 'danielegrattarola/keras-gat', 'url': 'https://github.com/danielegrattarola/keras-gat'}, {'title': 'HazyResearch/hgcn', 'url': 'https://github.com/HazyResearch/hgcn'}, {'title': 'GraphSAINT/GraphSAINT', 'url': 'https://github.com/GraphSAINT/GraphSAINT'}, {'title': 'lukecavabarrett/pna', 'url': 'https://github.com/lukecavabarrett/pna'}, {'title': 'Kaimaoge/IGNNK', 'url': 'https://github.com/Kaimaoge/IGNNK'}, {'title': 'zhao-tong/GNNs-easy-to-use', 'url': 'https://github.com/zhao-tong/GNNs-easy-to-use'}, {'title': 'snowkylin/gnn', 'url': 'https://github.com/snowkylin/gnn'}, {'title': 'marblet/GNN_models_pytorch_geometric', 'url': 'https://github.com/marblet/GNN_models_pytorch_geometric'}, {'title': 'marble0117/GNN_models_pytorch', 'url': 'https://github.com/marble0117/GNN_models_pytorch'}, {'title': 'marble0117/GNN_models_pytorch_geometric', 'url': 'https://github.com/marble0117/GNN_models_pytorch_geometric'}, {'title': 'HeapHop30/graph-attention-nets', 'url': 'https://github.com/HeapHop30/graph-attention-nets'}, {'title': 'weiyangfb/PyTorchSparseGAT', 'url': 'https://github.com/weiyangfb/PyTorchSparseGAT'}, {'title': 'marblet/gat-pytorch', 'url': 'https://github.com/marblet/gat-pytorch'}, {'title': 'ds4dm/sparse-gcn', 'url': 'https://github.com/ds4dm/sparse-gcn'}, {'title': 'ds4dm/sGat', 'url': 'https://github.com/ds4dm/sGat'}, {'title': 'gcucurull/jax-gat', 'url': 'https://github.com/gcucurull/jax-gat'}, {'title': 'calciver/Graph-Attention-Networks', 'url': 'https://github.com/calciver/Graph-Attention-Networks'}, {'title': 'noahtren/Graph-Attention-Networks-TensorFlow-2', 'url': 'https://github.com/noahtren/Graph-Attention-Networks-TensorFlow-2'}, {'title': 'Yindong-Zhang/myGAT', 'url': 'https://github.com/Yindong-Zhang/myGAT'}, {'title': 'liu6zijian/simplified-gcn-model', 'url': 'https://github.com/liu6zijian/simplified-gcn-model'}, {'title': 'taishan1994/pytorch_gat', 'url': 'https://github.com/taishan1994/pytorch_gat'}, {'title': 'BIG-S2/keras-gnm', 'url': 'https://github.com/BIG-S2/keras-gnm'}, {'title': 'Aveek-Saha/Graph-Attention-Net', 'url': 'https://github.com/Aveek-Saha/Graph-Attention-Net'}, {'title': 'zxhhh97/ABot', 'url': 'https://github.com/zxhhh97/ABot'}, {'title': 'qema/orca-py', 'url': 'https://github.com/qema/orca-py'}, {'title': 'mitya8128/experiments_notes', 'url': 'https://github.com/mitya8128/experiments_notes'}, {'title': 'giuseppefutia/link-prediction-code', 'url': 'https://github.com/giuseppefutia/link-prediction-code'}, {'title': 'AngusMonroe/GAT-pytorch', 'url': 'https://github.com/AngusMonroe/GAT-pytorch'}, {'title': 'handasontam/GAT-with-edgewise-attention', 'url': 'https://github.com/handasontam/GAT-with-edgewise-attention'}, {'title': 'fongyk/graph-attention', 'url': 'https://github.com/fongyk/graph-attention'}, {'title': 'mlzxzhou/keras-gnm', 'url': 'https://github.com/mlzxzhou/keras-gnm'}, {'title': 'YunseobShin/wiki_GAT', 'url': 'https://github.com/YunseobShin/wiki_GAT'}, {'title': 'dzb1998/pyGAT', 'url': 'https://github.com/dzb1998/pyGAT'}, {'title': 'Anou9531/GAT', 'url': 'https://github.com/Anou9531/GAT'}, {'title': 'WantingZhao/my_GAT', 'url': 'https://github.com/WantingZhao/my_GAT'}, {'title': 'TyngJiunKuo/deep-learning-project', 'url': 'https://github.com/TyngJiunKuo/deep-learning-project'}, {'title': 'iwzy7071/graph_neural_network', 'url': 'https://github.com/iwzy7071/graph_neural_network'}, {'title': 'PumpkinYing/GAT', 'url': 'https://github.com/PumpkinYing/GAT'}, {'title': 'anish-lu-yihe/SVRT-by-GAT', 'url': 'https://github.com/anish-lu-yihe/SVRT-by-GAT'}, {'title': 'ChengSashankh/trying-GAT', 'url': 'https://github.com/ChengSashankh/trying-GAT'}, {'title': 'subercui/pyGConvAT', 'url': 'https://github.com/subercui/pyGConvAT'}, {'title': 'ChengyuSun/gat', 'url': 'https://github.com/ChengyuSun/gat'}, {'title': 'blueberryc/pyGAT', 'url': 'https://github.com/blueberryc/pyGAT'}, {'title': 'zhangbo2008/GAT_network', 'url': 'https://github.com/zhangbo2008/GAT_network'}, {'title': 'whut2962575697/gat_sementic_segmentation', 'url': 'https://github.com/whut2962575697/gat_sementic_segmentation'}, {'title': 'RJ12138/Multilevel', 'url': 'https://github.com/RJ12138/Multilevel'}, {'title': 'galkampel/HyperNetworks', 'url': 'https://github.com/galkampel/HyperNetworks'}, {'title': 'tlmakinen/GAT-noise', 'url': 'https://github.com/tlmakinen/GAT-noise'}, {'title': 'Anak2016/GAT', 'url': 'https://github.com/Anak2016/GAT'}, {'title': 'snownus/COOP', 'url': 'https://github.com/snownus/COOP'}, {'title': 'ColdenChan/GAT_D', 'url': 'https://github.com/ColdenChan/GAT_D'}], 'metrics': {'10%': '58.1'}, 'model_links': [], 'model_name': 'GAT', 'paper_date': '2017-10-30', 'paper_title': 'Graph Attention Networks', 'paper_url': 'http://arxiv.org/abs/1710.10903v3', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Varying-view RGB-D Action-Skeleton', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-varying'}], 'description': '', 'sota': {'metrics': ['Accuracy (CS)', 'Accuracy (CV I)', 'Accuracy (CV II)', 'Accuracy (AV I)', 'Accuracy (AV II)'], 'rows': [{'code_links': [], 'metrics': {'Accuracy (AV I)': '57%', 'Accuracy (AV II)': '75%', 'Accuracy (CS)': '76%', 'Accuracy (CV I)': '29%', 'Accuracy (CV II)': '71%'}, 'model_links': [], 'model_name': 'VS-CNN', 'paper_date': '2019-04-24', 'paper_title': 'A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1904.10681v1', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmskeleton', 'url': 'https://github.com/open-mmlab/mmskeleton'}, {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'}, {'title': 'ericksiavichay/cs230-final-project', 'url': 'https://github.com/ericksiavichay/cs230-final-project'}, {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'}, {'title': 'stillarrow/S2VT_ACT', 'url': 'https://github.com/stillarrow/S2VT_ACT'}, {'title': 'ZhangNYG/ST-GCN', 'url': 'https://github.com/ZhangNYG/ST-GCN'}, {'title': 'DixinFan/st-gcn', 'url': 'https://github.com/DixinFan/st-gcn'}, {'title': 'ken724049/action-recognition', 'url': 'https://github.com/ken724049/action-recognition'}, {'title': 'antoniolq/st-gcn', 'url': 'https://github.com/antoniolq/st-gcn'}, {'title': 'github-zbx/ST-GCN', 'url': 'https://github.com/github-zbx/ST-GCN'}, {'title': 'GeyuanZhang/st-gcn-master', 'url': 'https://github.com/GeyuanZhang/st-gcn-master'}, {'title': 'KrisLee512/ST-GCN', 'url': 'https://github.com/KrisLee512/ST-GCN'}, {'title': 'AbiterVX/ST-GCN', 'url': 'https://github.com/AbiterVX/ST-GCN'}], 'metrics': {'Accuracy (AV I)': '53%', 'Accuracy (AV II)': '43%', 'Accuracy (CS)': '71%', 'Accuracy (CV I)': '25%', 'Accuracy (CV II)': '56%'}, 'model_links': [], 'model_name': 'ST-GCN', 'paper_date': '2018-01-23', 'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1801.07455v2', 'uses_additional_data': False}, {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition', 'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}], 'metrics': {'Accuracy (AV I)': '48%', 'Accuracy (AV II)': '68%', 'Accuracy (CS)': '63%', 'Accuracy (CV I)': '14%', 'Accuracy (CV II)': '48%'}, 'model_links': [], 'model_name': 'Res-TCN', 'paper_date': '2017-04-14', 'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks', 'paper_url': 'http://arxiv.org/abs/1704.04516v1', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (AV I)': '33%', 'Accuracy (AV II)': '50%', 'Accuracy (CS)': '60%', 'Accuracy (CV I)': '13%', 'Accuracy (CV II)': '33%'}, 'model_links': [], 'model_name': 'P-LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (AV I)': '43%', 'Accuracy (AV II)': '77%', 'Accuracy (CS)': '59%', 'Accuracy (CV I)': '26%', 'Accuracy (CV II)': '68%'}, 'model_links': [], 'model_name': 'SK-CNN', 'paper_date': '2017-08-01', 'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition', 'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030', 'uses_additional_data': False}, {'code_links': [{'title': 'coderSkyChen/Action_Recognition_Zoo', 'url': 'https://github.com/coderSkyChen/Action_Recognition_Zoo'}, {'title': 'colincsl/TemporalConvolutionalNetworks', 'url': 'https://github.com/colincsl/TemporalConvolutionalNetworks'}, {'title': 'yz-cnsdqz/TemporalActionParsing-FineGrained', 'url': 'https://github.com/yz-cnsdqz/TemporalActionParsing-FineGrained'}, {'title': 'sadari1/TumorDetectionDeepLearning', 'url': 'https://github.com/sadari1/TumorDetectionDeepLearning'}, {'title': 'BehnooshParsa/HumanActionRecognition_with_ErgonomicRisk', 'url': 'https://github.com/BehnooshParsa/HumanActionRecognition_with_ErgonomicRisk'}], 'metrics': {'Accuracy (AV I)': '43%', 'Accuracy (AV II)': '64%', 'Accuracy (CS)': '56%', 'Accuracy (CV I)': '16%', 'Accuracy (CV II)': '43%'}, 'model_links': [], 'model_name': 'TCN', 'paper_date': '2016-11-16', 'paper_title': 'Temporal Convolutional Networks for Action Segmentation and Detection', 'paper_url': 'http://arxiv.org/abs/1611.05267v1', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (AV I)': '31%', 'Accuracy (AV II)': '68%', 'Accuracy (CS)': '56%', 'Accuracy (CV I)': '16%', 'Accuracy (CV II)': '31%'}, 'model_links': [], 'model_name': 'LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Florence 3D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-florence'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '99.1%'}, 'model_links': [], 'model_name': 'Deep STGC_K', 'paper_date': '2018-02-27', 'paper_title': 'Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1802.09834v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '98.4%'}, 'model_links': [], 'model_name': 'Complete GR-GCN', 'paper_date': '2018-11-29', 'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression', 'paper_url': 'http://arxiv.org/abs/1811.12013v2', 'uses_additional_data': False}, {'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '95.81%'}, 'model_links': [], 'model_name': 'Temporal Spectral Clustering + Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '91.40%'}, 'model_links': [], 'model_name': 'Rolling Rotations (FTP)', 'paper_date': '2016-06-27', 'paper_title': 'Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data', 'paper_url': 'https://doi.org/10.1109/CVPR.2016.484', 'uses_additional_data': False}, {'code_links': [{'title': 'fdsa1860/skeletal', 'url': 'https://github.com/fdsa1860/skeletal'}], 'metrics': {'Accuracy': '90.9%'}, 'model_links': [], 'model_name': 'Lie Group', 'paper_date': '2014-06-23', 'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group', 'paper_url': 'https://doi.org/10.1109/CVPR.2014.82', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'NTU60-X', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu60-x'}], 'description': '', 'sota': {'metrics': ['Accuracy (Body + Fingers joints)', 'Accuracy (Body joints)', 'Accuracy (Body + Fingers + Face joints)'], 'rows': [{'code_links': [{'title': 'skelemoa/ntu-x', 'url': 'https://github.com/skelemoa/ntu-x'}], 'metrics': {'Accuracy (Body + Fingers + Face joints)': '89.64', 'Accuracy (Body + Fingers joints)': '91.78', 'Accuracy (Body joints)': '89.56'}, 'model_links': [], 'model_name': '4s-ShiftGCN', 'paper_date': '2021-01-27', 'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions', 'paper_url': 'https://arxiv.org/abs/2101.11529v2', 'uses_additional_data': False}, {'code_links': [{'title': 'skelemoa/ntu-x', 'url': 'https://github.com/skelemoa/ntu-x'}], 'metrics': {'Accuracy (Body + Fingers + Face joints)': '91.12', 'Accuracy (Body + Fingers joints)': '91.76', 'Accuracy (Body joints)': '91.26'}, 'model_links': [], 'model_name': 'MS-G3D', 'paper_date': '2021-01-27', 'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions', 'paper_url': 'https://arxiv.org/abs/2101.11529v2', 'uses_additional_data': False}, {'code_links': [{'title': 'skelemoa/ntu-x', 'url': 'https://github.com/skelemoa/ntu-x'}], 'metrics': {'Accuracy (Body + Fingers + Face joints)': '89.79', 'Accuracy (Body + Fingers joints)': '91.64', 'Accuracy (Body joints)': '89.98'}, 'model_links': [], 'model_name': 'PA-ResGCN', 'paper_date': '2021-01-27', 'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions', 'paper_url': 'https://arxiv.org/abs/2101.11529v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'TCG-dataset', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-tcg'}], 'description': '', 'sota': {'metrics': ['Acc'], 'rows': [{'code_links': [{'title': 'againerju/tcg_recognition', 'url': 'https://github.com/againerju/tcg_recognition'}], 'metrics': {'Acc': '87.24'}, 'model_links': [], 'model_name': 'Bidirectional LSTM', 'paper_date': '2020-07-31', 'paper_title': 'Traffic Control Gesture Recognition for Autonomous Vehicles', 'paper_url': 'https://arxiv.org/abs/2007.16072v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'SHREC 2017 track on 3D Hand Gesture Recognition', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-shrec'}], 'description': '', 'sota': {'metrics': ['28 gestures accuracy', 'Accuracy', '14 gestures accuracy', 'No. parameters', 'Speed  (FPS)'], 'rows': [{'code_links': [{'title': 'fandulu/DD-Net', 'url': 'https://github.com/fandulu/DD-Net'}, {'title': 'BlurryLight/DD-Net-Pytorch', 'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}], 'metrics': {'14 gestures accuracy': '94.6', '28 gestures accuracy': '91.9', 'Accuracy': '94.6 (14  gestures) , 91.9 (28 gestures )', 'No. parameters': '1.82M', 'Speed  (FPS)': '2,200'}, 'model_links': [], 'model_name': 'DD-Net', 'paper_date': '2019-07-23', 'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better', 'paper_url': 'https://arxiv.org/abs/1907.09658v8', 'uses_additional_data': False}, {'code_links': [{'title': 'AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition', 'url': 'https://github.com/AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition'}], 'metrics': {'14 gestures accuracy': '93.57', '28 gestures accuracy': '91.43'}, 'model_links': [], 'model_name': 'TCN-Summ', 'paper_date': '2021-03-03', 'paper_title': 'Domain and View-point Agnostic Hand Action Recognition', 'paper_url': 'https://arxiv.org/abs/2103.02303v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'14 gestures accuracy': '93.6', '28 gestures accuracy': '90.7', 'Speed  (FPS)': '161'}, 'model_links': [], 'model_name': 'STA-Res-TCN', 'paper_date': '2019-01-23', 'paper_title': 'Spatial-Temporal Attention Res-TCN for Skeleton-Based Dynamic Hand Gesture Recognition', 'paper_url': 'https://link.springer.com/chapter/10.1007/978-3-030-11024-6_18', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'14 gestures accuracy': '91.3', '28 gestures accuracy': '86.6', 'Speed  (FPS)': '361'}, 'model_links': [], 'model_name': 'MFA-Net', 'paper_date': '2019-01-10', 'paper_title': 'Motion feature augmented network for dynamic hand gesture recognition from skeletal data', 'paper_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6359639/', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'NTU RGB+D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu-rgbd'}], 'description': '', 'sota': {'metrics': ['Accuracy (CV)', 'Accuracy (CS)'], 'rows': [{'code_links': [{'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}], 'metrics': {'Accuracy (CS)': '94.1', 'Accuracy (CV)': '97.1'}, 'model_links': [], 'model_name': 'PoseC3D (w. HRNet 2D skeleton)', 'paper_date': '2021-04-28', 'paper_title': 'Revisiting Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2104.13586v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '91.0', 'Accuracy (CV)': '96.5'}, 'model_links': [], 'model_name': 'MS-AAGCN+TEM', 'paper_date': '2020-03-19', 'paper_title': 'Temporal Extension Module for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.08951v2', 'uses_additional_data': False}, {'code_links': [{'title': 'kchengiva/Shift-GCN', 'url': 'https://github.com/kchengiva/Shift-GCN'}], 'metrics': {'Accuracy (CS)': '90.7', 'Accuracy (CV)': '96.5'}, 'model_links': [], 'model_name': '4s Shift-GCN', 'paper_date': '2020-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '91.7', 'Accuracy (CV)': '96.4'}, 'model_links': [], 'model_name': 'AngNet-JA + BA + JBA + VJBA', 'paper_date': '2021-05-04', 'paper_title': 'Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2105.01563v3', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/DSTA-Net', 'url': 'https://github.com/lshiwjx/DSTA-Net'}], 'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.4'}, 'model_links': [], 'model_name': 'DSTA-Net', 'paper_date': '2020-07-07', 'paper_title': 'Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.03263v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '90.3', 'Accuracy (CV)': '96.4'}, 'model_links': [], 'model_name': 'CGCN', 'paper_date': '2020-03-06', 'paper_title': 'Centrality Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.03007v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '90.1', 'Accuracy (CV)': '96.4'}, 'model_links': [], 'model_name': 'Sym-GNN', 'paper_date': '2019-10-05', 'paper_title': 'Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction', 'paper_url': 'https://arxiv.org/abs/1910.02212v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '90.3', 'Accuracy (CV)': '96.3'}, 'model_links': [], 'model_name': 'BAGCN', 'paper_date': '2019-12-24', 'paper_title': 'Focusing and Diffusion: Bidirectional Attentive Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1912.11521v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '90.2', 'Accuracy (CV)': '96.3'}, 'model_links': [], 'model_name': 'FGCN-spatial+FGCN-motion', 'paper_date': '2020-03-17', 'paper_title': 'Feedback Graph Convolutional Network for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.07564v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/ms-g3d', 'url': 'https://github.com/kenziyuliu/ms-g3d'}], 'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.2'}, 'model_links': [], 'model_name': 'MS-G3D Net', 'paper_date': '2020-03-31', 'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.14111v2', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy (CS)': '90.0', 'Accuracy (CV)': '96.2'}, 'model_links': [], 'model_name': 'MS-AAGCN', 'paper_date': '2019-12-15', 'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1912.06971v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch', 'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}], 'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '96.1'}, 'model_links': [], 'model_name': 'DGNN', 'paper_date': '2019-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'Chiaraplizz/ST-TR', 'url': 'https://github.com/Chiaraplizz/ST-TR'}], 'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '96.1'}, 'model_links': [], 'model_name': 'ST-TR', 'paper_date': '2020-08-17', 'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks', 'paper_url': 'https://arxiv.org/abs/2008.07404v4', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.0'}, 'model_links': [], 'model_name': 'Dynamic GCN', 'paper_date': '2020-07-29', 'paper_title': 'Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.14690v1', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/ResGCNv1', 'url': 'https://github.com/yfsong0709/ResGCNv1'}, {'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (CS)': '90.9', 'Accuracy (CV)': '96'}, 'model_links': [], 'model_name': 'PA-ResGCN-B19', 'paper_date': '2020-10-20', 'paper_title': 'Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2010.09978v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '89.7', 'Accuracy (CV)': '96'}, 'model_links': [], 'model_name': 'Mix-Dimension', 'paper_date': '2020-07-30', 'paper_title': 'Mix Dimension in Poincar√© Geometry for 3D Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.15678v2', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '96.0'}, 'model_links': [], 'model_name': 'JB-AAGCN', 'paper_date': '2019-12-15', 'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1912.06971v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '89.58', 'Accuracy (CV)': '95.74'}, 'model_links': [], 'model_name': '2s-SDGCN', 'paper_date': '2019-10-27', 'paper_title': 'Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_ICCVW_2019/html/SGRL/Wu_Spatial_Residual_Layer_and_Dense_Connection_Block_Enhanced_Spatial_Temporal_ICCVW_2019_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (CS)': '91.7', 'Accuracy (CV)': '95.7'}, 'model_links': [], 'model_name': 'EfficientGCN-B4', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [{'title': 'xiaoiker/GCN-NAS', 'url': 'https://github.com/xiaoiker/GCN-NAS'}], 'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '95.7'}, 'model_links': [], 'model_name': 'GCN-NAS', 'paper_date': '2019-11-11', 'paper_title': 'Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching', 'paper_url': 'https://arxiv.org/abs/1911.04131v1', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (CS)': '90.9', 'Accuracy (CV)': '95.5'}, 'model_links': [], 'model_name': 'EfficientGCN-B2', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal', 'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'}, {'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy (CS)': '88.5', 'Accuracy (CV)': '95.1'}, 'model_links': [], 'model_name': '2s-AGCN', 'paper_date': '2018-05-20', 'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1805.07694v3', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}], 'metrics': {'Accuracy (CS)': '88.5', 'Accuracy (CV)': '95.1'}, 'model_links': [], 'model_name': '2s-NLGCN', 'paper_date': '2019-07-04', 'paper_title': 'Non-Local Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1805.07694v2', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '95.0'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '89.2', 'Accuracy (CV)': '95.0'}, 'model_links': [], 'model_name': 'AGC-LSTM (Joint&amp;Part)', 'paper_date': '2019-02-25', 'paper_title': 'An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1902.09130v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '89.1', 'Accuracy (CV)': '94.9'}, 'model_links': [], 'model_name': 'SLnL-rFA', 'paper_date': '2018-11-10', 'paper_title': 'Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention', 'paper_url': 'https://arxiv.org/abs/1811.04237v3', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '94.7'}, 'model_links': [], 'model_name': 'EfficientGCN-B0', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '87.5', 'Accuracy (CV)': '94.3'}, 'model_links': [], 'model_name': 'GR-GCN', 'paper_date': '2018-11-29', 'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression', 'paper_url': 'http://arxiv.org/abs/1811.12013v2', 'uses_additional_data': False}, {'code_links': [{'title': 'limaosen0/AS-GCN', 'url': 'https://github.com/limaosen0/AS-GCN'}], 'metrics': {'Accuracy (CS)': '86.8', 'Accuracy (CV)': '94.2'}, 'model_links': [], 'model_name': 'AS-GCN', 'paper_date': '2019-04-26', 'paper_title': 'Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1904.12659v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '86.2', 'Accuracy (CV)': '94.2'}, 'model_links': [], 'model_name': 'Sem-GCN', 'paper_date': '2020-05-01', 'paper_title': 'A Semantics-Guided Graph Convolutional Network for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1145/3390557.3394129', 'uses_additional_data': False}, {'code_links': [{'title': 'Sunnydreamrain/IndRNN_pytorch', 'url': 'https://github.com/Sunnydreamrain/IndRNN_pytorch'}], 'metrics': {'Accuracy (CS)': '86.70', 'Accuracy (CV)': '93.97'}, 'model_links': [], 'model_name': 'Dense IndRNN', 'paper_date': '2019-10-11', 'paper_title': 'Deep Independently Recurrent Neural Network (IndRNN)', 'paper_url': 'https://arxiv.org/abs/1910.06251v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '88.6', 'Accuracy (CV)': '93.7'}, 'model_links': [], 'model_name': '3SCNN', 'paper_date': '2019-06-16', 'paper_title': 'Three-Stream Convolutional Neural Network With Multi-Task and Ensemble Learning for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPRW_2019/html/PBVS/Liang_Three-Stream_Convolutional_Neural_Network_With_Multi-Task_and_Ensemble_Learning_for_CVPRW_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '88.0', 'Accuracy (CV)': '93.6'}, 'model_links': [], 'model_name': 'PGCN-TCA', 'paper_date': '2020-01-06', 'paper_title': 'PGCN-TCA: Pseudo Graph Convolutional Network With Temporal and Channel-Wise Attention for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1109/ACCESS.2020.2964115', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/RA-GCNv1', 'url': 'https://github.com/yfsong0709/RA-GCNv1'}, {'title': 'yfsong0709/RA-GCNv2', 'url': 'https://github.com/yfsong0709/RA-GCNv2'}, {'title': 'peter-yys-yoon/pegcnv2', 'url': 'https://github.com/peter-yys-yoon/pegcnv2'}], 'metrics': {'Accuracy (CS)': '87.3', 'Accuracy (CV)': '93.6'}, 'model_links': [], 'model_name': '3s RA-GCN', 'paper_date': '2020-08-09', 'paper_title': 'Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2008.03791v2', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/RA-GCNv1', 'url': 'https://github.com/yfsong0709/RA-GCNv1'}, {'title': 'yfsong0709/RA-GCNv2', 'url': 'https://github.com/yfsong0709/RA-GCNv2'}, {'title': 'peter-yys-yoon/pegcnv2', 'url': 'https://github.com/peter-yys-yoon/pegcnv2'}], 'metrics': {'Accuracy (CS)': '85.9', 'Accuracy (CV)': '93.5'}, 'model_links': [], 'model_name': '3s RA-GCN', 'paper_date': '2019-05-16', 'paper_title': 'Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons', 'paper_url': 'https://arxiv.org/abs/1905.06774v2', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/SGN', 'url': 'https://github.com/microsoft/SGN'}], 'metrics': {'Accuracy (CS)': '86.6', 'Accuracy (CV)': '93.4'}, 'model_links': [], 'model_name': 'SGN', 'paper_date': '2019-04-02', 'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1904.01189v3', 'uses_additional_data': False}, {'code_links': [{'title': 'andreYoo/PeGCNs', 'url': 'https://github.com/andreYoo/PeGCNs'}], 'metrics': {'Accuracy (CS)': '85.6', 'Accuracy (CV)': '93.4'}, 'model_links': [], 'model_name': 'PeGCN', 'paper_date': '2020-03-17', 'paper_title': 'Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.07514v1', 'uses_additional_data': False}, {'code_links': [{'title': 'memory-attention-networks/MANs', 'url': 'https://github.com/memory-attention-networks/MANs'}], 'metrics': {'Accuracy (CS)': '82.67', 'Accuracy (CV)': '93.22'}, 'model_links': [], 'model_name': 'MANs (DenseNet-161)', 'paper_date': '2018-04-23', 'paper_title': 'Memory Attention Networks for Skeleton-based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1804.08254v2', 'uses_additional_data': False}, {'code_links': [{'title': 'kalpitthakkar/pb-gcn', 'url': 'https://github.com/kalpitthakkar/pb-gcn'}], 'metrics': {'Accuracy (CS)': '87.5', 'Accuracy (CV)': '93.2'}, 'model_links': [], 'model_name': 'PB-GCN', 'paper_date': '2018-09-13', 'paper_title': 'Part-based Graph Convolutional Network for Action Recognition', 'paper_url': 'http://arxiv.org/abs/1809.04983v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '85.1', 'Accuracy (CV)': '93.2'}, 'model_links': [], 'model_name': 'AR-GCN', 'paper_date': '2019-11-27', 'paper_title': 'An Attention-Enhanced Recurrent Graph Convolutional Network for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1145/3372806.3372814', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/RA-GCNv1', 'url': 'https://github.com/yfsong0709/RA-GCNv1'}, {'title': 'yfsong0709/RA-GCNv2', 'url': 'https://github.com/yfsong0709/RA-GCNv2'}, {'title': 'peter-yys-yoon/pegcnv2', 'url': 'https://github.com/peter-yys-yoon/pegcnv2'}], 'metrics': {'Accuracy (CS)': '85.8', 'Accuracy (CV)': '93.0'}, 'model_links': [], 'model_name': '2s RA-GCN', 'paper_date': '2019-05-16', 'paper_title': 'Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons', 'paper_url': 'https://arxiv.org/abs/1905.06774v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '85.3', 'Accuracy (CV)': '92.8'}, 'model_links': [], 'model_name': 'GVFE+ AS-GCN with DH-TCN', 'paper_date': '2019-12-20', 'paper_title': 'Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition', 'paper_url': 'https://arxiv.org/abs/1912.09745v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '87.2', 'Accuracy (CV)': '92.7'}, 'model_links': [], 'model_name': 'TS-SAN', 'paper_date': '2019-12-18', 'paper_title': 'Self-Attention Network for Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1912.08435v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '84.8', 'Accuracy (CV)': '92.4'}, 'model_links': [], 'model_name': 'SR-TSL', 'paper_date': '2018-05-07', 'paper_title': 'Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning', 'paper_url': 'http://arxiv.org/abs/1805.02335v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '85.0', 'Accuracy (CV)': '92.3'}, 'model_links': [], 'model_name': '3scale ResNet152', 'paper_date': '2017-04-19', 'paper_title': 'Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn', 'paper_url': 'http://arxiv.org/abs/1704.05645v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '85.2', 'Accuracy (CV)': '91.7'}, 'model_links': [], 'model_name': 'PR-GCN', 'paper_date': '2020-10-14', 'paper_title': 'Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2010.07367v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '86.8', 'Accuracy (CV)': '91.6'}, 'model_links': [], 'model_name': 'RF-Action', 'paper_date': '2019-09-20', 'paper_title': 'Making the Invisible Visible: Action Recognition Through Walls and Occlusions', 'paper_url': 'https://arxiv.org/abs/1909.09300v1', 'uses_additional_data': False}, {'code_links': [{'title': 'huguyuehuhu/HCN-pytorch', 'url': 'https://github.com/huguyuehuhu/HCN-pytorch'}, {'title': 'fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition', 'url': 'https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition'}, {'title': 'maxstrobel/HCN-PrototypeLoss-PyTorch', 'url': 'https://github.com/maxstrobel/HCN-PrototypeLoss-PyTorch'}, {'title': 'hhe-distance/AIF-CNN', 'url': 'https://github.com/hhe-distance/AIF-CNN'}, {'title': 'natepuppy/HCN-pytorch', 'url': 'https://github.com/natepuppy/HCN-pytorch'}], 'metrics': {'Accuracy (CS)': '86.5', 'Accuracy (CV)': '91.1'}, 'model_links': [], 'model_name': 'HCN', 'paper_date': '2018-04-17', 'paper_title': 'Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation', 'paper_url': 'http://arxiv.org/abs/1804.06055v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '82.83', 'Accuracy (CV)': '90.05'}, 'model_links': [], 'model_name': 'FO-GASTM', 'paper_date': '2019-07-08', 'paper_title': 'Learning Shape-Motion Representations from Geometric Algebra Spatio-Temporal Model for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1109/ICME.2019.00187', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '83.5', 'Accuracy (CV)': '89.8'}, 'model_links': [], 'model_name': 'DPRL', 'paper_date': '2018-06-01', 'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'magnux/DMNN', 'url': 'https://github.com/magnux/DMNN'}], 'metrics': {'Accuracy (CS)': '82.0', 'Accuracy (CV)': '89.5'}, 'model_links': [], 'model_name': 'DM-3DCNN', 'paper_date': '2017-10-23', 'paper_title': '3D CNNs on Distance Matrices for Human Action Recognition', 'paper_url': 'https://doi.org/10.1145/3123266.3123299', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '83.2', 'Accuracy (CV)': '89.3'}, 'model_links': [], 'model_name': 'CNN+Motion+Trans', 'paper_date': '2017-04-25', 'paper_title': 'Skeleton-based Action Recognition with Convolutional Neural Networks', 'paper_url': 'http://arxiv.org/abs/1704.07595v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '84.23', 'Accuracy (CV)': '89.27'}, 'model_links': [], 'model_name': 'RGB+Skeleton (cross-attention)', 'paper_date': '2020-01-20', 'paper_title': 'Context-Aware Cross-Attention for Skeleton-Based Human Action Recognition', 'paper_url': 'https://doi.org/10.1109/ACCESS.2020.2968054', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '81.8', 'Accuracy (CV)': '89.0'}, 'model_links': [], 'model_name': 'Bayesian GC-LSTM', 'paper_date': '2019-10-01', 'paper_title': 'Bayesian Graph Convolution LSTM for Skeleton Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Bayesian_Graph_Convolution_LSTM_for_Skeleton_Based_Action_Recognition_ICCV_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '83.36', 'Accuracy (CV)': '88.84'}, 'model_links': [], 'model_name': 'ST-GCN-jpd', 'paper_date': '2019-06-24', 'paper_title': 'A Comparative Review of Recent Kinect-based Action Recognition Algorithms', 'paper_url': 'https://arxiv.org/abs/1906.09955v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '80.7', 'Accuracy (CV)': '88.8'}, 'model_links': [], 'model_name': 'ARRN-LSTM', 'paper_date': '2018-05-07', 'paper_title': 'Relational Network for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1805.02556v4', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '80.7', 'Accuracy (CV)': '88.4'}, 'model_links': [], 'model_name': 'EleAtt-GRU (aug.)', 'paper_date': '2019-09-03', 'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks', 'paper_url': 'https://arxiv.org/abs/1909.01939v1', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmskeleton', 'url': 'https://github.com/open-mmlab/mmskeleton'}, {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'}, {'title': 'ericksiavichay/cs230-final-project', 'url': 'https://github.com/ericksiavichay/cs230-final-project'}, {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'}, {'title': 'stillarrow/S2VT_ACT', 'url': 'https://github.com/stillarrow/S2VT_ACT'}, {'title': 'ZhangNYG/ST-GCN', 'url': 'https://github.com/ZhangNYG/ST-GCN'}, {'title': 'DixinFan/st-gcn', 'url': 'https://github.com/DixinFan/st-gcn'}, {'title': 'ken724049/action-recognition', 'url': 'https://github.com/ken724049/action-recognition'}, {'title': 'antoniolq/st-gcn', 'url': 'https://github.com/antoniolq/st-gcn'}, {'title': 'github-zbx/ST-GCN', 'url': 'https://github.com/github-zbx/ST-GCN'}, {'title': 'GeyuanZhang/st-gcn-master', 'url': 'https://github.com/GeyuanZhang/st-gcn-master'}, {'title': 'KrisLee512/ST-GCN', 'url': 'https://github.com/KrisLee512/ST-GCN'}, {'title': 'AbiterVX/ST-GCN', 'url': 'https://github.com/AbiterVX/ST-GCN'}], 'metrics': {'Accuracy (CS)': '81.5', 'Accuracy (CV)': '88.3'}, 'model_links': [], 'model_name': 'ST-GCN', 'paper_date': '2018-01-23', 'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1801.07455v2', 'uses_additional_data': False}, {'code_links': [{'title': 'TobiasLee/Text-Classification', 'url': 'https://github.com/TobiasLee/Text-Classification'}, {'title': 'batzner/indrnn', 'url': 'https://github.com/batzner/indrnn'}, {'title': 'lmnt-com/haste', 'url': 'https://github.com/lmnt-com/haste'}, {'title': 'StefOe/indrnn-pytorch', 'url': 'https://github.com/StefOe/indrnn-pytorch'}, {'title': 'Sunnydreamrain/IndRNN_pytorch', 'url': 'https://github.com/Sunnydreamrain/IndRNN_pytorch'}, {'title': 'Sunnydreamrain/IndRNN_Theano_Lasagne', 'url': 'https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne'}, {'title': 'trevor-richardson/rnn_zoo', 'url': 'https://github.com/trevor-richardson/rnn_zoo'}, {'title': 'amcs1729/Predicting-cloud-CPU-usage-on-Azure-data', 'url': 'https://github.com/amcs1729/Predicting-cloud-CPU-usage-on-Azure-data'}, {'title': 'Sunnydreamrain/IndRNN', 'url': 'https://github.com/Sunnydreamrain/IndRNN'}, {'title': 'secretlyvogon/IndRNNTF', 'url': 'https://github.com/secretlyvogon/IndRNNTF'}, {'title': 'secretlyvogon/Neural-Network-Implementations', 'url': 'https://github.com/secretlyvogon/Neural-Network-Implementations'}], 'metrics': {'Accuracy (CS)': '81.8', 'Accuracy (CV)': '88.0'}, 'model_links': [], 'model_name': 'Ind-RNN', 'paper_date': '2018-03-13', 'paper_title': 'Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN', 'paper_url': 'http://arxiv.org/abs/1803.04831v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '79.4', 'Accuracy (CV)': '87.6'}, 'model_links': [], 'model_name': 'VA-LSTM', 'paper_date': '2017-03-24', 'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data', 'paper_url': 'http://arxiv.org/abs/1703.08274v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '80', 'Accuracy (CV)': '87.2'}, 'model_links': [], 'model_name': 'Synthesized CNN', 'paper_date': '2017-08-01', 'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition', 'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '79.8', 'Accuracy (CV)': '87.1'}, 'model_links': [], 'model_name': 'EleAtt-GRU', 'paper_date': '2018-07-12', 'paper_title': 'Adding Attentiveness to the Neurons in Recurrent Neural Networks', 'paper_url': 'http://arxiv.org/abs/1807.04445v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '80.9', 'Accuracy (CV)': '86.1'}, 'model_links': [], 'model_name': 'HPM_RGB+HPM_3D+Traj', 'paper_date': '2017-07-04', 'paper_title': 'Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition', 'paper_url': 'http://arxiv.org/abs/1707.00823v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '79.6', 'Accuracy (CV)': '84.8'}, 'model_links': [], 'model_name': 'Clips+CNN+MTLN', 'paper_date': '2017-03-09', 'paper_title': 'A New Representation of Skeleton Sequences for 3D Action Recognition', 'paper_url': 'http://arxiv.org/abs/1703.03492v3', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (CS)': '76.5', 'Accuracy (CV)': '84.7'}, 'model_links': [], 'model_name': 'Skelemotion + Yang et al.', 'paper_date': '2019-07-30', 'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition', 'paper_url': 'https://arxiv.org/abs/1907.13025v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '79.6', 'Accuracy (CV)': '84.6'}, 'model_links': [], 'model_name': 'F2CSkeleton', 'paper_date': '2018-05-30', 'paper_title': 'A Fine-to-Coarse Convolutional Neural Network for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1805.11790v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '76.10', 'Accuracy (CV)': '84.00'}, 'model_links': [], 'model_name': 'GCA-LSTM', 'paper_date': '2017-07-01', 'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '74.6', 'Accuracy (CV)': '83.2'}, 'model_links': [], 'model_name': 'URNN-2L-T', 'paper_date': '2017-10-22', 'paper_title': 'Adaptive RNN Tree for Large-Scale Human Action Recognition', 'paper_url': 'https://ieeexplore.ieee.org/document/8237423', 'uses_additional_data': False}, {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition', 'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}], 'metrics': {'Accuracy (CS)': '74.3', 'Accuracy (CV)': '83.1'}, 'model_links': [], 'model_name': 'TCN', 'paper_date': '2017-04-14', 'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks', 'paper_url': 'http://arxiv.org/abs/1704.04516v1', 'uses_additional_data': False}, {'code_links': [{'title': 'dzwallkilled/IEforAR', 'url': 'https://github.com/dzwallkilled/IEforAR'}], 'metrics': {'Accuracy (CV)': '82.31'}, 'model_links': [], 'model_name': 'Five Spatial Skeleton Features', 'paper_date': '2017-05-02', 'paper_title': 'Investigation of Different Skeleton Features for CNN-based 3D Action Recognition', 'paper_url': 'http://arxiv.org/abs/1705.00835v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '74.60', 'Accuracy (CV)': '81.25'}, 'model_links': [], 'model_name': 'Ensemble TS-LSTM v2', 'paper_date': '2017-10-01', 'paper_title': 'Ensemble Deep Learning for Skeleton-Based Action Recognition Using Temporal Sliding LSTM Networks', 'paper_url': 'http://openaccess.thecvf.com/content_iccv_2017/html/Lee_Ensemble_Deep_Learning_ICCV_2017_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '75.9', 'Accuracy (CV)': '81.2'}, 'model_links': [], 'model_name': 'SkeletonNet', 'paper_date': '2017-03-31', 'paper_title': 'Skeletonnet: Mining deep part features for 3-d action recognition', 'paper_url': 'https://doi.org/10.1109/LSP.2017.2690339', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '73.4', 'Accuracy (CV)': '81.2'}, 'model_links': [], 'model_name': 'STA-LSTM', 'paper_date': '2016-11-18', 'paper_title': 'An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data', 'paper_url': 'http://arxiv.org/abs/1611.06067v1', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (CS)': '73.3', 'Accuracy (CV)': '80.3'}, 'model_links': [], 'model_name': 'TSRJI (Late Fusion)', 'paper_date': '2019-09-11', 'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints', 'paper_url': 'https://arxiv.org/abs/1909.05704v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '71.3', 'Accuracy (CV)': '79.5'}, 'model_links': [], 'model_name': 'Two-Stream RNN', 'paper_date': '2017-04-09', 'paper_title': 'Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks', 'paper_url': 'http://arxiv.org/abs/1704.02581v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '69.2', 'Accuracy (CV)': '77.7'}, 'model_links': [], 'model_name': 'Trust Gate ST-LSTM', 'paper_date': '2016-07-24', 'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1607.07043v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '61.70', 'Accuracy (CV)': '75.50'}, 'model_links': [], 'model_name': 'ST-LSTM', 'paper_date': '2016-07-24', 'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1607.07043v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '66.8', 'Accuracy (CV)': '72.6'}, 'model_links': [], 'model_name': 'Two-Stream 3DCNN', 'paper_date': '2017-05-23', 'paper_title': 'Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1705.08106v2', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (CS)': '62.93', 'Accuracy (CV)': '70.27'}, 'model_links': [], 'model_name': 'Part-aware LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (CS)': '60.7', 'Accuracy (CV)': '67.3'}, 'model_links': [], 'model_name': 'Deep LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '60.2', 'Accuracy (CV)': '65.2'}, 'model_links': [], 'model_name': 'Dynamic Skeletons', 'paper_date': '2016-12-15', 'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition', 'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '59.1', 'Accuracy (CV)': '64.0'}, 'model_links': [], 'model_name': 'H-RNN', 'paper_date': '2015-06-07', 'paper_title': 'Hierarchical recurrent neural network for skeleton based action recognition', 'paper_url': 'https://doi.org/10.1109/CVPR.2015.7298714', 'uses_additional_data': False}, {'code_links': [{'title': 'fdsa1860/skeletal', 'url': 'https://github.com/fdsa1860/skeletal'}], 'metrics': {'Accuracy (CS)': '50.1', 'Accuracy (CV)': '52.8'}, 'model_links': [], 'model_name': 'Lie Group', 'paper_date': '2014-06-23', 'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group', 'paper_url': 'https://doi.org/10.1109/CVPR.2014.82', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '38.6', 'Accuracy (CV)': '41.4'}, 'model_links': [], 'model_name': 'Skeleton Quads', 'paper_date': '2014-08-24', 'paper_title': 'Skeletal quads: Human action recognition using joint quadruples', 'paper_url': 'https://doi.org/10.1109/ICPR.2014.772', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'JHMDB (2D poses only)', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-jhmdb-2d'}], 'description': '', 'sota': {'metrics': ['Average accuracy of 3 splits', 'Accuracy', 'No. parameters'], 'rows': [{'code_links': [{'title': 'fandulu/DD-Net', 'url': 'https://github.com/fandulu/DD-Net'}, {'title': 'BlurryLight/DD-Net-Pytorch', 'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}], 'metrics': {'Accuracy': '78.0 (average of 3 split train/test)', 'Average accuracy of 3 splits': '77.2', 'No. parameters': '1.82 M'}, 'model_links': [], 'model_name': 'DD-Net', 'paper_date': '2019-07-23', 'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better', 'paper_url': 'https://arxiv.org/abs/1907.09658v8', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Average accuracy of 3 splits': '67.9', 'No. parameters': '-'}, 'model_links': [], 'model_name': 'PoTion', 'paper_date': '2018-06-01', 'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'noboevbo/ehpi_action_recognition', 'url': 'https://github.com/noboevbo/ehpi_action_recognition'}], 'metrics': {'Average accuracy of 3 splits': '65.5'}, 'model_links': [], 'model_name': 'EHPI', 'paper_date': '2019-04-19', 'paper_title': 'Simple yet efficient real-time pose-based action recognition', 'paper_url': 'http://arxiv.org/abs/1904.09140v1', 'uses_additional_data': False}, {'code_links': [{'title': 'mzolfaghari/chained-multistream-networks', 'url': 'https://github.com/mzolfaghari/chained-multistream-networks'}], 'metrics': {'Average accuracy of 3 splits': '56.8'}, 'model_links': [], 'model_name': 'Chained', 'paper_date': '2017-04-03', 'paper_title': 'Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection', 'paper_url': 'http://arxiv.org/abs/1704.00616v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'PKU-MMD', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-pku-mmd'}], 'description': '', 'sota': {'metrics': ['mAP@0.50 (CV)', 'mAP@0.50 (CS)'], 'rows': [{'code_links': [], 'metrics': {'mAP@0.50 (CS)': '92.9', 'mAP@0.50 (CV)': '94.4'}, 'model_links': [], 'model_name': 'RF-Action', 'paper_date': '2019-09-20', 'paper_title': 'Making the Invisible Visible: Action Recognition Through Walls and Occlusions', 'paper_url': 'https://arxiv.org/abs/1909.09300v1', 'uses_additional_data': False}, {'code_links': [{'title': 'huguyuehuhu/HCN-pytorch', 'url': 'https://github.com/huguyuehuhu/HCN-pytorch'}, {'title': 'fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition', 'url': 'https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition'}, {'title': 'maxstrobel/HCN-PrototypeLoss-PyTorch', 'url': 'https://github.com/maxstrobel/HCN-PrototypeLoss-PyTorch'}, {'title': 'hhe-distance/AIF-CNN', 'url': 'https://github.com/hhe-distance/AIF-CNN'}, {'title': 'natepuppy/HCN-pytorch', 'url': 'https://github.com/natepuppy/HCN-pytorch'}], 'metrics': {'mAP@0.50 (CS)': '92.6', 'mAP@0.50 (CV)': '94.2'}, 'model_links': [], 'model_name': 'HCN', 'paper_date': '2018-04-17', 'paper_title': 'Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation', 'paper_url': 'http://arxiv.org/abs/1804.06055v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'mAP@0.50 (CS)': '90.4', 'mAP@0.50 (CV)': '93.7'}, 'model_links': [], 'model_name': 'Li et al. [[Li et al.2017b]]', 'paper_date': '2017-04-25', 'paper_title': 'Skeleton-based Action Recognition with Convolutional Neural Networks', 'paper_url': 'http://arxiv.org/abs/1704.07595v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Skeletics-152', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on'}], 'description': '', 'sota': {'metrics': ['Accuracy (Cross-Subject)'], 'rows': [{'code_links': [{'title': 'skelemoa/quovadis', 'url': 'https://github.com/skelemoa/quovadis'}], 'metrics': {'Accuracy (Cross-Subject)': '57.01 %'}, 'model_links': [], 'model_name': '4s-ShiftGCN', 'paper_date': '2020-07-04', 'paper_title': 'Quo Vadis, Skeleton Action Recognition ?', 'paper_url': 'https://arxiv.org/abs/2007.02072v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Skeleton-Mimetics', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-skeleton'}], 'description': '', 'sota': {'metrics': ['Accuracy (%)'], 'rows': [{'code_links': [{'title': 'skelemoa/quovadis', 'url': 'https://github.com/skelemoa/quovadis'}], 'metrics': {'Accuracy (%)': '57.37 %'}, 'model_links': [], 'model_name': 'MS-G3D', 'paper_date': '2020-07-04', 'paper_title': 'Quo Vadis, Skeleton Action Recognition ?', 'paper_url': 'https://arxiv.org/abs/2007.02072v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'MSR Action3D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msr'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '88.51%'}, 'model_links': [], 'model_name': 'Temporal K-Means Clustering + Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}, {'code_links': [{'title': 'rort1989/HDM', 'url': 'https://github.com/rort1989/HDM'}], 'metrics': {'Accuracy': '86.1'}, 'model_links': [], 'model_name': 'HDM-BG', 'paper_date': '2019-06-01', 'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '74'}, 'model_links': [], 'model_name': 'GFT', 'paper_date': '2019-08-26', 'paper_title': 'Graph Based Skeleton Modeling for Human Activity Analysis', 'paper_url': 'https://doi.org/10.1109/ICIP.2019.8803186', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'MSR ActionPairs', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msr-1'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '98.02%'}, 'model_links': [], 'model_name': 'Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'CAD-120', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-cad-120'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '91.1%'}, 'model_links': [], 'model_name': 'NGM (5-shot)', 'paper_date': '2018-09-01', 'paper_title': 'Neural Graph Matching Networks for Fewshot 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '89.3%'}, 'model_links': [], 'model_name': 'All Features (w ground truth)', 'paper_date': '2013-02-01', 'paper_title': 'Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation', 'paper_url': 'http://proceedings.mlr.press/v28/koppula13.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '86.0%'}, 'model_links': [], 'model_name': 'KGS', 'paper_date': '2012-10-04', 'paper_title': 'Learning Human Activities and Object Affordances from RGB-D Videos', 'paper_url': 'http://arxiv.org/abs/1210.1207v2', 'uses_additional_data': False}, {'code_links': [{'title': 'asheshjain399/RNNexp', 'url': 'https://github.com/asheshjain399/RNNexp'}, {'title': 'zhaolongkzz/human_motion', 'url': 'https://github.com/zhaolongkzz/human_motion'}], 'metrics': {'Accuracy': '85.4%'}, 'model_links': [], 'model_name': 'S-RNN (5-shot)', 'paper_date': '2015-11-17', 'paper_title': 'Structural-RNN: Deep Learning on Spatio-Temporal Graphs', 'paper_url': 'http://arxiv.org/abs/1511.05298v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '85.0%'}, 'model_links': [], 'model_name': 'NGM w/o Edges  (5-shot)', 'paper_date': '2018-09-01', 'paper_title': 'Neural Graph Matching Networks for Fewshot 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '70.3%'}, 'model_links': [], 'model_name': 'Our DP seg. + moves + heuristic seg.', 'paper_date': '2013-02-01', 'paper_title': 'Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation', 'paper_url': 'http://proceedings.mlr.press/v28/koppula13.html', 'uses_additional_data': False}, {'code_links': [{'title': 'charlesq34/pointnet', 'url': 'https://github.com/charlesq34/pointnet'}, {'title': 'fxia22/pointnet.pytorch', 'url': 'https://github.com/fxia22/pointnet.pytorch'}, {'title': 'vinits5/learning3d', 'url': 'https://github.com/vinits5/learning3d'}, {'title': 'maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification', 'url': 'https://github.com/maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification'}, {'title': 'DylanWusee/pointconv_pytorch', 'url': 'https://github.com/DylanWusee/pointconv_pytorch'}, {'title': 'nikitakaraevv/pointnet', 'url': 'https://github.com/nikitakaraevv/pointnet'}, {'title': 'princeton-vl/SimpleView', 'url': 'https://github.com/princeton-vl/SimpleView'}, {'title': 'ZhihaoZhu/PointNet-Implementation-Tensorflow', 'url': 'https://github.com/ZhihaoZhu/PointNet-Implementation-Tensorflow'}, {'title': 'sarthakTUM/roofn3d', 'url': 'https://github.com/sarthakTUM/roofn3d'}, {'title': 'ajhamdi/AdvPC', 'url': 'https://github.com/ajhamdi/AdvPC'}, {'title': 'romaintha/pytorch_pointnet', 'url': 'https://github.com/romaintha/pytorch_pointnet'}, {'title': 'AI-Guru/pointcloud_experiments', 'url': 'https://github.com/AI-Guru/pointcloud_experiments'}, {'title': 'YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti', 'url': 'https://github.com/YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti'}, {'title': 'YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024', 'url': 'https://github.com/YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024'}, {'title': 'saaries/PointNet', 'url': 'https://github.com/saaries/PointNet'}, {'title': 'y2kmz/pointnetv2', 'url': 'https://github.com/y2kmz/pointnetv2'}, {'title': 'Fragjacker/Pointcloud-grad-CAM', 'url': 'https://github.com/Fragjacker/Pointcloud-grad-CAM'}, {'title': 'abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds', 'url': 'https://github.com/abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds'}, {'title': 'donshen/pointnet.phasedetection', 'url': 'https://github.com/donshen/pointnet.phasedetection'}, {'title': 'Yuto0107/pointnet', 'url': 'https://github.com/Yuto0107/pointnet'}, {'title': 'opeco17/pointnet', 'url': 'https://github.com/opeco17/pointnet'}, {'title': 'YanWei123/Pointnet_encoder_Foldingnet_decoder_quantization', 'url': 'https://github.com/YanWei123/Pointnet_encoder_Foldingnet_decoder_quantization'}, {'title': 'yanxp/PointNet', 'url': 'https://github.com/yanxp/PointNet'}, {'title': 'amyllykoski/CycleGAN', 'url': 'https://github.com/amyllykoski/CycleGAN'}, {'title': 'ftdlyc/pointnet_pytorch', 'url': 'https://github.com/ftdlyc/pointnet_pytorch'}, {'title': 'alpemek/ais3d', 'url': 'https://github.com/alpemek/ais3d'}, {'title': 'Dir-b/PointNet', 'url': 'https://github.com/Dir-b/PointNet'}, {'title': 'PaParaZz1/PointNet', 'url': 'https://github.com/PaParaZz1/PointNet'}, {'title': 'witignite/Frustum-PointNet', 'url': 'https://github.com/witignite/Frustum-PointNet'}, {'title': 'sanantoniochili/PointCloud_KNN', 'url': 'https://github.com/sanantoniochili/PointCloud_KNN'}, {'title': 'zgx0534/pointnet_win', 'url': 'https://github.com/zgx0534/pointnet_win'}, {'title': 'KhusDM/PointNetTree', 'url': 'https://github.com/KhusDM/PointNetTree'}, {'title': 'timothylimyl/PointNet-Pytorch', 'url': 'https://github.com/timothylimyl/PointNet-Pytorch'}, {'title': 'minhncedutw/pointnet1_keras', 'url': 'https://github.com/minhncedutw/pointnet1_keras'}, {'title': 'kenakai16/pointconv_pytorch', 'url': 'https://github.com/kenakai16/pointconv_pytorch'}, {'title': 'xurui1217/pointnet.pytorch-master', 'url': 'https://github.com/xurui1217/pointnet.pytorch-master'}, {'title': 'GOD-GOD-Autonomous-Vehicle/self-pointnet', 'url': 'https://github.com/GOD-GOD-Autonomous-Vehicle/self-pointnet'}, {'title': 'Young98CN/pointconv_pytorch', 'url': 'https://github.com/Young98CN/pointconv_pytorch'}, {'title': 'm-117/PointNet-ein-Implementationsbeispiel-mit-Jupyter-Notebooks', 'url': 'https://github.com/m-117/PointNet-ein-Implementationsbeispiel-mit-Jupyter-Notebooks'}, {'title': 'hnVfly/pointnet.mxnet', 'url': 'https://github.com/hnVfly/pointnet.mxnet'}, {'title': 'LebronGG/PointNet', 'url': 'https://github.com/LebronGG/PointNet'}, {'title': 'ytng001/sensemaking', 'url': 'https://github.com/ytng001/sensemaking'}, {'title': 'lingzhang1/pointnet_tensorflow', 'url': 'https://github.com/lingzhang1/pointnet_tensorflow'}, {'title': 'coconutzs/PointNet_zs', 'url': 'https://github.com/coconutzs/PointNet_zs'}, {'title': 'Fnjn/UCSD-CSE-291I', 'url': 'https://github.com/Fnjn/UCSD-CSE-291I'}, {'title': 'aviros/roatationPointnet', 'url': 'https://github.com/aviros/roatationPointnet'}, {'title': 'ShadowShadowWong/update-pointnet-work', 'url': 'https://github.com/ShadowShadowWong/update-pointnet-work'}, {'title': 'Yang2446/pointnet', 'url': 'https://github.com/Yang2446/pointnet'}, {'title': 'dwtstore/sfm1', 'url': 'https://github.com/dwtstore/sfm1'}, {'title': 'ModelBunker/PointNet-TensorFlow', 'url': 'https://github.com/ModelBunker/PointNet-TensorFlow'}, {'title': 'Lw510107/pointnet-2018.6.27-', 'url': 'https://github.com/Lw510107/pointnet-2018.6.27-'}, {'title': 'freddieee/pn_6d_single', 'url': 'https://github.com/freddieee/pn_6d_single'}, {'title': 'mengxingshifen1218/pointnet.pytorch', 'url': 'https://github.com/mengxingshifen1218/pointnet.pytorch'}, {'title': 'LONG-9621/Extract_Point_3D', 'url': 'https://github.com/LONG-9621/Extract_Point_3D'}, {'title': 'lingzhang1/pointnet_pytorch', 'url': 'https://github.com/lingzhang1/pointnet_pytorch'}, {'title': 'THHHomas/mls', 'url': 'https://github.com/THHHomas/mls'}, {'title': 'ahmed-anas/thesis-pointnet', 'url': 'https://github.com/ahmed-anas/thesis-pointnet'}, {'title': 'liuch37/pointnet', 'url': 'https://github.com/liuch37/pointnet'}, {'title': 'CheesyB/cpointnet', 'url': 'https://github.com/CheesyB/cpointnet'}, {'title': 'aviros/pointnet_totations', 'url': 'https://github.com/aviros/pointnet_totations'}, {'title': 'Taeuk-Jang/pointcompletion', 'url': 'https://github.com/Taeuk-Jang/pointcompletion'}, {'title': 'bt77/pointnet', 'url': 'https://github.com/bt77/pointnet'}, {'title': 'yanx27/Pointnet', 'url': 'https://github.com/yanx27/Pointnet'}, {'title': 'monacv/pointnet', 'url': 'https://github.com/monacv/pointnet'}, {'title': 'merazlab/3D_Deep_Learning_Link', 'url': 'https://github.com/merazlab/3D_Deep_Learning_Link'}, {'title': 'ajertec/PointNetKeras', 'url': 'https://github.com/ajertec/PointNetKeras'}, {'title': 'AlfredoZermini/PointNet', 'url': 'https://github.com/AlfredoZermini/PointNet'}, {'title': 'YiruS/pointnet_adversarial', 'url': 'https://github.com/YiruS/pointnet_adversarial'}, {'title': 'wonderland-dsg/pointnet-grid', 'url': 'https://github.com/wonderland-dsg/pointnet-grid'}, {'title': 'KiranAkadas/My_Pointnet_v2', 'url': 'https://github.com/KiranAkadas/My_Pointnet_v2'}, {'title': 'Q-Qgao/pointnet', 'url': 'https://github.com/Q-Qgao/pointnet'}, {'title': 'LONG-9621/PointNet', 'url': 'https://github.com/LONG-9621/PointNet'}, {'title': 'wuryantoAji/POINTNET', 'url': 'https://github.com/wuryantoAji/POINTNET'}, {'title': 'WLK12580/12', 'url': 'https://github.com/WLK12580/12'}, {'title': 'SBPL-Cruz/perch_pose_sampler', 'url': 'https://github.com/SBPL-Cruz/perch_pose_sampler'}, {'title': 'Harut0726/my-pointnet-tensorflow', 'url': 'https://github.com/Harut0726/my-pointnet-tensorflow'}, {'title': 'BPMJG/annotated_pointnet', 'url': 'https://github.com/BPMJG/annotated_pointnet'}, {'title': 'SonuDileep/3-D-Object-Detection-using-PointNet', 'url': 'https://github.com/SonuDileep/3-D-Object-Detection-using-PointNet'}, {'title': 'prasadsawant5/PointNet', 'url': 'https://github.com/prasadsawant5/PointNet'}, {'title': 'zhijie-yang/pointnet.pytorch', 'url': 'https://github.com/zhijie-yang/pointnet.pytorch'}, {'title': 'KaidongLi/tf-3d-alpha', 'url': 'https://github.com/KaidongLi/tf-3d-alpha'}, {'title': 'hz-ants/pointnet.pytorch', 'url': 'https://github.com/hz-ants/pointnet.pytorch'}, {'title': 'AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation', 'url': 'https://github.com/AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation'}, {'title': 'VaanHUANG/CSCI5210HW1', 'url': 'https://github.com/VaanHUANG/CSCI5210HW1'}], 'metrics': {'Accuracy': '69.1%'}, 'model_links': [], 'model_name': 'PointNet (5-shot)', 'paper_date': '2016-12-02', 'paper_title': 'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation', 'paper_url': 'http://arxiv.org/abs/1612.00593v2', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy': '68.1%'}, 'model_links': [], 'model_name': 'P-LSTM (5-shot)', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'First-Person Hand Action Benchmark', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-first'}], 'description': '', 'sota': {'metrics': ['1:3 Accuracy', '1:1 Accuracy', '3:1 Accuracy', 'Cross-person Accuracy'], 'rows': [{'code_links': [{'title': 'AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition', 'url': 'https://github.com/AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition'}], 'metrics': {'1:1 Accuracy': '95.93', '1:3 Accuracy': '92.9', '3:1 Accuracy': '96.76', 'Cross-person Accuracy': '88.70'}, 'model_links': [], 'model_name': 'TCN-Summ', 'paper_date': '2021-03-03', 'paper_title': 'Domain and View-point Agnostic Hand Action Recognition', 'paper_url': 'https://arxiv.org/abs/2103.02303v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UAV-Human', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-uav'}], 'description': '', 'sota': {'metrics': ['Average Accuracy'], 'rows': [{'code_links': [{'title': 'kchengiva/Shift-GCN', 'url': 'https://github.com/kchengiva/Shift-GCN'}], 'metrics': {'Average Accuracy': '37.98'}, 'model_links': [], 'model_name': 'Shift-GCN', 'paper_date': '2020-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Average Accuracy': '36.97'}, 'model_links': [], 'model_name': 'HARD-Net', 'paper_date': None, 'paper_title': 'HARD-Net: Hardness-AwaRe Discrimination Network for 3D Early Activity Prediction', 'paper_url': 'https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1360_ECCV_2020_paper.php', 'uses_additional_data': False}, {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal', 'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'}, {'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Average Accuracy': '34.84'}, 'model_links': [], 'model_name': '2S-AGCN', 'paper_date': '2018-05-20', 'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1805.07694v3', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmskeleton', 'url': 'https://github.com/open-mmlab/mmskeleton'}, {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'}, {'title': 'ericksiavichay/cs230-final-project', 'url': 'https://github.com/ericksiavichay/cs230-final-project'}, {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'}, {'title': 'stillarrow/S2VT_ACT', 'url': 'https://github.com/stillarrow/S2VT_ACT'}, {'title': 'ZhangNYG/ST-GCN', 'url': 'https://github.com/ZhangNYG/ST-GCN'}, {'title': 'DixinFan/st-gcn', 'url': 'https://github.com/DixinFan/st-gcn'}, {'title': 'ken724049/action-recognition', 'url': 'https://github.com/ken724049/action-recognition'}, {'title': 'antoniolq/st-gcn', 'url': 'https://github.com/antoniolq/st-gcn'}, {'title': 'github-zbx/ST-GCN', 'url': 'https://github.com/github-zbx/ST-GCN'}, {'title': 'GeyuanZhang/st-gcn-master', 'url': 'https://github.com/GeyuanZhang/st-gcn-master'}, {'title': 'KrisLee512/ST-GCN', 'url': 'https://github.com/KrisLee512/ST-GCN'}, {'title': 'AbiterVX/ST-GCN', 'url': 'https://github.com/AbiterVX/ST-GCN'}], 'metrics': {'Average Accuracy': '30.25'}, 'model_links': [], 'model_name': 'ST-GCN', 'paper_date': '2018-01-23', 'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1801.07455v2', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch', 'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}], 'metrics': {'Average Accuracy': '29.90'}, 'model_links': [], 'model_name': 'DGNN', 'paper_date': '2019-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'J-HMDB', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-j-hmdb'}], 'description': '', 'sota': {'metrics': ['Accuracy (RGB+pose)', 'Accuracy (pose)'], 'rows': [{'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '90.4', 'Accuracy (pose)': '67.9'}, 'model_links': [], 'model_name': 'Potion', 'paper_date': '2018-06-01', 'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '86.1'}, 'model_links': [], 'model_name': 'PA3D+RPAN', 'paper_date': '2019-06-01', 'paper_title': 'PA3D: Pose-Action 3D Machine for Video Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '85.5'}, 'model_links': [], 'model_name': 'I3D + Potion', 'paper_date': '2018-06-01', 'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'deepmind/kinetics-i3d', 'url': 'https://github.com/deepmind/kinetics-i3d'}, {'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}, {'title': 'piergiaj/pytorch-i3d', 'url': 'https://github.com/piergiaj/pytorch-i3d'}, {'title': 'hassony2/kinetics_i3d_pytorch', 'url': 'https://github.com/hassony2/kinetics_i3d_pytorch'}, {'title': 'yaohungt/GSTEG_CVPR_2019', 'url': 'https://github.com/yaohungt/GSTEG_CVPR_2019'}, {'title': 'dlpbc/keras-kinetics-i3d', 'url': 'https://github.com/dlpbc/keras-kinetics-i3d'}, {'title': 'StanfordVL/RubiksNet', 'url': 'https://github.com/StanfordVL/RubiksNet'}, {'title': 'FrederikSchorr/sign-language', 'url': 'https://github.com/FrederikSchorr/sign-language'}, {'title': 'CMU-CREATE-Lab/deep-smoke-machine', 'url': 'https://github.com/CMU-CREATE-Lab/deep-smoke-machine'}, {'title': 'JeffCHEN2017/WSSTG', 'url': 'https://github.com/JeffCHEN2017/WSSTG'}, {'title': 'OanaIgnat/i3d_keras', 'url': 'https://github.com/OanaIgnat/i3d_keras'}, {'title': 'ahsaniqbal/Kinetics-FeatureExtractor', 'url': 'https://github.com/ahsaniqbal/Kinetics-FeatureExtractor'}, {'title': 'prinshul/GWSDR', 'url': 'https://github.com/prinshul/GWSDR'}, {'title': 'PPPrior/i3d-pytorch', 'url': 'https://github.com/PPPrior/i3d-pytorch'}, {'title': 'sebastiantiesmeyer/deeplabchop3d', 'url': 'https://github.com/sebastiantiesmeyer/deeplabchop3d'}, {'title': 'anonymous-p/Flickering_Adversarial_Video', 'url': 'https://github.com/anonymous-p/Flickering_Adversarial_Video'}, {'title': 'vijayvee/behavior-recognition', 'url': 'https://github.com/vijayvee/behavior-recognition'}, {'title': 'LukasHedegaard/co3d', 'url': 'https://github.com/LukasHedegaard/co3d'}, {'title': 'vijayvee/behavior_recognition', 'url': 'https://github.com/vijayvee/behavior_recognition'}, {'title': 'AbdurrahmanNadi/activity_recognition_web_service', 'url': 'https://github.com/AbdurrahmanNadi/activity_recognition_web_service'}, {'title': 'hjchoi-minds/i3dnia', 'url': 'https://github.com/hjchoi-minds/i3dnia'}, {'title': 'Alexyuda/action_recognition', 'url': 'https://github.com/Alexyuda/action_recognition'}, {'title': 'helloxy96/CS5242_Project2020', 'url': 'https://github.com/helloxy96/CS5242_Project2020'}, {'title': 'ShobhitMaheshwari/sign-language1', 'url': 'https://github.com/ShobhitMaheshwari/sign-language1'}, {'title': 'mHealthBuet/SegCodeNet', 'url': 'https://github.com/mHealthBuet/SegCodeNet'}], 'metrics': {'Accuracy (RGB+pose)': '84.1'}, 'model_links': [], 'model_name': 'I3D', 'paper_date': '2017-05-22', 'paper_title': 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset', 'paper_url': 'http://arxiv.org/abs/1705.07750v3', 'uses_additional_data': False}, {'code_links': [{'title': 'agethen/RPAN', 'url': 'https://github.com/agethen/RPAN'}], 'metrics': {'Accuracy (RGB+pose)': '83.9'}, 'model_links': [], 'model_name': 'RPAN', 'paper_date': '2017-10-22', 'paper_title': 'RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos', 'paper_url': 'https://doi.org/10.1109/ICCV.2017.402', 'uses_additional_data': False}, {'code_links': [{'title': 'mzolfaghari/chained-multistream-networks', 'url': 'https://github.com/mzolfaghari/chained-multistream-networks'}], 'metrics': {'Accuracy (RGB+pose)': '76.1', 'Accuracy (pose)': '56.8'}, 'model_links': [], 'model_name': 'Chained (RGB+Flow +Pose)', 'paper_date': '2017-04-03', 'paper_title': 'Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection', 'paper_url': 'http://arxiv.org/abs/1704.00616v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '71.1'}, 'model_links': [], 'model_name': 'MR Two-Sream R-CNN', 'paper_date': '2016-09-17', 'paper_title': 'Multi-region two-stream R-CNN for action detection', 'paper_url': 'https://doi.org/10.1007/978-3-319-46493-0_45', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '69.5'}, 'model_links': [], 'model_name': 'PA3D', 'paper_date': '2019-06-01', 'paper_title': 'PA3D: Pose-Action 3D Machine for Video Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '64.3'}, 'model_links': [], 'model_name': 'STAR-Net', 'paper_date': '2019-02-26', 'paper_title': 'STAR-Net: Action Recognition using Spatio-Temporal Activation Reprojection', 'paper_url': 'http://arxiv.org/abs/1902.10024v1', 'uses_additional_data': False}, {'code_links': [{'title': 'JeffCHEN2017/WSSTG', 'url': 'https://github.com/JeffCHEN2017/WSSTG'}], 'metrics': {'Accuracy (RGB+pose)': '62.5'}, 'model_links': [], 'model_name': 'Action Tubes', 'paper_date': '2014-11-21', 'paper_title': 'Finding Action Tubes', 'paper_url': 'http://arxiv.org/abs/1411.6031v1', 'uses_additional_data': False}, {'code_links': [{'title': 'fandulu/DD-Net', 'url': 'https://github.com/fandulu/DD-Net'}, {'title': 'BlurryLight/DD-Net-Pytorch', 'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}], 'metrics': {'Accuracy (RGB+pose)': '-', 'Accuracy (pose)': '77.2'}, 'model_links': [], 'model_name': 'DD-Net', 'paper_date': '2019-07-23', 'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better', 'paper_url': 'https://arxiv.org/abs/1907.09658v8', 'uses_additional_data': False}, {'code_links': [{'title': 'noboevbo/ehpi_action_recognition', 'url': 'https://github.com/noboevbo/ehpi_action_recognition'}], 'metrics': {'Accuracy (RGB+pose)': '-', 'Accuracy (pose)': '65.5'}, 'model_links': [], 'model_name': 'EHPI', 'paper_date': '2019-04-19', 'paper_title': 'Simple yet efficient real-time pose-based action recognition', 'paper_url': 'http://arxiv.org/abs/1904.09140v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UPenn Action', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-upenn'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'google-research/google-research', 'url': 'https://github.com/google-research/google-research/tree/master/poem'}], 'metrics': {'Accuracy': '97.5'}, 'model_links': [], 'model_name': 'Pr-VIPE', 'paper_date': '2019-12-02', 'paper_title': 'View-Invariant Probabilistic Embedding for Human Pose', 'paper_url': 'https://arxiv.org/abs/1912.01001v4', 'uses_additional_data': False}, {'code_links': [{'title': 'rort1989/HDM', 'url': 'https://github.com/rort1989/HDM'}], 'metrics': {'Accuracy': '93.4'}, 'model_links': [], 'model_name': 'HDM-BG', 'paper_date': '2019-06-01', 'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}]}, 'subdatasets': []}], 'description': '&lt;span style=\"color:grey; opacity: 0.6\"&gt;( Image credit: [View Adaptive Neural Networks for High\r\n",
       "Performance Skeleton-based Human Action\r\n",
       "Recognition](https://arxiv.org/pdf/1804.07453v3.pdf) )&lt;/span&gt;', 'source_link': None, 'subtasks': [], 'synonyms': [], 'task': 'Skeleton Based Action Recognition'}, {'categories': [], 'datasets': [{'dataset': 'NTU RGB+D 120', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/one-shot-3d-action-recognition-on-ntu-rgbd'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'raphaelmemmesheimer/sl-dml', 'url': 'https://github.com/raphaelmemmesheimer/sl-dml'}], 'metrics': {'Accuracy': '49.6%'}, 'model_links': [], 'model_name': 'Deep Metric Learning (Triplet Loss, Signals)', 'paper_date': '2020-04-23', 'paper_title': 'SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action Recognition', 'paper_url': 'https://arxiv.org/abs/2004.11085v4', 'uses_additional_data': False}, {'code_links': [{'title': 'AlbertoSabater/Skeleton-based-One-shot-Action-Recognition', 'url': 'https://github.com/AlbertoSabater/Skeleton-based-One-shot-Action-Recognition'}], 'metrics': {'Accuracy': '46.5%'}, 'model_links': [], 'model_name': 'TCN_OneShot', 'paper_date': '2021-02-17', 'paper_title': 'One-shot action recognition in challenging therapy scenarios', 'paper_url': 'https://arxiv.org/abs/2102.08997v3', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}, {'title': 'LinguoLi/CrosSCLR', 'url': 'https://github.com/LinguoLi/CrosSCLR'}], 'metrics': {'Accuracy': '45.3%'}, 'model_links': [], 'model_name': 'APSR', 'paper_date': '2019-05-12', 'paper_title': 'NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding', 'paper_url': 'https://arxiv.org/abs/1905.04757v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '42.9%'}, 'model_links': [], 'model_name': 'Average Pooling', 'paper_date': '2017-06-26', 'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates', 'paper_url': 'http://arxiv.org/abs/1706.08276v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '42.1%'}, 'model_links': [], 'model_name': 'Fully Connected', 'paper_date': '2017-07-01', 'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '41.0%'}, 'model_links': [], 'model_name': 'Attention Network', 'paper_date': '2017-07-01', 'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html', 'uses_additional_data': False}]}, 'subdatasets': []}], 'description': '', 'source_link': None, 'subtasks': [], 'synonyms': [], 'task': 'One-Shot 3D Action Recognition'}, {'categories': [], 'datasets': [], 'description': 'Detect if two people are looking at each other', 'source_link': None, 'subtasks': [], 'synonyms': [], 'task': 'Mutual Gaze'}]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Human Interaction Recognition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            categories  \\\n",
       "100  [Computer Vision]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        datasets  \\\n",
       "100  [{'dataset': 'NTU RGB+D 120', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-ntu-rgb-d-1'}], 'description': '', 'sota': {'metrics': ['Accuracy (Cross-Setup)', 'Accuracy (Cross-Subject)'], 'rows': [{'code_links': [{'title': 'mauriciolp/inter-rel-net', 'url': 'https://github.com/mauriciolp/inter-rel-net'}], 'metrics': {'Accuracy (Cross-Setup)': '79.6', 'Accuracy (Cross-Subject)': '77.7'}, 'model_links': [], 'model_name': \"LSTM-IRN'fc1inter+intra\", 'paper_date': '2019-10-11', 'paper_title': 'Interaction Relational Network for Mutual Action Recognition', 'paper_url': 'https://arxiv.org/abs/1910.04963v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'BIT', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-bit'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '94.03'}, 'model_links': [], 'model_name': 'H-LSTCM', 'paper_date': '2018-11-01', 'paper_title': 'Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition', 'paper_url': 'http://arxiv.org/abs/1811.00270v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '92.88'}, 'model_links': [], 'model_name': 'Co-LSTSM', 'paper_date': '2017-06-03', 'paper_title': 'Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition', 'paper_url': 'http://arxiv.org/abs/1706.00931v1', 'uses_additional_data': False}, {'code_links': [{'title': 'garythung/torch-lrcn', 'url': 'https://github.com/garythung/torch-lrcn'}, {'title': 'doronharitan/human_activity_recognition_LRCN', 'url': 'https://github.com/doronharitan/human_activity_recognition_LRCN'}, {'title': 'DJAlexJ/LRCN-for-Video-Regression', 'url': 'https://github.com/DJAlexJ/LRCN-for-Video-Regression'}, {'title': 'sujaygarlanka/tennis_stroke_recognition', 'url': 'https://github.com/sujaygarlanka/tennis_stroke_recognition'}, {'title': 'rlaengud123/CMC_LRCN', 'url': 'https://github.com/rlaengud123/CMC_LRCN'}, {'title': 'Deepu1992/VideoClassification', 'url': 'https://github.com/Deepu1992/VideoClassification'}, {'title': 'kahnchana/RNN', 'url': 'https://github.com/kahnchana/RNN'}], 'metrics': {'Accuracy': '80.13'}, 'model_links': [], 'model_name': 'Donahue et al.', 'paper_date': '2014-11-17', 'paper_title': 'Long-term Recurrent Convolutional Networks for Visual Recognition and Description', 'paper_url': 'http://arxiv.org/abs/1411.4389v4', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'SBU', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-sbu'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'mauriciolp/inter-rel-net', 'url': 'https://github.com/mauriciolp/inter-rel-net'}], 'metrics': {'Accuracy': '98.2'}, 'model_links': [], 'model_name': \"LSTM-IRN'fc1inter+intra\", 'paper_date': '2019-10-11', 'paper_title': 'Interaction Relational Network for Mutual Action Recognition', 'paper_url': 'https://arxiv.org/abs/1910.04963v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UT-Interaction', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-ut-1'}], 'description': '', 'sota': {'metrics': ['Accuracy (Set 1)', 'Accuracy (Set 2)'], 'rows': [{'code_links': [{'title': 'mauriciolp/inter-rel-net', 'url': 'https://github.com/mauriciolp/inter-rel-net'}], 'metrics': {'Accuracy (Set 1)': '98.3', 'Accuracy (Set 2)': '96.7'}, 'model_links': [], 'model_name': \"LSTM-IRN'fc1inter+intra\", 'paper_date': '2019-10-11', 'paper_title': 'Interaction Relational Network for Mutual Action Recognition', 'paper_url': 'https://arxiv.org/abs/1910.04963v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'NTU RGB+D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-ntu-rgb-d'}], 'description': '', 'sota': {'metrics': ['Accuracy (Cross-Subject)', 'Accuracy (Cross-View)'], 'rows': [{'code_links': [{'title': 'mauriciolp/inter-rel-net', 'url': 'https://github.com/mauriciolp/inter-rel-net'}], 'metrics': {'Accuracy (Cross-Subject)': '90.5', 'Accuracy (Cross-View)': '93.5'}, 'model_links': [], 'model_name': \"LSTM-IRN'fc1inter+intra\", 'paper_date': '2019-10-11', 'paper_title': 'Interaction Relational Network for Mutual Action Recognition', 'paper_url': 'https://arxiv.org/abs/1910.04963v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UT', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/human-interaction-recognition-on-ut'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '98.33'}, 'model_links': [], 'model_name': 'H-LSTCM', 'paper_date': '2018-11-01', 'paper_title': 'Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition', 'paper_url': 'http://arxiv.org/abs/1811.00270v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '95.00'}, 'model_links': [], 'model_name': 'Co-LSTSM', 'paper_date': '2017-06-03', 'paper_title': 'Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition', 'paper_url': 'http://arxiv.org/abs/1706.00931v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '93.30'}, 'model_links': [], 'model_name': 'Raptis et al.', 'paper_date': '2013-06-01', 'paper_title': 'Poselet Key-Framing: A Model for Human Activity Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2013/html/Raptis_Poselet_Key-Framing_A_2013_CVPR_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'garythung/torch-lrcn', 'url': 'https://github.com/garythung/torch-lrcn'}, {'title': 'doronharitan/human_activity_recognition_LRCN', 'url': 'https://github.com/doronharitan/human_activity_recognition_LRCN'}, {'title': 'DJAlexJ/LRCN-for-Video-Regression', 'url': 'https://github.com/DJAlexJ/LRCN-for-Video-Regression'}, {'title': 'sujaygarlanka/tennis_stroke_recognition', 'url': 'https://github.com/sujaygarlanka/tennis_stroke_recognition'}, {'title': 'rlaengud123/CMC_LRCN', 'url': 'https://github.com/rlaengud123/CMC_LRCN'}, {'title': 'Deepu1992/VideoClassification', 'url': 'https://github.com/Deepu1992/VideoClassification'}, {'title': 'kahnchana/RNN', 'url': 'https://github.com/kahnchana/RNN'}], 'metrics': {'Accuracy': '85.00'}, 'model_links': [], 'model_name': 'Donahue et al.', 'paper_date': '2014-11-17', 'paper_title': 'Long-term Recurrent Convolutional Networks for Visual Recognition and Description', 'paper_url': 'http://arxiv.org/abs/1411.4389v4', 'uses_additional_data': False}]}, 'subdatasets': []}]   \n",
       "\n",
       "    description  source_link  \\\n",
       "100                      NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             subtasks  \\\n",
       "100  [{'categories': [], 'datasets': [{'dataset': 'SYSU 3D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sysu-3d'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'microsoft/SGN', 'url': 'https://github.com/microsoft/SGN'}], 'metrics': {'Accuracy': '86.9%'}, 'model_links': [], 'model_name': 'SGN', 'paper_date': '2019-04-02', 'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1904.01189v3', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '86.7%'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '85.7%'}, 'model_links': [], 'model_name': 'EleAtt-GRU (aug.)', 'paper_date': '2019-09-03', 'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks', 'paper_url': 'https://arxiv.org/abs/1909.01939v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '83.14%'}, 'model_links': [], 'model_name': 'Local+LGN', 'paper_date': '2019-09-02', 'paper_title': 'Learning Latent Global Network for Skeleton-based Action Prediction', 'paper_url': 'https://doi.org/10.1109/TIP.2019.2937757', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '77.9%'}, 'model_links': [], 'model_name': 'Complete GR-GCN', 'paper_date': '2018-11-29', 'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression', 'paper_url': 'http://arxiv.org/abs/1811.12013v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '77.5%'}, 'model_links': [], 'model_name': 'VA-LSTM', 'paper_date': '2017-03-24', 'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data', 'paper_url': 'http://arxiv.org/abs/1703.08274v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '76.9%'}, 'model_links': [], 'model_name': 'DPRL', 'paper_date': '2018-06-01', 'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '75.5%'}, 'model_links': [], 'model_name': 'Dynamic Skeletons', 'paper_date': '2016-12-15', 'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition', 'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '73.4%'}, 'model_links': [], 'model_name': 'ST-LSTM (Tree)', 'paper_date': '2017-06-26', 'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates', 'paper_url': 'http://arxiv.org/abs/1706.08276v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UT-Kinect', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ut'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '99.50%'}, 'model_links': [], 'model_name': 'Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '98.5%'}, 'model_links': [], 'model_name': 'Complete GR-GCN', 'paper_date': '2018-11-29', 'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression', 'paper_url': 'http://arxiv.org/abs/1811.12013v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '98.5%'}, 'model_links': [], 'model_name': 'DPRL', 'paper_date': '2018-06-01', 'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'fdsa1860/skeletal', 'url': 'https://github.com/fdsa1860/skeletal'}], 'metrics': {'Accuracy': '97.1%'}, 'model_links': [], 'model_name': 'Lie Group', 'paper_date': '2014-06-23', 'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group', 'paper_url': 'https://doi.org/10.1109/CVPR.2014.82', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '96%'}, 'model_links': [], 'model_name': 'GFT', 'paper_date': '2019-08-26', 'paper_title': 'Graph Based Skeleton Modeling for Human Activity Analysis', 'paper_url': 'https://doi.org/10.1109/ICIP.2019.8803186', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'N-UCLA', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-n-ucla'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '93.99'}, 'model_links': [], 'model_name': 'Hierarchical Action Classification (RGB + Pose)', 'paper_date': '2020-07-30', 'paper_title': 'Hierarchical Action Classification with Network Pruning', 'paper_url': 'https://arxiv.org/abs/2007.15244v1', 'uses_additional_data': False}, {'code_links': [{'title': 'srijandas07/VPN', 'url': 'https://github.com/srijandas07/VPN'}], 'metrics': {'Accuracy': '93.5'}, 'model_links': [], 'model_name': 'VPN (RGB + Pose)', 'paper_date': '2020-07-06', 'paper_title': 'VPN: Learning Video-Pose Embedding for Activities of Daily Living', 'paper_url': 'https://arxiv.org/abs/2007.03056v1', 'uses_additional_data': False}, {'code_links': [{'title': 'srijandas07/vpnplusplus', 'url': 'https://github.com/srijandas07/vpnplusplus'}], 'metrics': {'Accuracy': '93.5'}, 'model_links': [], 'model_name': 'VPN++ (RGB + Pose)', 'paper_date': '2021-05-17', 'paper_title': 'VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living', 'paper_url': 'https://arxiv.org/abs/2105.08141v1', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/SGN', 'url': 'https://github.com/microsoft/SGN'}], 'metrics': {'Accuracy': '92.5%'}, 'model_links': [], 'model_name': 'SGN', 'paper_date': '2019-04-02', 'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1904.01189v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '92.3%'}, 'model_links': [], 'model_name': 'Action Machine', 'paper_date': '2018-12-14', 'paper_title': 'Action Machine: Rethinking Action Recognition in Trimmed Videos', 'paper_url': 'http://arxiv.org/abs/1812.05770v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '90.7%'}, 'model_links': [], 'model_name': 'EleAtt-GRU (aug.)', 'paper_date': '2019-09-03', 'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks', 'paper_url': 'https://arxiv.org/abs/1909.01939v1', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '88.1%'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [{'title': 'fabienbaradel/glimpse_clouds', 'url': 'https://github.com/fabienbaradel/glimpse_clouds'}], 'metrics': {'Accuracy': '87.6%'}, 'model_links': [], 'model_name': 'Glimpse Clouds', 'paper_date': '2018-02-22', 'paper_title': 'Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points', 'paper_url': 'http://arxiv.org/abs/1802.07898v4', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'SBU', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sbu'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'Sy-Zhang/Geometric-Feature-Release', 'url': 'https://github.com/Sy-Zhang/Geometric-Feature-Release'}], 'metrics': {'Accuracy': '99.02%'}, 'model_links': [], 'model_name': 'Joint Line Distance', 'paper_date': '2017-03-01', 'paper_title': 'On Geometric Features for Skeleton-Based Action Recognition using Multilayer LSTM Networks', 'paper_url': 'https://doi.org/10.1109/WACV.2017.24', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '98.60%'}, 'model_links': [], 'model_name': 'MLGCN', 'paper_date': '2019-09-11', 'paper_title': 'MLGCN: Multi-Laplacian Graph Convolutional Networks for Human Action Recognition', 'paper_url': 'https://bmvc2019.org/wp-content/uploads/papers/1103-paper.pdf', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '98.3%'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [{'title': 'mdeff/cnn_graph', 'url': 'https://github.com/mdeff/cnn_graph'}, {'title': 'xbresson/spectral_graph_convnets', 'url': 'https://github.com/xbresson/spectral_graph_convnets'}, {'title': 'jasonseu/cnn_graph.pytorch', 'url': 'https://github.com/jasonseu/cnn_graph.pytorch'}, {'title': 'andrejmiscic/gcn-pytorch', 'url': 'https://github.com/andrejmiscic/gcn-pytorch'}, {'title': 'mdeff/paper-cnn-graph-nips2016', 'url': 'https://github.com/mdeff/paper-cnn-graph-nips2016'}, {'title': 'hazdzz/ChebyNet', 'url': 'https://github.com/hazdzz/ChebyNet'}, {'title': 'stsfaroz/Graph-Convolutional-Networks-with-Cora-Dataset', 'url': 'https://github.com/stsfaroz/Graph-Convolutional-Networks-with-Cora-Dataset'}], 'metrics': {'Accuracy': '96.00%'}, 'model_links': [], 'model_name': 'ChebyNet', 'paper_date': '2016-06-30', 'paper_title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering', 'paper_url': 'http://arxiv.org/abs/1606.09375v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '96.00%'}, 'model_links': [], 'model_name': 'ArmaConv', 'paper_date': '2019-01-05', 'paper_title': 'Graph Neural Networks with convolutional ARMA filters', 'paper_url': 'https://arxiv.org/abs/1901.01343v7', 'uses_additional_data': False}, {'code_links': [{'title': 'Maghoumi/DeepGRU', 'url': 'https://github.com/Maghoumi/DeepGRU'}], 'metrics': {'Accuracy': '95.7%'}, 'model_links': [], 'model_name': 'DeepGRU', 'paper_date': '2018-10-30', 'paper_title': 'DeepGRU: Deep Gesture Recognition Utility', 'paper_url': 'https://arxiv.org/abs/1810.12514v4', 'uses_additional_data': False}, {'code_links': [{'title': 'Tiiiger/SGC', 'url': 'https://github.com/Tiiiger/SGC'}, {'title': 'pulkit1joshi/SGC', 'url': 'https://github.com/pulkit1joshi/SGC'}, {'title': 'hazdzz/SGC', 'url': 'https://github.com/hazdzz/SGC'}], 'metrics': {'Accuracy': '94.0%'}, 'model_links': [], 'model_name': 'SGCConv', 'paper_date': '2019-02-19', 'paper_title': 'Simplifying Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1902.07153v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '93.3%'}, 'model_links': [], 'model_name': 'ST-LSTM + Trust Gate', 'paper_date': '2016-07-24', 'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1607.07043v1', 'uses_additional_data': False}, {'code_links': [{'title': 'tkipf/gcn', 'url': 'https://github.com/tkipf/gcn'}, {'title': 'tkipf/pygcn', 'url': 'https://github.com/tkipf/pygcn'}, {'title': 'tkipf/keras-gcn', 'url': 'https://github.com/tkipf/keras-gcn'}, {'title': 'LeeWooJung/GCN_reproduce', 'url': 'https://github.com/LeeWooJung/GCN_reproduce'}, {'title': 'hazdzz/GCN', 'url': 'https://github.com/hazdzz/GCN'}, {'title': 'giuseppefutia/link-prediction-code', 'url': 'https://github.com/giuseppefutia/link-prediction-code'}, {'title': 'andrejmiscic/gcn-pytorch', 'url': 'https://github.com/andrejmiscic/gcn-pytorch'}, {'title': 'switiz/gnn-gcn-gat', 'url': 'https://github.com/switiz/gnn-gcn-gat'}, {'title': 'dtriepke/Graph_Convolutional_Network', 'url': 'https://github.com/dtriepke/Graph_Convolutional_Network'}, {'title': 'bcsrn/gcn', 'url': 'https://github.com/bcsrn/gcn'}, {'title': 'Anieca/GCN', 'url': 'https://github.com/Anieca/GCN'}, {'title': 'HoganZhang/pygcn_python3', 'url': 'https://github.com/HoganZhang/pygcn_python3'}, {'title': 'lipingcoding/pygcn', 'url': 'https://github.com/lipingcoding/pygcn'}, {'title': 'KimMeen/GCN', 'url': 'https://github.com/KimMeen/GCN'}, {'title': 'darnbi/pygcn', 'url': 'https://github.com/darnbi/pygcn'}, {'title': 'thanhtrunghuynh93/pygcn', 'url': 'https://github.com/thanhtrunghuynh93/pygcn'}, {'title': 'ChengSashankh/gcn-graph-classification', 'url': 'https://github.com/ChengSashankh/gcn-graph-classification'}, {'title': 'LouisDumont/GCN---re-implementation', 'url': 'https://github.com/LouisDumont/GCN---re-implementation'}], 'metrics': {'Accuracy': '90.00%'}, 'model_links': [], 'model_name': 'GCNConv', 'paper_date': '2016-09-09', 'paper_title': 'Semi-Supervised Classification with Graph Convolutional Networks', 'paper_url': 'http://arxiv.org/abs/1609.02907v4', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'JHMDB Pose Tracking', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-jhmdb'}], 'description': '', 'sota': {'metrics': ['PCK@0.1', 'PCK@0.2', 'PCK@0.3', 'PCK@0.4', 'PCK@0.5'], 'rows': [{'code_links': [{'title': 'aimerykong/predictive-filter-flow', 'url': 'https://github.com/aimerykong/predictive-filter-flow'}, {'title': 'bestaar/predictiveFilterFlow', 'url': 'https://github.com/bestaar/predictiveFilterFlow'}], 'metrics': {'PCK@0.1': '58.4', 'PCK@0.2': '78.1', 'PCK@0.3': '85.9', 'PCK@0.4': '89.8', 'PCK@0.5': '92.4'}, 'model_links': [], 'model_name': 'mgPFF+ft 1st', 'paper_date': '2019-04-02', 'paper_title': 'Multigrid Predictive Filter Flow for Unsupervised Learning on Videos', 'paper_url': 'http://arxiv.org/abs/1904.01693v1', 'uses_additional_data': False}, {'code_links': [{'title': 'hyperparameters/tracking_via_colorization', 'url': 'https://github.com/hyperparameters/tracking_via_colorization'}], 'metrics': {'PCK@0.1': '45.2', 'PCK@0.2': '69.6', 'PCK@0.3': '80.8', 'PCK@0.4': '87.5', 'PCK@0.5': '91.4'}, 'model_links': [], 'model_name': 'ColorPointer', 'paper_date': '2018-06-25', 'paper_title': 'Tracking Emerges by Colorizing Videos', 'paper_url': 'http://arxiv.org/abs/1806.09594v2', 'uses_additional_data': False}, {'code_links': [{'title': 'NVIDIA/flownet2-pytorch', 'url': 'https://github.com/NVIDIA/flownet2-pytorch'}, {'title': 'philferriere/tfoptflow', 'url': 'https://github.com/philferriere/tfoptflow'}, {'title': 'ElliotHYLee/VisualOdometry3D', 'url': 'https://github.com/ElliotHYLee/VisualOdometry3D'}, {'title': 'mcgridles/LENS', 'url': 'https://github.com/mcgridles/LENS'}, {'title': 'rickyHong/tfoptflow-repl', 'url': 'https://github.com/rickyHong/tfoptflow-repl'}], 'metrics': {'PCK@0.1': '45.2', 'PCK@0.2': '62.9', 'PCK@0.3': '73.5', 'PCK@0.4': '80.6', 'PCK@0.5': '85.5'}, 'model_links': [], 'model_name': 'FlowNet2', 'paper_date': '2016-12-06', 'paper_title': 'FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks', 'paper_url': 'http://arxiv.org/abs/1612.01925v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'NTU RGB+D 120', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu-rgbd-1'}], 'description': '', 'sota': {'metrics': ['Accuracy (Cross-Subject)', 'Accuracy (Cross-Setup)'], 'rows': [{'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (Cross-Setup)': '89.1', 'Accuracy (Cross-Subject)': '88.3'}, 'model_links': [], 'model_name': 'EfficientGCN-B4', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '89.2%', 'Accuracy (Cross-Subject)': '88.2%'}, 'model_links': [], 'model_name': 'AngNet-JA + BA + JBA + VJBA', 'paper_date': '2021-05-04', 'paper_title': 'Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2105.01563v3', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (Cross-Setup)': '88.0', 'Accuracy (Cross-Subject)': '87.9'}, 'model_links': [], 'model_name': 'EfficientGCN-B2', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/ResGCNv1', 'url': 'https://github.com/yfsong0709/ResGCNv1'}, {'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (Cross-Setup)': '88.3%', 'Accuracy (Cross-Subject)': '87.3%'}, 'model_links': [], 'model_name': 'PA-ResGCN-B19', 'paper_date': '2020-10-20', 'paper_title': 'Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2010.09978v1', 'uses_additional_data': False}, {'code_links': [{'title': 'skelemoa/quovadis', 'url': 'https://github.com/skelemoa/quovadis'}], 'metrics': {'Accuracy (Cross-Setup)': '88.8%', 'Accuracy (Cross-Subject)': '87.22%'}, 'model_links': [], 'model_name': 'Ensemble-top5 (MS-G3D Net + 4s Shift-GCN + VA-CNN (ResNeXt101) + 2s SDGCN + GCN-NAS (retrained))', 'paper_date': '2020-07-04', 'paper_title': 'Quo Vadis, Skeleton Action Recognition ?', 'paper_url': 'https://arxiv.org/abs/2007.02072v2', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}], 'metrics': {'Accuracy (Cross-Setup)': '90.3', 'Accuracy (Cross-Subject)': '86.9'}, 'model_links': [], 'model_name': 'PoseC3D (w. HRNet 2D Skeleton)', 'paper_date': '2021-04-28', 'paper_title': 'Revisiting Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2104.13586v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/ms-g3d', 'url': 'https://github.com/kenziyuliu/ms-g3d'}], 'metrics': {'Accuracy (Cross-Setup)': '88.4%', 'Accuracy (Cross-Subject)': '86.9%'}, 'model_links': [], 'model_name': 'MS-G3D Net', 'paper_date': '2020-03-31', 'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.14111v2', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/DSTA-Net', 'url': 'https://github.com/lshiwjx/DSTA-Net'}], 'metrics': {'Accuracy (Cross-Setup)': '89.0 %', 'Accuracy (Cross-Subject)': '86.6%'}, 'model_links': [], 'model_name': 'DSTA-Net', 'paper_date': '2020-07-07', 'paper_title': 'Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.03263v1', 'uses_additional_data': False}, {'code_links': [{'title': 'srijandas07/VPN', 'url': 'https://github.com/srijandas07/VPN'}], 'metrics': {'Accuracy (Cross-Setup)': '87.8', 'Accuracy (Cross-Subject)': '86.3'}, 'model_links': [], 'model_name': 'VPN', 'paper_date': '2020-07-06', 'paper_title': 'VPN: Learning Video-Pose Embedding for Activities of Daily Living', 'paper_url': 'https://arxiv.org/abs/2007.03056v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kchengiva/Shift-GCN', 'url': 'https://github.com/kchengiva/Shift-GCN'}], 'metrics': {'Accuracy (Cross-Setup)': '87.6%', 'Accuracy (Cross-Subject)': '85.9%'}, 'model_links': [], 'model_name': '4s Shift-GCN', 'paper_date': '2020-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (Cross-Setup)': '84.3', 'Accuracy (Cross-Subject)': '85.9'}, 'model_links': [], 'model_name': 'EfficientGCN-B0', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '87.4%', 'Accuracy (Cross-Subject)': '85.4%'}, 'model_links': [], 'model_name': 'FGCN ', 'paper_date': '2020-03-17', 'paper_title': 'Feedback Graph Convolutional Network for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.07564v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '86.90', 'Accuracy (Cross-Subject)': '84.88'}, 'model_links': [], 'model_name': 'VA-CNN (ResNeXt-101)', 'paper_date': None, 'paper_title': '', 'paper_url': '', 'uses_additional_data': False}, {'code_links': [{'title': 'Chiaraplizz/ST-TR', 'url': 'https://github.com/Chiaraplizz/ST-TR'}], 'metrics': {'Accuracy (Cross-Setup)': '84.7%', 'Accuracy (Cross-Subject)': '82.7%'}, 'model_links': [], 'model_name': 'ST-TR-agcn', 'paper_date': '2020-08-17', 'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks', 'paper_url': 'https://arxiv.org/abs/2008.07404v4', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/RA-GCNv1', 'url': 'https://github.com/yfsong0709/RA-GCNv1'}, {'title': 'yfsong0709/RA-GCNv2', 'url': 'https://github.com/yfsong0709/RA-GCNv2'}, {'title': 'peter-yys-yoon/pegcnv2', 'url': 'https://github.com/peter-yys-yoon/pegcnv2'}], 'metrics': {'Accuracy (Cross-Setup)': '82.7%', 'Accuracy (Cross-Subject)': '81.1%'}, 'model_links': [], 'model_name': '3s RA-GCN', 'paper_date': '2020-08-09', 'paper_title': 'Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2008.03791v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '83.2%', 'Accuracy (Cross-Subject)': '80.5%'}, 'model_links': [], 'model_name': 'Mix-Dimension', 'paper_date': '2020-07-30', 'paper_title': 'Mix Dimension in Poincar√© Geometry for 3D Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.15678v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '79.8%', 'Accuracy (Cross-Subject)': '78.3%'}, 'model_links': [], 'model_name': 'GVFE + AS-GCN with DH-TCN', 'paper_date': '2019-12-20', 'paper_title': 'Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition', 'paper_url': 'https://arxiv.org/abs/1912.09745v1', 'uses_additional_data': False}, {'code_links': [{'title': 'airglow/gimme_signals_action_recognition', 'url': 'https://github.com/airglow/gimme_signals_action_recognition'}, {'title': 'raphaelmemmesheimer/gimme_signals_action_recognition', 'url': 'https://github.com/raphaelmemmesheimer/gimme_signals_action_recognition'}], 'metrics': {'Accuracy (Cross-Setup)': '71.6%', 'Accuracy (Cross-Subject)': '70.8%'}, 'model_links': [], 'model_name': 'Gimme Signals (Skeleton, AIS)', 'paper_date': '2020-03-13', 'paper_title': 'Gimme Signals: Discriminative signal encoding for multimodal activity recognition', 'paper_url': 'https://arxiv.org/abs/2003.06156v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '67.2%', 'Accuracy (Cross-Subject)': '68.3%'}, 'model_links': [], 'model_name': 'Logsig-RNN', 'paper_date': '2019-08-22', 'paper_title': 'Learning stochastic differential equations using RNN with log signature features', 'paper_url': 'https://arxiv.org/abs/1908.08286v2', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (Cross-Setup)': '62.8%', 'Accuracy (Cross-Subject)': '67.9%'}, 'model_links': [], 'model_name': 'TSRJI (Late Fusion) + HCN', 'paper_date': '2019-09-11', 'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints', 'paper_url': 'https://arxiv.org/abs/1909.05704v1', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (Cross-Setup)': '66.9%', 'Accuracy (Cross-Subject)': '67.7%'}, 'model_links': [], 'model_name': 'SkeleMotion + Yang et al. (2018)', 'paper_date': '2019-07-30', 'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition', 'paper_url': 'https://arxiv.org/abs/1907.13025v1', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (Cross-Setup)': '59.7%', 'Accuracy (Cross-Subject)': '65.5%'}, 'model_links': [], 'model_name': 'TSRJI (Late Fusion)', 'paper_date': '2019-09-11', 'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints', 'paper_url': 'https://arxiv.org/abs/1909.05704v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '66.9%', 'Accuracy (Cross-Subject)': '64.6%'}, 'model_links': [], 'model_name': 'Body Pose Evolution Map', 'paper_date': '2018-06-01', 'paper_title': 'Recognizing Human Actions as the Evolution of Pose Estimation Maps', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Recognizing_Human_Actions_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (Cross-Setup)': '63.0%', 'Accuracy (Cross-Subject)': '62.9%'}, 'model_links': [], 'model_name': 'SkeleMotion [Magnitude-Orientation (TSA)]', 'paper_date': '2019-07-30', 'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition', 'paper_url': 'https://arxiv.org/abs/1907.13025v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '61.8%', 'Accuracy (Cross-Subject)': '62.2%'}, 'model_links': [], 'model_name': 'Multi-Task CNN with RotClips', 'paper_date': '2018-03-05', 'paper_title': 'Learning clip representations for skeleton-based 3d action recognition', 'paper_url': 'https://doi.org/10.1109/TIP.2018.2812099', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '63.3%', 'Accuracy (Cross-Subject)': '61.2%'}, 'model_links': [], 'model_name': 'Two-Stream Attention LSTM', 'paper_date': '2017-07-18', 'paper_title': 'Skeleton-Based Human Action Recognition with Global Context-Aware Attention LSTM Networks', 'paper_url': 'http://arxiv.org/abs/1707.05740v5', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '63.2%', 'Accuracy (Cross-Subject)': '60.3%'}, 'model_links': [], 'model_name': 'Skeleton Visualization (Single Stream)', 'paper_date': '2017-08-01', 'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition', 'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '62.4%', 'Accuracy (Cross-Subject)': '59.9%'}, 'model_links': [], 'model_name': 'FSNet', 'paper_date': '2019-02-08', 'paper_title': 'Skeleton-Based Online Action Prediction Using Scale Selection Network', 'paper_url': 'http://arxiv.org/abs/1902.03084v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '57.9%', 'Accuracy (Cross-Subject)': '58.4%'}, 'model_links': [], 'model_name': 'Multi-Task Learning Network', 'paper_date': '2017-03-09', 'paper_title': 'A New Representation of Skeleton Sequences for 3D Action Recognition', 'paper_url': 'http://arxiv.org/abs/1703.03492v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '59.2%', 'Accuracy (Cross-Subject)': '58.3%'}, 'model_links': [], 'model_name': 'GCA-LSTM', 'paper_date': '2017-07-01', 'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '60.9%', 'Accuracy (Cross-Subject)': '58.2%'}, 'model_links': [], 'model_name': 'Internal Feature Fusion', 'paper_date': '2017-06-26', 'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates', 'paper_url': 'http://arxiv.org/abs/1706.08276v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '57.9%', 'Accuracy (Cross-Subject)': '55.7%'}, 'model_links': [], 'model_name': 'Spatio-Temporal LSTM', 'paper_date': '2016-07-24', 'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1607.07043v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '54.7%', 'Accuracy (Cross-Subject)': '50.8%'}, 'model_links': [], 'model_name': 'Dynamic Skeletons', 'paper_date': '2016-12-15', 'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition', 'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (Cross-Setup)': '44.9%', 'Accuracy (Cross-Subject)': '36.3%'}, 'model_links': [], 'model_name': 'Soft RNN', 'paper_date': '2018-08-06', 'paper_title': 'Early action prediction by soft regression', 'paper_url': 'https://doi.org/10.1109/TPAMI.2018.2863279', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (Cross-Setup)': '26.3%', 'Accuracy (Cross-Subject)': '25.5%'}, 'model_links': [], 'model_name': 'Part-Aware LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'HDM05', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-hdm05'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '89.80%'}, 'model_links': [], 'model_name': 'Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Gaming 3D (G3D)', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-gaming'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '96.0'}, 'model_links': [], 'model_name': 'CNN', 'paper_date': '2016-12-30', 'paper_title': 'Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks', 'paper_url': 'http://arxiv.org/abs/1612.09401v1', 'uses_additional_data': False}, {'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '92.91%'}, 'model_links': [], 'model_name': 'Temporal K-Means Clustering + Temporal Covariance Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}, {'code_links': [{'title': 'rort1989/HDM', 'url': 'https://github.com/rort1989/HDM'}], 'metrics': {'Accuracy': '92.0'}, 'model_links': [], 'model_name': 'HDM-BG', 'paper_date': '2019-06-01', 'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '90.94'}, 'model_links': [], 'model_name': 'Rolling Rotations (FTP)', 'paper_date': '2016-06-27', 'paper_title': 'Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data', 'paper_url': 'https://doi.org/10.1109/CVPR.2016.484', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UWA3D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-uwa3d'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '81.4%'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '73.8%'}, 'model_links': [], 'model_name': 'ESV (Synthesized + Pre-trained)', 'paper_date': '2017-08-01', 'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition', 'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '17.7%'}, 'model_links': [], 'model_name': 'HOJ3D', 'paper_date': '2012-07-16', 'paper_title': 'View invariant human action recognition using histograms of 3D joints', 'paper_url': 'https://doi.org/10.1109/CVPRW.2012.6239233', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'MSRC-12', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msrc-12'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '99.08%'}, 'model_links': [], 'model_name': 'Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Kinetics-Skeleton dataset', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-kinetics'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}], 'metrics': {'Accuracy': '47.7'}, 'model_links': [], 'model_name': 'PoseC3D (w. HRNet 2D skeleton)', 'paper_date': '2021-04-28', 'paper_title': 'Revisiting Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2104.13586v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '38.6'}, 'model_links': [], 'model_name': '2s-AGCN+TEM', 'paper_date': '2020-03-19', 'paper_title': 'Temporal Extension Module for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.08951v2', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/ms-g3d', 'url': 'https://github.com/kenziyuliu/ms-g3d'}], 'metrics': {'Accuracy': '38.0'}, 'model_links': [], 'model_name': 'MS-G3D', 'paper_date': '2020-03-31', 'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.14111v2', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}], 'metrics': {'Accuracy': '38.0'}, 'model_links': [], 'model_name': 'PoseC3D', 'paper_date': '2021-04-28', 'paper_title': 'Revisiting Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2104.13586v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '37.9'}, 'model_links': [], 'model_name': 'Dynamic GCN', 'paper_date': '2020-07-29', 'paper_title': 'Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.14690v1', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '37.8'}, 'model_links': [], 'model_name': 'MS-AAGCN', 'paper_date': '2019-12-15', 'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1912.06971v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '37.5'}, 'model_links': [], 'model_name': 'CGCN', 'paper_date': '2020-03-06', 'paper_title': 'Centrality Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.03007v1', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '37.4'}, 'model_links': [], 'model_name': 'JB-AAGCN', 'paper_date': '2019-12-15', 'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1912.06971v1', 'uses_additional_data': False}, {'code_links': [{'title': 'Chiaraplizz/ST-TR', 'url': 'https://github.com/Chiaraplizz/ST-TR'}], 'metrics': {'Accuracy': '37.4'}, 'model_links': [], 'model_name': 'ST-TR-agcn', 'paper_date': '2020-08-17', 'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks', 'paper_url': 'https://arxiv.org/abs/2008.07404v4', 'uses_additional_data': False}, {'code_links': [{'title': 'xiaoiker/GCN-NAS', 'url': 'https://github.com/xiaoiker/GCN-NAS'}], 'metrics': {'Accuracy': '37.1'}, 'model_links': [], 'model_name': 'GCN-NAS', 'paper_date': '2019-11-11', 'paper_title': 'Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching', 'paper_url': 'https://arxiv.org/abs/1911.04131v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch', 'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}], 'metrics': {'Accuracy': '36.9'}, 'model_links': [], 'model_name': 'DGNN', 'paper_date': '2019-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '36.6'}, 'model_links': [], 'model_name': 'SLnL-rFA', 'paper_date': '2018-11-10', 'paper_title': 'Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention', 'paper_url': 'https://arxiv.org/abs/1811.04237v3', 'uses_additional_data': False}, {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal', 'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'}, {'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy': '36.1'}, 'model_links': [], 'model_name': '2s-AGCN', 'paper_date': '2018-05-20', 'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1805.07694v3', 'uses_additional_data': False}, {'code_links': [{'title': 'limaosen0/AS-GCN', 'url': 'https://github.com/limaosen0/AS-GCN'}], 'metrics': {'Accuracy': '34.8'}, 'model_links': [], 'model_name': 'AS-GCN', 'paper_date': '2019-04-26', 'paper_title': 'Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1904.12659v1', 'uses_additional_data': False}, {'code_links': [{'title': 'andreYoo/PeGCNs', 'url': 'https://github.com/andreYoo/PeGCNs'}], 'metrics': {'Accuracy': '34.8'}, 'model_links': [], 'model_name': 'PeGCN', 'paper_date': '2020-03-17', 'paper_title': 'Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.07514v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '33.7 '}, 'model_links': [], 'model_name': 'PR-GCN', 'paper_date': '2020-10-14', 'paper_title': 'Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2010.07367v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '33.6'}, 'model_links': [], 'model_name': 'ST-GR', 'paper_date': '2019-07-17', 'paper_title': 'Spatiotemporal graph routing for skeleton-based action recognition', 'paper_url': 'https://doi.org/10.1609/aaai.v33i01.33018561', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '33.5'}, 'model_links': [], 'model_name': 'AR-GCN', 'paper_date': '2019-11-27', 'paper_title': 'An Attention-Enhanced Recurrent Graph Convolutional Network for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1145/3372806.3372814', 'uses_additional_data': False}, {'code_links': [{'title': 'raymondyeh07/chirality_nets', 'url': 'https://github.com/raymondyeh07/chirality_nets'}], 'metrics': {'Accuracy': '30.9'}, 'model_links': [], 'model_name': 'Ours-Conv-Chiral', 'paper_date': '2019-10-31', 'paper_title': 'Chirality Nets for Human Pose Regression', 'paper_url': 'https://arxiv.org/abs/1911.00029v1', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmskeleton', 'url': 'https://github.com/open-mmlab/mmskeleton'}, {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'}, {'title': 'ericksiavichay/cs230-final-project', 'url': 'https://github.com/ericksiavichay/cs230-final-project'}, {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'}, {'title': 'stillarrow/S2VT_ACT', 'url': 'https://github.com/stillarrow/S2VT_ACT'}, {'title': 'ZhangNYG/ST-GCN', 'url': 'https://github.com/ZhangNYG/ST-GCN'}, {'title': 'DixinFan/st-gcn', 'url': 'https://github.com/DixinFan/st-gcn'}, {'title': 'ken724049/action-recognition', 'url': 'https://github.com/ken724049/action-recognition'}, {'title': 'antoniolq/st-gcn', 'url': 'https://github.com/antoniolq/st-gcn'}, {'title': 'github-zbx/ST-GCN', 'url': 'https://github.com/github-zbx/ST-GCN'}, {'title': 'GeyuanZhang/st-gcn-master', 'url': 'https://github.com/GeyuanZhang/st-gcn-master'}, {'title': 'KrisLee512/ST-GCN', 'url': 'https://github.com/KrisLee512/ST-GCN'}, {'title': 'AbiterVX/ST-GCN', 'url': 'https://github.com/AbiterVX/ST-GCN'}], 'metrics': {'Accuracy': '30.7'}, 'model_links': [], 'model_name': 'ST-GCN', 'paper_date': '2018-01-23', 'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1801.07455v2', 'uses_additional_data': False}, {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition', 'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}], 'metrics': {'Accuracy': '20.3'}, 'model_links': [], 'model_name': 'Res-TCN', 'paper_date': '2017-04-14', 'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks', 'paper_url': 'http://arxiv.org/abs/1704.04516v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'J-HMBD Early Action', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-j-hmbd'}], 'description': '', 'sota': {'metrics': ['10%'], 'rows': [{'code_links': [{'title': 'ser-art/RAE-vs-AE', 'url': 'https://github.com/ser-art/RAE-vs-AE'}, {'title': 'rk68657/AutoEncoders', 'url': 'https://github.com/rk68657/AutoEncoders'}], 'metrics': {'10%': '60.6'}, 'model_links': [], 'model_name': 'DR^2N', 'paper_date': '2018-02-09', 'paper_title': 'Relational Autoencoder for Feature Extraction', 'paper_url': 'http://arxiv.org/abs/1802.03145v1', 'uses_additional_data': False}, {'code_links': [{'title': 'labmlai/annotated_deep_learning_paper_implementations', 'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/graphs/gat'}, {'title': 'PetarV-/GAT', 'url': 'https://github.com/PetarV-/GAT'}, {'title': 'Diego999/pyGAT', 'url': 'https://github.com/Diego999/pyGAT'}, {'title': 'gordicaleksa/pytorch-GAT', 'url': 'https://github.com/gordicaleksa/pytorch-GAT'}, {'title': 'shenweichen/GraphNeuralNetwork', 'url': 'https://github.com/shenweichen/GraphNeuralNetwork'}, {'title': 'danielegrattarola/keras-gat', 'url': 'https://github.com/danielegrattarola/keras-gat'}, {'title': 'HazyResearch/hgcn', 'url': 'https://github.com/HazyResearch/hgcn'}, {'title': 'GraphSAINT/GraphSAINT', 'url': 'https://github.com/GraphSAINT/GraphSAINT'}, {'title': 'lukecavabarrett/pna', 'url': 'https://github.com/lukecavabarrett/pna'}, {'title': 'Kaimaoge/IGNNK', 'url': 'https://github.com/Kaimaoge/IGNNK'}, {'title': 'zhao-tong/GNNs-easy-to-use', 'url': 'https://github.com/zhao-tong/GNNs-easy-to-use'}, {'title': 'snowkylin/gnn', 'url': 'https://github.com/snowkylin/gnn'}, {'title': 'marblet/GNN_models_pytorch_geometric', 'url': 'https://github.com/marblet/GNN_models_pytorch_geometric'}, {'title': 'marble0117/GNN_models_pytorch', 'url': 'https://github.com/marble0117/GNN_models_pytorch'}, {'title': 'marble0117/GNN_models_pytorch_geometric', 'url': 'https://github.com/marble0117/GNN_models_pytorch_geometric'}, {'title': 'HeapHop30/graph-attention-nets', 'url': 'https://github.com/HeapHop30/graph-attention-nets'}, {'title': 'weiyangfb/PyTorchSparseGAT', 'url': 'https://github.com/weiyangfb/PyTorchSparseGAT'}, {'title': 'marblet/gat-pytorch', 'url': 'https://github.com/marblet/gat-pytorch'}, {'title': 'ds4dm/sparse-gcn', 'url': 'https://github.com/ds4dm/sparse-gcn'}, {'title': 'ds4dm/sGat', 'url': 'https://github.com/ds4dm/sGat'}, {'title': 'gcucurull/jax-gat', 'url': 'https://github.com/gcucurull/jax-gat'}, {'title': 'calciver/Graph-Attention-Networks', 'url': 'https://github.com/calciver/Graph-Attention-Networks'}, {'title': 'noahtren/Graph-Attention-Networks-TensorFlow-2', 'url': 'https://github.com/noahtren/Graph-Attention-Networks-TensorFlow-2'}, {'title': 'Yindong-Zhang/myGAT', 'url': 'https://github.com/Yindong-Zhang/myGAT'}, {'title': 'liu6zijian/simplified-gcn-model', 'url': 'https://github.com/liu6zijian/simplified-gcn-model'}, {'title': 'taishan1994/pytorch_gat', 'url': 'https://github.com/taishan1994/pytorch_gat'}, {'title': 'BIG-S2/keras-gnm', 'url': 'https://github.com/BIG-S2/keras-gnm'}, {'title': 'Aveek-Saha/Graph-Attention-Net', 'url': 'https://github.com/Aveek-Saha/Graph-Attention-Net'}, {'title': 'zxhhh97/ABot', 'url': 'https://github.com/zxhhh97/ABot'}, {'title': 'qema/orca-py', 'url': 'https://github.com/qema/orca-py'}, {'title': 'mitya8128/experiments_notes', 'url': 'https://github.com/mitya8128/experiments_notes'}, {'title': 'giuseppefutia/link-prediction-code', 'url': 'https://github.com/giuseppefutia/link-prediction-code'}, {'title': 'AngusMonroe/GAT-pytorch', 'url': 'https://github.com/AngusMonroe/GAT-pytorch'}, {'title': 'handasontam/GAT-with-edgewise-attention', 'url': 'https://github.com/handasontam/GAT-with-edgewise-attention'}, {'title': 'fongyk/graph-attention', 'url': 'https://github.com/fongyk/graph-attention'}, {'title': 'mlzxzhou/keras-gnm', 'url': 'https://github.com/mlzxzhou/keras-gnm'}, {'title': 'YunseobShin/wiki_GAT', 'url': 'https://github.com/YunseobShin/wiki_GAT'}, {'title': 'dzb1998/pyGAT', 'url': 'https://github.com/dzb1998/pyGAT'}, {'title': 'Anou9531/GAT', 'url': 'https://github.com/Anou9531/GAT'}, {'title': 'WantingZhao/my_GAT', 'url': 'https://github.com/WantingZhao/my_GAT'}, {'title': 'TyngJiunKuo/deep-learning-project', 'url': 'https://github.com/TyngJiunKuo/deep-learning-project'}, {'title': 'iwzy7071/graph_neural_network', 'url': 'https://github.com/iwzy7071/graph_neural_network'}, {'title': 'PumpkinYing/GAT', 'url': 'https://github.com/PumpkinYing/GAT'}, {'title': 'anish-lu-yihe/SVRT-by-GAT', 'url': 'https://github.com/anish-lu-yihe/SVRT-by-GAT'}, {'title': 'ChengSashankh/trying-GAT', 'url': 'https://github.com/ChengSashankh/trying-GAT'}, {'title': 'subercui/pyGConvAT', 'url': 'https://github.com/subercui/pyGConvAT'}, {'title': 'ChengyuSun/gat', 'url': 'https://github.com/ChengyuSun/gat'}, {'title': 'blueberryc/pyGAT', 'url': 'https://github.com/blueberryc/pyGAT'}, {'title': 'zhangbo2008/GAT_network', 'url': 'https://github.com/zhangbo2008/GAT_network'}, {'title': 'whut2962575697/gat_sementic_segmentation', 'url': 'https://github.com/whut2962575697/gat_sementic_segmentation'}, {'title': 'RJ12138/Multilevel', 'url': 'https://github.com/RJ12138/Multilevel'}, {'title': 'galkampel/HyperNetworks', 'url': 'https://github.com/galkampel/HyperNetworks'}, {'title': 'tlmakinen/GAT-noise', 'url': 'https://github.com/tlmakinen/GAT-noise'}, {'title': 'Anak2016/GAT', 'url': 'https://github.com/Anak2016/GAT'}, {'title': 'snownus/COOP', 'url': 'https://github.com/snownus/COOP'}, {'title': 'ColdenChan/GAT_D', 'url': 'https://github.com/ColdenChan/GAT_D'}], 'metrics': {'10%': '58.1'}, 'model_links': [], 'model_name': 'GAT', 'paper_date': '2017-10-30', 'paper_title': 'Graph Attention Networks', 'paper_url': 'http://arxiv.org/abs/1710.10903v3', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Varying-view RGB-D Action-Skeleton', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-varying'}], 'description': '', 'sota': {'metrics': ['Accuracy (CS)', 'Accuracy (CV I)', 'Accuracy (CV II)', 'Accuracy (AV I)', 'Accuracy (AV II)'], 'rows': [{'code_links': [], 'metrics': {'Accuracy (AV I)': '57%', 'Accuracy (AV II)': '75%', 'Accuracy (CS)': '76%', 'Accuracy (CV I)': '29%', 'Accuracy (CV II)': '71%'}, 'model_links': [], 'model_name': 'VS-CNN', 'paper_date': '2019-04-24', 'paper_title': 'A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1904.10681v1', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmskeleton', 'url': 'https://github.com/open-mmlab/mmskeleton'}, {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'}, {'title': 'ericksiavichay/cs230-final-project', 'url': 'https://github.com/ericksiavichay/cs230-final-project'}, {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'}, {'title': 'stillarrow/S2VT_ACT', 'url': 'https://github.com/stillarrow/S2VT_ACT'}, {'title': 'ZhangNYG/ST-GCN', 'url': 'https://github.com/ZhangNYG/ST-GCN'}, {'title': 'DixinFan/st-gcn', 'url': 'https://github.com/DixinFan/st-gcn'}, {'title': 'ken724049/action-recognition', 'url': 'https://github.com/ken724049/action-recognition'}, {'title': 'antoniolq/st-gcn', 'url': 'https://github.com/antoniolq/st-gcn'}, {'title': 'github-zbx/ST-GCN', 'url': 'https://github.com/github-zbx/ST-GCN'}, {'title': 'GeyuanZhang/st-gcn-master', 'url': 'https://github.com/GeyuanZhang/st-gcn-master'}, {'title': 'KrisLee512/ST-GCN', 'url': 'https://github.com/KrisLee512/ST-GCN'}, {'title': 'AbiterVX/ST-GCN', 'url': 'https://github.com/AbiterVX/ST-GCN'}], 'metrics': {'Accuracy (AV I)': '53%', 'Accuracy (AV II)': '43%', 'Accuracy (CS)': '71%', 'Accuracy (CV I)': '25%', 'Accuracy (CV II)': '56%'}, 'model_links': [], 'model_name': 'ST-GCN', 'paper_date': '2018-01-23', 'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1801.07455v2', 'uses_additional_data': False}, {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition', 'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}], 'metrics': {'Accuracy (AV I)': '48%', 'Accuracy (AV II)': '68%', 'Accuracy (CS)': '63%', 'Accuracy (CV I)': '14%', 'Accuracy (CV II)': '48%'}, 'model_links': [], 'model_name': 'Res-TCN', 'paper_date': '2017-04-14', 'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks', 'paper_url': 'http://arxiv.org/abs/1704.04516v1', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (AV I)': '33%', 'Accuracy (AV II)': '50%', 'Accuracy (CS)': '60%', 'Accuracy (CV I)': '13%', 'Accuracy (CV II)': '33%'}, 'model_links': [], 'model_name': 'P-LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (AV I)': '43%', 'Accuracy (AV II)': '77%', 'Accuracy (CS)': '59%', 'Accuracy (CV I)': '26%', 'Accuracy (CV II)': '68%'}, 'model_links': [], 'model_name': 'SK-CNN', 'paper_date': '2017-08-01', 'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition', 'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030', 'uses_additional_data': False}, {'code_links': [{'title': 'coderSkyChen/Action_Recognition_Zoo', 'url': 'https://github.com/coderSkyChen/Action_Recognition_Zoo'}, {'title': 'colincsl/TemporalConvolutionalNetworks', 'url': 'https://github.com/colincsl/TemporalConvolutionalNetworks'}, {'title': 'yz-cnsdqz/TemporalActionParsing-FineGrained', 'url': 'https://github.com/yz-cnsdqz/TemporalActionParsing-FineGrained'}, {'title': 'sadari1/TumorDetectionDeepLearning', 'url': 'https://github.com/sadari1/TumorDetectionDeepLearning'}, {'title': 'BehnooshParsa/HumanActionRecognition_with_ErgonomicRisk', 'url': 'https://github.com/BehnooshParsa/HumanActionRecognition_with_ErgonomicRisk'}], 'metrics': {'Accuracy (AV I)': '43%', 'Accuracy (AV II)': '64%', 'Accuracy (CS)': '56%', 'Accuracy (CV I)': '16%', 'Accuracy (CV II)': '43%'}, 'model_links': [], 'model_name': 'TCN', 'paper_date': '2016-11-16', 'paper_title': 'Temporal Convolutional Networks for Action Segmentation and Detection', 'paper_url': 'http://arxiv.org/abs/1611.05267v1', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (AV I)': '31%', 'Accuracy (AV II)': '68%', 'Accuracy (CS)': '56%', 'Accuracy (CV I)': '16%', 'Accuracy (CV II)': '31%'}, 'model_links': [], 'model_name': 'LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Florence 3D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-florence'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '99.1%'}, 'model_links': [], 'model_name': 'Deep STGC_K', 'paper_date': '2018-02-27', 'paper_title': 'Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1802.09834v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '98.4%'}, 'model_links': [], 'model_name': 'Complete GR-GCN', 'paper_date': '2018-11-29', 'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression', 'paper_url': 'http://arxiv.org/abs/1811.12013v2', 'uses_additional_data': False}, {'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '95.81%'}, 'model_links': [], 'model_name': 'Temporal Spectral Clustering + Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '91.40%'}, 'model_links': [], 'model_name': 'Rolling Rotations (FTP)', 'paper_date': '2016-06-27', 'paper_title': 'Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data', 'paper_url': 'https://doi.org/10.1109/CVPR.2016.484', 'uses_additional_data': False}, {'code_links': [{'title': 'fdsa1860/skeletal', 'url': 'https://github.com/fdsa1860/skeletal'}], 'metrics': {'Accuracy': '90.9%'}, 'model_links': [], 'model_name': 'Lie Group', 'paper_date': '2014-06-23', 'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group', 'paper_url': 'https://doi.org/10.1109/CVPR.2014.82', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'NTU60-X', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu60-x'}], 'description': '', 'sota': {'metrics': ['Accuracy (Body + Fingers joints)', 'Accuracy (Body joints)', 'Accuracy (Body + Fingers + Face joints)'], 'rows': [{'code_links': [{'title': 'skelemoa/ntu-x', 'url': 'https://github.com/skelemoa/ntu-x'}], 'metrics': {'Accuracy (Body + Fingers + Face joints)': '89.64', 'Accuracy (Body + Fingers joints)': '91.78', 'Accuracy (Body joints)': '89.56'}, 'model_links': [], 'model_name': '4s-ShiftGCN', 'paper_date': '2021-01-27', 'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions', 'paper_url': 'https://arxiv.org/abs/2101.11529v2', 'uses_additional_data': False}, {'code_links': [{'title': 'skelemoa/ntu-x', 'url': 'https://github.com/skelemoa/ntu-x'}], 'metrics': {'Accuracy (Body + Fingers + Face joints)': '91.12', 'Accuracy (Body + Fingers joints)': '91.76', 'Accuracy (Body joints)': '91.26'}, 'model_links': [], 'model_name': 'MS-G3D', 'paper_date': '2021-01-27', 'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions', 'paper_url': 'https://arxiv.org/abs/2101.11529v2', 'uses_additional_data': False}, {'code_links': [{'title': 'skelemoa/ntu-x', 'url': 'https://github.com/skelemoa/ntu-x'}], 'metrics': {'Accuracy (Body + Fingers + Face joints)': '89.79', 'Accuracy (Body + Fingers joints)': '91.64', 'Accuracy (Body joints)': '89.98'}, 'model_links': [], 'model_name': 'PA-ResGCN', 'paper_date': '2021-01-27', 'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions', 'paper_url': 'https://arxiv.org/abs/2101.11529v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'TCG-dataset', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-tcg'}], 'description': '', 'sota': {'metrics': ['Acc'], 'rows': [{'code_links': [{'title': 'againerju/tcg_recognition', 'url': 'https://github.com/againerju/tcg_recognition'}], 'metrics': {'Acc': '87.24'}, 'model_links': [], 'model_name': 'Bidirectional LSTM', 'paper_date': '2020-07-31', 'paper_title': 'Traffic Control Gesture Recognition for Autonomous Vehicles', 'paper_url': 'https://arxiv.org/abs/2007.16072v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'SHREC 2017 track on 3D Hand Gesture Recognition', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-shrec'}], 'description': '', 'sota': {'metrics': ['28 gestures accuracy', 'Accuracy', '14 gestures accuracy', 'No. parameters', 'Speed  (FPS)'], 'rows': [{'code_links': [{'title': 'fandulu/DD-Net', 'url': 'https://github.com/fandulu/DD-Net'}, {'title': 'BlurryLight/DD-Net-Pytorch', 'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}], 'metrics': {'14 gestures accuracy': '94.6', '28 gestures accuracy': '91.9', 'Accuracy': '94.6 (14  gestures) , 91.9 (28 gestures )', 'No. parameters': '1.82M', 'Speed  (FPS)': '2,200'}, 'model_links': [], 'model_name': 'DD-Net', 'paper_date': '2019-07-23', 'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better', 'paper_url': 'https://arxiv.org/abs/1907.09658v8', 'uses_additional_data': False}, {'code_links': [{'title': 'AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition', 'url': 'https://github.com/AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition'}], 'metrics': {'14 gestures accuracy': '93.57', '28 gestures accuracy': '91.43'}, 'model_links': [], 'model_name': 'TCN-Summ', 'paper_date': '2021-03-03', 'paper_title': 'Domain and View-point Agnostic Hand Action Recognition', 'paper_url': 'https://arxiv.org/abs/2103.02303v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'14 gestures accuracy': '93.6', '28 gestures accuracy': '90.7', 'Speed  (FPS)': '161'}, 'model_links': [], 'model_name': 'STA-Res-TCN', 'paper_date': '2019-01-23', 'paper_title': 'Spatial-Temporal Attention Res-TCN for Skeleton-Based Dynamic Hand Gesture Recognition', 'paper_url': 'https://link.springer.com/chapter/10.1007/978-3-030-11024-6_18', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'14 gestures accuracy': '91.3', '28 gestures accuracy': '86.6', 'Speed  (FPS)': '361'}, 'model_links': [], 'model_name': 'MFA-Net', 'paper_date': '2019-01-10', 'paper_title': 'Motion feature augmented network for dynamic hand gesture recognition from skeletal data', 'paper_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6359639/', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'NTU RGB+D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu-rgbd'}], 'description': '', 'sota': {'metrics': ['Accuracy (CV)', 'Accuracy (CS)'], 'rows': [{'code_links': [{'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}], 'metrics': {'Accuracy (CS)': '94.1', 'Accuracy (CV)': '97.1'}, 'model_links': [], 'model_name': 'PoseC3D (w. HRNet 2D skeleton)', 'paper_date': '2021-04-28', 'paper_title': 'Revisiting Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2104.13586v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '91.0', 'Accuracy (CV)': '96.5'}, 'model_links': [], 'model_name': 'MS-AAGCN+TEM', 'paper_date': '2020-03-19', 'paper_title': 'Temporal Extension Module for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.08951v2', 'uses_additional_data': False}, {'code_links': [{'title': 'kchengiva/Shift-GCN', 'url': 'https://github.com/kchengiva/Shift-GCN'}], 'metrics': {'Accuracy (CS)': '90.7', 'Accuracy (CV)': '96.5'}, 'model_links': [], 'model_name': '4s Shift-GCN', 'paper_date': '2020-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '91.7', 'Accuracy (CV)': '96.4'}, 'model_links': [], 'model_name': 'AngNet-JA + BA + JBA + VJBA', 'paper_date': '2021-05-04', 'paper_title': 'Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2105.01563v3', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/DSTA-Net', 'url': 'https://github.com/lshiwjx/DSTA-Net'}], 'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.4'}, 'model_links': [], 'model_name': 'DSTA-Net', 'paper_date': '2020-07-07', 'paper_title': 'Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.03263v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '90.3', 'Accuracy (CV)': '96.4'}, 'model_links': [], 'model_name': 'CGCN', 'paper_date': '2020-03-06', 'paper_title': 'Centrality Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.03007v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '90.1', 'Accuracy (CV)': '96.4'}, 'model_links': [], 'model_name': 'Sym-GNN', 'paper_date': '2019-10-05', 'paper_title': 'Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction', 'paper_url': 'https://arxiv.org/abs/1910.02212v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '90.3', 'Accuracy (CV)': '96.3'}, 'model_links': [], 'model_name': 'BAGCN', 'paper_date': '2019-12-24', 'paper_title': 'Focusing and Diffusion: Bidirectional Attentive Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1912.11521v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '90.2', 'Accuracy (CV)': '96.3'}, 'model_links': [], 'model_name': 'FGCN-spatial+FGCN-motion', 'paper_date': '2020-03-17', 'paper_title': 'Feedback Graph Convolutional Network for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.07564v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/ms-g3d', 'url': 'https://github.com/kenziyuliu/ms-g3d'}], 'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.2'}, 'model_links': [], 'model_name': 'MS-G3D Net', 'paper_date': '2020-03-31', 'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.14111v2', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy (CS)': '90.0', 'Accuracy (CV)': '96.2'}, 'model_links': [], 'model_name': 'MS-AAGCN', 'paper_date': '2019-12-15', 'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1912.06971v1', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch', 'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}], 'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '96.1'}, 'model_links': [], 'model_name': 'DGNN', 'paper_date': '2019-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'Chiaraplizz/ST-TR', 'url': 'https://github.com/Chiaraplizz/ST-TR'}], 'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '96.1'}, 'model_links': [], 'model_name': 'ST-TR', 'paper_date': '2020-08-17', 'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks', 'paper_url': 'https://arxiv.org/abs/2008.07404v4', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.0'}, 'model_links': [], 'model_name': 'Dynamic GCN', 'paper_date': '2020-07-29', 'paper_title': 'Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.14690v1', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/ResGCNv1', 'url': 'https://github.com/yfsong0709/ResGCNv1'}, {'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (CS)': '90.9', 'Accuracy (CV)': '96'}, 'model_links': [], 'model_name': 'PA-ResGCN-B19', 'paper_date': '2020-10-20', 'paper_title': 'Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2010.09978v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '89.7', 'Accuracy (CV)': '96'}, 'model_links': [], 'model_name': 'Mix-Dimension', 'paper_date': '2020-07-30', 'paper_title': 'Mix Dimension in Poincar√© Geometry for 3D Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2007.15678v2', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '96.0'}, 'model_links': [], 'model_name': 'JB-AAGCN', 'paper_date': '2019-12-15', 'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks', 'paper_url': 'https://arxiv.org/abs/1912.06971v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '89.58', 'Accuracy (CV)': '95.74'}, 'model_links': [], 'model_name': '2s-SDGCN', 'paper_date': '2019-10-27', 'paper_title': 'Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_ICCVW_2019/html/SGRL/Wu_Spatial_Residual_Layer_and_Dense_Connection_Block_Enhanced_Spatial_Temporal_ICCVW_2019_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (CS)': '91.7', 'Accuracy (CV)': '95.7'}, 'model_links': [], 'model_name': 'EfficientGCN-B4', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [{'title': 'xiaoiker/GCN-NAS', 'url': 'https://github.com/xiaoiker/GCN-NAS'}], 'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '95.7'}, 'model_links': [], 'model_name': 'GCN-NAS', 'paper_date': '2019-11-11', 'paper_title': 'Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching', 'paper_url': 'https://arxiv.org/abs/1911.04131v1', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (CS)': '90.9', 'Accuracy (CV)': '95.5'}, 'model_links': [], 'model_name': 'EfficientGCN-B2', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal', 'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'}, {'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy (CS)': '88.5', 'Accuracy (CV)': '95.1'}, 'model_links': [], 'model_name': '2s-AGCN', 'paper_date': '2018-05-20', 'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1805.07694v3', 'uses_additional_data': False}, {'code_links': [{'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}], 'metrics': {'Accuracy (CS)': '88.5', 'Accuracy (CV)': '95.1'}, 'model_links': [], 'model_name': '2s-NLGCN', 'paper_date': '2019-07-04', 'paper_title': 'Non-Local Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1805.07694v2', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition', 'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '95.0'}, 'model_links': [], 'model_name': 'VA-fusion (aug.)', 'paper_date': '2018-04-20', 'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1804.07453v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '89.2', 'Accuracy (CV)': '95.0'}, 'model_links': [], 'model_name': 'AGC-LSTM (Joint&Part)', 'paper_date': '2019-02-25', 'paper_title': 'An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1902.09130v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '89.1', 'Accuracy (CV)': '94.9'}, 'model_links': [], 'model_name': 'SLnL-rFA', 'paper_date': '2018-11-10', 'paper_title': 'Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention', 'paper_url': 'https://arxiv.org/abs/1811.04237v3', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/EfficientGCNv1', 'url': 'https://github.com/yfsong0709/EfficientGCNv1'}], 'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '94.7'}, 'model_links': [], 'model_name': 'EfficientGCN-B0', 'paper_date': '2021-06-29', 'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2106.15125v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '87.5', 'Accuracy (CV)': '94.3'}, 'model_links': [], 'model_name': 'GR-GCN', 'paper_date': '2018-11-29', 'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression', 'paper_url': 'http://arxiv.org/abs/1811.12013v2', 'uses_additional_data': False}, {'code_links': [{'title': 'limaosen0/AS-GCN', 'url': 'https://github.com/limaosen0/AS-GCN'}], 'metrics': {'Accuracy (CS)': '86.8', 'Accuracy (CV)': '94.2'}, 'model_links': [], 'model_name': 'AS-GCN', 'paper_date': '2019-04-26', 'paper_title': 'Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1904.12659v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '86.2', 'Accuracy (CV)': '94.2'}, 'model_links': [], 'model_name': 'Sem-GCN', 'paper_date': '2020-05-01', 'paper_title': 'A Semantics-Guided Graph Convolutional Network for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1145/3390557.3394129', 'uses_additional_data': False}, {'code_links': [{'title': 'Sunnydreamrain/IndRNN_pytorch', 'url': 'https://github.com/Sunnydreamrain/IndRNN_pytorch'}], 'metrics': {'Accuracy (CS)': '86.70', 'Accuracy (CV)': '93.97'}, 'model_links': [], 'model_name': 'Dense IndRNN', 'paper_date': '2019-10-11', 'paper_title': 'Deep Independently Recurrent Neural Network (IndRNN)', 'paper_url': 'https://arxiv.org/abs/1910.06251v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '88.6', 'Accuracy (CV)': '93.7'}, 'model_links': [], 'model_name': '3SCNN', 'paper_date': '2019-06-16', 'paper_title': 'Three-Stream Convolutional Neural Network With Multi-Task and Ensemble Learning for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPRW_2019/html/PBVS/Liang_Three-Stream_Convolutional_Neural_Network_With_Multi-Task_and_Ensemble_Learning_for_CVPRW_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '88.0', 'Accuracy (CV)': '93.6'}, 'model_links': [], 'model_name': 'PGCN-TCA', 'paper_date': '2020-01-06', 'paper_title': 'PGCN-TCA: Pseudo Graph Convolutional Network With Temporal and Channel-Wise Attention for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1109/ACCESS.2020.2964115', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/RA-GCNv1', 'url': 'https://github.com/yfsong0709/RA-GCNv1'}, {'title': 'yfsong0709/RA-GCNv2', 'url': 'https://github.com/yfsong0709/RA-GCNv2'}, {'title': 'peter-yys-yoon/pegcnv2', 'url': 'https://github.com/peter-yys-yoon/pegcnv2'}], 'metrics': {'Accuracy (CS)': '87.3', 'Accuracy (CV)': '93.6'}, 'model_links': [], 'model_name': '3s RA-GCN', 'paper_date': '2020-08-09', 'paper_title': 'Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2008.03791v2', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/RA-GCNv1', 'url': 'https://github.com/yfsong0709/RA-GCNv1'}, {'title': 'yfsong0709/RA-GCNv2', 'url': 'https://github.com/yfsong0709/RA-GCNv2'}, {'title': 'peter-yys-yoon/pegcnv2', 'url': 'https://github.com/peter-yys-yoon/pegcnv2'}], 'metrics': {'Accuracy (CS)': '85.9', 'Accuracy (CV)': '93.5'}, 'model_links': [], 'model_name': '3s RA-GCN', 'paper_date': '2019-05-16', 'paper_title': 'Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons', 'paper_url': 'https://arxiv.org/abs/1905.06774v2', 'uses_additional_data': False}, {'code_links': [{'title': 'microsoft/SGN', 'url': 'https://github.com/microsoft/SGN'}], 'metrics': {'Accuracy (CS)': '86.6', 'Accuracy (CV)': '93.4'}, 'model_links': [], 'model_name': 'SGN', 'paper_date': '2019-04-02', 'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1904.01189v3', 'uses_additional_data': False}, {'code_links': [{'title': 'andreYoo/PeGCNs', 'url': 'https://github.com/andreYoo/PeGCNs'}], 'metrics': {'Accuracy (CS)': '85.6', 'Accuracy (CV)': '93.4'}, 'model_links': [], 'model_name': 'PeGCN', 'paper_date': '2020-03-17', 'paper_title': 'Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2003.07514v1', 'uses_additional_data': False}, {'code_links': [{'title': 'memory-attention-networks/MANs', 'url': 'https://github.com/memory-attention-networks/MANs'}], 'metrics': {'Accuracy (CS)': '82.67', 'Accuracy (CV)': '93.22'}, 'model_links': [], 'model_name': 'MANs (DenseNet-161)', 'paper_date': '2018-04-23', 'paper_title': 'Memory Attention Networks for Skeleton-based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1804.08254v2', 'uses_additional_data': False}, {'code_links': [{'title': 'kalpitthakkar/pb-gcn', 'url': 'https://github.com/kalpitthakkar/pb-gcn'}], 'metrics': {'Accuracy (CS)': '87.5', 'Accuracy (CV)': '93.2'}, 'model_links': [], 'model_name': 'PB-GCN', 'paper_date': '2018-09-13', 'paper_title': 'Part-based Graph Convolutional Network for Action Recognition', 'paper_url': 'http://arxiv.org/abs/1809.04983v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '85.1', 'Accuracy (CV)': '93.2'}, 'model_links': [], 'model_name': 'AR-GCN', 'paper_date': '2019-11-27', 'paper_title': 'An Attention-Enhanced Recurrent Graph Convolutional Network for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1145/3372806.3372814', 'uses_additional_data': False}, {'code_links': [{'title': 'yfsong0709/RA-GCNv1', 'url': 'https://github.com/yfsong0709/RA-GCNv1'}, {'title': 'yfsong0709/RA-GCNv2', 'url': 'https://github.com/yfsong0709/RA-GCNv2'}, {'title': 'peter-yys-yoon/pegcnv2', 'url': 'https://github.com/peter-yys-yoon/pegcnv2'}], 'metrics': {'Accuracy (CS)': '85.8', 'Accuracy (CV)': '93.0'}, 'model_links': [], 'model_name': '2s RA-GCN', 'paper_date': '2019-05-16', 'paper_title': 'Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons', 'paper_url': 'https://arxiv.org/abs/1905.06774v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '85.3', 'Accuracy (CV)': '92.8'}, 'model_links': [], 'model_name': 'GVFE+ AS-GCN with DH-TCN', 'paper_date': '2019-12-20', 'paper_title': 'Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition', 'paper_url': 'https://arxiv.org/abs/1912.09745v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '87.2', 'Accuracy (CV)': '92.7'}, 'model_links': [], 'model_name': 'TS-SAN', 'paper_date': '2019-12-18', 'paper_title': 'Self-Attention Network for Skeleton-based Human Action Recognition', 'paper_url': 'https://arxiv.org/abs/1912.08435v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '84.8', 'Accuracy (CV)': '92.4'}, 'model_links': [], 'model_name': 'SR-TSL', 'paper_date': '2018-05-07', 'paper_title': 'Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning', 'paper_url': 'http://arxiv.org/abs/1805.02335v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '85.0', 'Accuracy (CV)': '92.3'}, 'model_links': [], 'model_name': '3scale ResNet152', 'paper_date': '2017-04-19', 'paper_title': 'Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn', 'paper_url': 'http://arxiv.org/abs/1704.05645v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '85.2', 'Accuracy (CV)': '91.7'}, 'model_links': [], 'model_name': 'PR-GCN', 'paper_date': '2020-10-14', 'paper_title': 'Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition', 'paper_url': 'https://arxiv.org/abs/2010.07367v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '86.8', 'Accuracy (CV)': '91.6'}, 'model_links': [], 'model_name': 'RF-Action', 'paper_date': '2019-09-20', 'paper_title': 'Making the Invisible Visible: Action Recognition Through Walls and Occlusions', 'paper_url': 'https://arxiv.org/abs/1909.09300v1', 'uses_additional_data': False}, {'code_links': [{'title': 'huguyuehuhu/HCN-pytorch', 'url': 'https://github.com/huguyuehuhu/HCN-pytorch'}, {'title': 'fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition', 'url': 'https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition'}, {'title': 'maxstrobel/HCN-PrototypeLoss-PyTorch', 'url': 'https://github.com/maxstrobel/HCN-PrototypeLoss-PyTorch'}, {'title': 'hhe-distance/AIF-CNN', 'url': 'https://github.com/hhe-distance/AIF-CNN'}, {'title': 'natepuppy/HCN-pytorch', 'url': 'https://github.com/natepuppy/HCN-pytorch'}], 'metrics': {'Accuracy (CS)': '86.5', 'Accuracy (CV)': '91.1'}, 'model_links': [], 'model_name': 'HCN', 'paper_date': '2018-04-17', 'paper_title': 'Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation', 'paper_url': 'http://arxiv.org/abs/1804.06055v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '82.83', 'Accuracy (CV)': '90.05'}, 'model_links': [], 'model_name': 'FO-GASTM', 'paper_date': '2019-07-08', 'paper_title': 'Learning Shape-Motion Representations from Geometric Algebra Spatio-Temporal Model for Skeleton-Based Action Recognition', 'paper_url': 'https://doi.org/10.1109/ICME.2019.00187', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '83.5', 'Accuracy (CV)': '89.8'}, 'model_links': [], 'model_name': 'DPRL', 'paper_date': '2018-06-01', 'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'magnux/DMNN', 'url': 'https://github.com/magnux/DMNN'}], 'metrics': {'Accuracy (CS)': '82.0', 'Accuracy (CV)': '89.5'}, 'model_links': [], 'model_name': 'DM-3DCNN', 'paper_date': '2017-10-23', 'paper_title': '3D CNNs on Distance Matrices for Human Action Recognition', 'paper_url': 'https://doi.org/10.1145/3123266.3123299', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '83.2', 'Accuracy (CV)': '89.3'}, 'model_links': [], 'model_name': 'CNN+Motion+Trans', 'paper_date': '2017-04-25', 'paper_title': 'Skeleton-based Action Recognition with Convolutional Neural Networks', 'paper_url': 'http://arxiv.org/abs/1704.07595v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '84.23', 'Accuracy (CV)': '89.27'}, 'model_links': [], 'model_name': 'RGB+Skeleton (cross-attention)', 'paper_date': '2020-01-20', 'paper_title': 'Context-Aware Cross-Attention for Skeleton-Based Human Action Recognition', 'paper_url': 'https://doi.org/10.1109/ACCESS.2020.2968054', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '81.8', 'Accuracy (CV)': '89.0'}, 'model_links': [], 'model_name': 'Bayesian GC-LSTM', 'paper_date': '2019-10-01', 'paper_title': 'Bayesian Graph Convolution LSTM for Skeleton Based Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Bayesian_Graph_Convolution_LSTM_for_Skeleton_Based_Action_Recognition_ICCV_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '83.36', 'Accuracy (CV)': '88.84'}, 'model_links': [], 'model_name': 'ST-GCN-jpd', 'paper_date': '2019-06-24', 'paper_title': 'A Comparative Review of Recent Kinect-based Action Recognition Algorithms', 'paper_url': 'https://arxiv.org/abs/1906.09955v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '80.7', 'Accuracy (CV)': '88.8'}, 'model_links': [], 'model_name': 'ARRN-LSTM', 'paper_date': '2018-05-07', 'paper_title': 'Relational Network for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1805.02556v4', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '80.7', 'Accuracy (CV)': '88.4'}, 'model_links': [], 'model_name': 'EleAtt-GRU (aug.)', 'paper_date': '2019-09-03', 'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks', 'paper_url': 'https://arxiv.org/abs/1909.01939v1', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmskeleton', 'url': 'https://github.com/open-mmlab/mmskeleton'}, {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'}, {'title': 'ericksiavichay/cs230-final-project', 'url': 'https://github.com/ericksiavichay/cs230-final-project'}, {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'}, {'title': 'stillarrow/S2VT_ACT', 'url': 'https://github.com/stillarrow/S2VT_ACT'}, {'title': 'ZhangNYG/ST-GCN', 'url': 'https://github.com/ZhangNYG/ST-GCN'}, {'title': 'DixinFan/st-gcn', 'url': 'https://github.com/DixinFan/st-gcn'}, {'title': 'ken724049/action-recognition', 'url': 'https://github.com/ken724049/action-recognition'}, {'title': 'antoniolq/st-gcn', 'url': 'https://github.com/antoniolq/st-gcn'}, {'title': 'github-zbx/ST-GCN', 'url': 'https://github.com/github-zbx/ST-GCN'}, {'title': 'GeyuanZhang/st-gcn-master', 'url': 'https://github.com/GeyuanZhang/st-gcn-master'}, {'title': 'KrisLee512/ST-GCN', 'url': 'https://github.com/KrisLee512/ST-GCN'}, {'title': 'AbiterVX/ST-GCN', 'url': 'https://github.com/AbiterVX/ST-GCN'}], 'metrics': {'Accuracy (CS)': '81.5', 'Accuracy (CV)': '88.3'}, 'model_links': [], 'model_name': 'ST-GCN', 'paper_date': '2018-01-23', 'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1801.07455v2', 'uses_additional_data': False}, {'code_links': [{'title': 'TobiasLee/Text-Classification', 'url': 'https://github.com/TobiasLee/Text-Classification'}, {'title': 'batzner/indrnn', 'url': 'https://github.com/batzner/indrnn'}, {'title': 'lmnt-com/haste', 'url': 'https://github.com/lmnt-com/haste'}, {'title': 'StefOe/indrnn-pytorch', 'url': 'https://github.com/StefOe/indrnn-pytorch'}, {'title': 'Sunnydreamrain/IndRNN_pytorch', 'url': 'https://github.com/Sunnydreamrain/IndRNN_pytorch'}, {'title': 'Sunnydreamrain/IndRNN_Theano_Lasagne', 'url': 'https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne'}, {'title': 'trevor-richardson/rnn_zoo', 'url': 'https://github.com/trevor-richardson/rnn_zoo'}, {'title': 'amcs1729/Predicting-cloud-CPU-usage-on-Azure-data', 'url': 'https://github.com/amcs1729/Predicting-cloud-CPU-usage-on-Azure-data'}, {'title': 'Sunnydreamrain/IndRNN', 'url': 'https://github.com/Sunnydreamrain/IndRNN'}, {'title': 'secretlyvogon/IndRNNTF', 'url': 'https://github.com/secretlyvogon/IndRNNTF'}, {'title': 'secretlyvogon/Neural-Network-Implementations', 'url': 'https://github.com/secretlyvogon/Neural-Network-Implementations'}], 'metrics': {'Accuracy (CS)': '81.8', 'Accuracy (CV)': '88.0'}, 'model_links': [], 'model_name': 'Ind-RNN', 'paper_date': '2018-03-13', 'paper_title': 'Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN', 'paper_url': 'http://arxiv.org/abs/1803.04831v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '79.4', 'Accuracy (CV)': '87.6'}, 'model_links': [], 'model_name': 'VA-LSTM', 'paper_date': '2017-03-24', 'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data', 'paper_url': 'http://arxiv.org/abs/1703.08274v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '80', 'Accuracy (CV)': '87.2'}, 'model_links': [], 'model_name': 'Synthesized CNN', 'paper_date': '2017-08-01', 'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition', 'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '79.8', 'Accuracy (CV)': '87.1'}, 'model_links': [], 'model_name': 'EleAtt-GRU', 'paper_date': '2018-07-12', 'paper_title': 'Adding Attentiveness to the Neurons in Recurrent Neural Networks', 'paper_url': 'http://arxiv.org/abs/1807.04445v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '80.9', 'Accuracy (CV)': '86.1'}, 'model_links': [], 'model_name': 'HPM_RGB+HPM_3D+Traj', 'paper_date': '2017-07-04', 'paper_title': 'Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition', 'paper_url': 'http://arxiv.org/abs/1707.00823v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '79.6', 'Accuracy (CV)': '84.8'}, 'model_links': [], 'model_name': 'Clips+CNN+MTLN', 'paper_date': '2017-03-09', 'paper_title': 'A New Representation of Skeleton Sequences for 3D Action Recognition', 'paper_url': 'http://arxiv.org/abs/1703.03492v3', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (CS)': '76.5', 'Accuracy (CV)': '84.7'}, 'model_links': [], 'model_name': 'Skelemotion + Yang et al.', 'paper_date': '2019-07-30', 'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition', 'paper_url': 'https://arxiv.org/abs/1907.13025v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '79.6', 'Accuracy (CV)': '84.6'}, 'model_links': [], 'model_name': 'F2CSkeleton', 'paper_date': '2018-05-30', 'paper_title': 'A Fine-to-Coarse Convolutional Neural Network for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1805.11790v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '76.10', 'Accuracy (CV)': '84.00'}, 'model_links': [], 'model_name': 'GCA-LSTM', 'paper_date': '2017-07-01', 'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '74.6', 'Accuracy (CV)': '83.2'}, 'model_links': [], 'model_name': 'URNN-2L-T', 'paper_date': '2017-10-22', 'paper_title': 'Adaptive RNN Tree for Large-Scale Human Action Recognition', 'paper_url': 'https://ieeexplore.ieee.org/document/8237423', 'uses_additional_data': False}, {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition', 'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}], 'metrics': {'Accuracy (CS)': '74.3', 'Accuracy (CV)': '83.1'}, 'model_links': [], 'model_name': 'TCN', 'paper_date': '2017-04-14', 'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks', 'paper_url': 'http://arxiv.org/abs/1704.04516v1', 'uses_additional_data': False}, {'code_links': [{'title': 'dzwallkilled/IEforAR', 'url': 'https://github.com/dzwallkilled/IEforAR'}], 'metrics': {'Accuracy (CV)': '82.31'}, 'model_links': [], 'model_name': 'Five Spatial Skeleton Features', 'paper_date': '2017-05-02', 'paper_title': 'Investigation of Different Skeleton Features for CNN-based 3D Action Recognition', 'paper_url': 'http://arxiv.org/abs/1705.00835v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '74.60', 'Accuracy (CV)': '81.25'}, 'model_links': [], 'model_name': 'Ensemble TS-LSTM v2', 'paper_date': '2017-10-01', 'paper_title': 'Ensemble Deep Learning for Skeleton-Based Action Recognition Using Temporal Sliding LSTM Networks', 'paper_url': 'http://openaccess.thecvf.com/content_iccv_2017/html/Lee_Ensemble_Deep_Learning_ICCV_2017_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '75.9', 'Accuracy (CV)': '81.2'}, 'model_links': [], 'model_name': 'SkeletonNet', 'paper_date': '2017-03-31', 'paper_title': 'Skeletonnet: Mining deep part features for 3-d action recognition', 'paper_url': 'https://doi.org/10.1109/LSP.2017.2690339', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '73.4', 'Accuracy (CV)': '81.2'}, 'model_links': [], 'model_name': 'STA-LSTM', 'paper_date': '2016-11-18', 'paper_title': 'An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data', 'paper_url': 'http://arxiv.org/abs/1611.06067v1', 'uses_additional_data': False}, {'code_links': [{'title': 'carloscaetano/skeleton-images', 'url': 'https://github.com/carloscaetano/skeleton-images'}], 'metrics': {'Accuracy (CS)': '73.3', 'Accuracy (CV)': '80.3'}, 'model_links': [], 'model_name': 'TSRJI (Late Fusion)', 'paper_date': '2019-09-11', 'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints', 'paper_url': 'https://arxiv.org/abs/1909.05704v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '71.3', 'Accuracy (CV)': '79.5'}, 'model_links': [], 'model_name': 'Two-Stream RNN', 'paper_date': '2017-04-09', 'paper_title': 'Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks', 'paper_url': 'http://arxiv.org/abs/1704.02581v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '69.2', 'Accuracy (CV)': '77.7'}, 'model_links': [], 'model_name': 'Trust Gate ST-LSTM', 'paper_date': '2016-07-24', 'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1607.07043v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '61.70', 'Accuracy (CV)': '75.50'}, 'model_links': [], 'model_name': 'ST-LSTM', 'paper_date': '2016-07-24', 'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', 'paper_url': 'http://arxiv.org/abs/1607.07043v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '66.8', 'Accuracy (CV)': '72.6'}, 'model_links': [], 'model_name': 'Two-Stream 3DCNN', 'paper_date': '2017-05-23', 'paper_title': 'Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1705.08106v2', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (CS)': '62.93', 'Accuracy (CV)': '70.27'}, 'model_links': [], 'model_name': 'Part-aware LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy (CS)': '60.7', 'Accuracy (CV)': '67.3'}, 'model_links': [], 'model_name': 'Deep LSTM', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '60.2', 'Accuracy (CV)': '65.2'}, 'model_links': [], 'model_name': 'Dynamic Skeletons', 'paper_date': '2016-12-15', 'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition', 'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '59.1', 'Accuracy (CV)': '64.0'}, 'model_links': [], 'model_name': 'H-RNN', 'paper_date': '2015-06-07', 'paper_title': 'Hierarchical recurrent neural network for skeleton based action recognition', 'paper_url': 'https://doi.org/10.1109/CVPR.2015.7298714', 'uses_additional_data': False}, {'code_links': [{'title': 'fdsa1860/skeletal', 'url': 'https://github.com/fdsa1860/skeletal'}], 'metrics': {'Accuracy (CS)': '50.1', 'Accuracy (CV)': '52.8'}, 'model_links': [], 'model_name': 'Lie Group', 'paper_date': '2014-06-23', 'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group', 'paper_url': 'https://doi.org/10.1109/CVPR.2014.82', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (CS)': '38.6', 'Accuracy (CV)': '41.4'}, 'model_links': [], 'model_name': 'Skeleton Quads', 'paper_date': '2014-08-24', 'paper_title': 'Skeletal quads: Human action recognition using joint quadruples', 'paper_url': 'https://doi.org/10.1109/ICPR.2014.772', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'JHMDB (2D poses only)', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-jhmdb-2d'}], 'description': '', 'sota': {'metrics': ['Average accuracy of 3 splits', 'Accuracy', 'No. parameters'], 'rows': [{'code_links': [{'title': 'fandulu/DD-Net', 'url': 'https://github.com/fandulu/DD-Net'}, {'title': 'BlurryLight/DD-Net-Pytorch', 'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}], 'metrics': {'Accuracy': '78.0 (average of 3 split train/test)', 'Average accuracy of 3 splits': '77.2', 'No. parameters': '1.82 M'}, 'model_links': [], 'model_name': 'DD-Net', 'paper_date': '2019-07-23', 'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better', 'paper_url': 'https://arxiv.org/abs/1907.09658v8', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Average accuracy of 3 splits': '67.9', 'No. parameters': '-'}, 'model_links': [], 'model_name': 'PoTion', 'paper_date': '2018-06-01', 'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'noboevbo/ehpi_action_recognition', 'url': 'https://github.com/noboevbo/ehpi_action_recognition'}], 'metrics': {'Average accuracy of 3 splits': '65.5'}, 'model_links': [], 'model_name': 'EHPI', 'paper_date': '2019-04-19', 'paper_title': 'Simple yet efficient real-time pose-based action recognition', 'paper_url': 'http://arxiv.org/abs/1904.09140v1', 'uses_additional_data': False}, {'code_links': [{'title': 'mzolfaghari/chained-multistream-networks', 'url': 'https://github.com/mzolfaghari/chained-multistream-networks'}], 'metrics': {'Average accuracy of 3 splits': '56.8'}, 'model_links': [], 'model_name': 'Chained', 'paper_date': '2017-04-03', 'paper_title': 'Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection', 'paper_url': 'http://arxiv.org/abs/1704.00616v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'PKU-MMD', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-pku-mmd'}], 'description': '', 'sota': {'metrics': ['mAP@0.50 (CV)', 'mAP@0.50 (CS)'], 'rows': [{'code_links': [], 'metrics': {'mAP@0.50 (CS)': '92.9', 'mAP@0.50 (CV)': '94.4'}, 'model_links': [], 'model_name': 'RF-Action', 'paper_date': '2019-09-20', 'paper_title': 'Making the Invisible Visible: Action Recognition Through Walls and Occlusions', 'paper_url': 'https://arxiv.org/abs/1909.09300v1', 'uses_additional_data': False}, {'code_links': [{'title': 'huguyuehuhu/HCN-pytorch', 'url': 'https://github.com/huguyuehuhu/HCN-pytorch'}, {'title': 'fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition', 'url': 'https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition'}, {'title': 'maxstrobel/HCN-PrototypeLoss-PyTorch', 'url': 'https://github.com/maxstrobel/HCN-PrototypeLoss-PyTorch'}, {'title': 'hhe-distance/AIF-CNN', 'url': 'https://github.com/hhe-distance/AIF-CNN'}, {'title': 'natepuppy/HCN-pytorch', 'url': 'https://github.com/natepuppy/HCN-pytorch'}], 'metrics': {'mAP@0.50 (CS)': '92.6', 'mAP@0.50 (CV)': '94.2'}, 'model_links': [], 'model_name': 'HCN', 'paper_date': '2018-04-17', 'paper_title': 'Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation', 'paper_url': 'http://arxiv.org/abs/1804.06055v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'mAP@0.50 (CS)': '90.4', 'mAP@0.50 (CV)': '93.7'}, 'model_links': [], 'model_name': 'Li et al. [[Li et al.2017b]]', 'paper_date': '2017-04-25', 'paper_title': 'Skeleton-based Action Recognition with Convolutional Neural Networks', 'paper_url': 'http://arxiv.org/abs/1704.07595v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Skeletics-152', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on'}], 'description': '', 'sota': {'metrics': ['Accuracy (Cross-Subject)'], 'rows': [{'code_links': [{'title': 'skelemoa/quovadis', 'url': 'https://github.com/skelemoa/quovadis'}], 'metrics': {'Accuracy (Cross-Subject)': '57.01 %'}, 'model_links': [], 'model_name': '4s-ShiftGCN', 'paper_date': '2020-07-04', 'paper_title': 'Quo Vadis, Skeleton Action Recognition ?', 'paper_url': 'https://arxiv.org/abs/2007.02072v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'Skeleton-Mimetics', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-skeleton'}], 'description': '', 'sota': {'metrics': ['Accuracy (%)'], 'rows': [{'code_links': [{'title': 'skelemoa/quovadis', 'url': 'https://github.com/skelemoa/quovadis'}], 'metrics': {'Accuracy (%)': '57.37 %'}, 'model_links': [], 'model_name': 'MS-G3D', 'paper_date': '2020-07-04', 'paper_title': 'Quo Vadis, Skeleton Action Recognition ?', 'paper_url': 'https://arxiv.org/abs/2007.02072v2', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'MSR Action3D', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msr'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '88.51%'}, 'model_links': [], 'model_name': 'Temporal K-Means Clustering + Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}, {'code_links': [{'title': 'rort1989/HDM', 'url': 'https://github.com/rort1989/HDM'}], 'metrics': {'Accuracy': '86.1'}, 'model_links': [], 'model_name': 'HDM-BG', 'paper_date': '2019-06-01', 'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '74'}, 'model_links': [], 'model_name': 'GFT', 'paper_date': '2019-08-26', 'paper_title': 'Graph Based Skeleton Modeling for Human Activity Analysis', 'paper_url': 'https://doi.org/10.1109/ICIP.2019.8803186', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'MSR ActionPairs', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msr-1'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition', 'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}], 'metrics': {'Accuracy': '98.02%'}, 'model_links': [], 'model_name': 'Temporal Subspace Clustering', 'paper_date': '2020-06-21', 'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning', 'paper_url': 'https://arxiv.org/abs/2006.11812v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'CAD-120', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-cad-120'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [], 'metrics': {'Accuracy': '91.1%'}, 'model_links': [], 'model_name': 'NGM (5-shot)', 'paper_date': '2018-09-01', 'paper_title': 'Neural Graph Matching Networks for Fewshot 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '89.3%'}, 'model_links': [], 'model_name': 'All Features (w ground truth)', 'paper_date': '2013-02-01', 'paper_title': 'Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation', 'paper_url': 'http://proceedings.mlr.press/v28/koppula13.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '86.0%'}, 'model_links': [], 'model_name': 'KGS', 'paper_date': '2012-10-04', 'paper_title': 'Learning Human Activities and Object Affordances from RGB-D Videos', 'paper_url': 'http://arxiv.org/abs/1210.1207v2', 'uses_additional_data': False}, {'code_links': [{'title': 'asheshjain399/RNNexp', 'url': 'https://github.com/asheshjain399/RNNexp'}, {'title': 'zhaolongkzz/human_motion', 'url': 'https://github.com/zhaolongkzz/human_motion'}], 'metrics': {'Accuracy': '85.4%'}, 'model_links': [], 'model_name': 'S-RNN (5-shot)', 'paper_date': '2015-11-17', 'paper_title': 'Structural-RNN: Deep Learning on Spatio-Temporal Graphs', 'paper_url': 'http://arxiv.org/abs/1511.05298v3', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '85.0%'}, 'model_links': [], 'model_name': 'NGM w/o Edges  (5-shot)', 'paper_date': '2018-09-01', 'paper_title': 'Neural Graph Matching Networks for Fewshot 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '70.3%'}, 'model_links': [], 'model_name': 'Our DP seg. + moves + heuristic seg.', 'paper_date': '2013-02-01', 'paper_title': 'Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation', 'paper_url': 'http://proceedings.mlr.press/v28/koppula13.html', 'uses_additional_data': False}, {'code_links': [{'title': 'charlesq34/pointnet', 'url': 'https://github.com/charlesq34/pointnet'}, {'title': 'fxia22/pointnet.pytorch', 'url': 'https://github.com/fxia22/pointnet.pytorch'}, {'title': 'vinits5/learning3d', 'url': 'https://github.com/vinits5/learning3d'}, {'title': 'maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification', 'url': 'https://github.com/maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification'}, {'title': 'DylanWusee/pointconv_pytorch', 'url': 'https://github.com/DylanWusee/pointconv_pytorch'}, {'title': 'nikitakaraevv/pointnet', 'url': 'https://github.com/nikitakaraevv/pointnet'}, {'title': 'princeton-vl/SimpleView', 'url': 'https://github.com/princeton-vl/SimpleView'}, {'title': 'ZhihaoZhu/PointNet-Implementation-Tensorflow', 'url': 'https://github.com/ZhihaoZhu/PointNet-Implementation-Tensorflow'}, {'title': 'sarthakTUM/roofn3d', 'url': 'https://github.com/sarthakTUM/roofn3d'}, {'title': 'ajhamdi/AdvPC', 'url': 'https://github.com/ajhamdi/AdvPC'}, {'title': 'romaintha/pytorch_pointnet', 'url': 'https://github.com/romaintha/pytorch_pointnet'}, {'title': 'AI-Guru/pointcloud_experiments', 'url': 'https://github.com/AI-Guru/pointcloud_experiments'}, {'title': 'YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti', 'url': 'https://github.com/YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti'}, {'title': 'YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024', 'url': 'https://github.com/YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024'}, {'title': 'saaries/PointNet', 'url': 'https://github.com/saaries/PointNet'}, {'title': 'y2kmz/pointnetv2', 'url': 'https://github.com/y2kmz/pointnetv2'}, {'title': 'Fragjacker/Pointcloud-grad-CAM', 'url': 'https://github.com/Fragjacker/Pointcloud-grad-CAM'}, {'title': 'abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds', 'url': 'https://github.com/abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds'}, {'title': 'donshen/pointnet.phasedetection', 'url': 'https://github.com/donshen/pointnet.phasedetection'}, {'title': 'Yuto0107/pointnet', 'url': 'https://github.com/Yuto0107/pointnet'}, {'title': 'opeco17/pointnet', 'url': 'https://github.com/opeco17/pointnet'}, {'title': 'YanWei123/Pointnet_encoder_Foldingnet_decoder_quantization', 'url': 'https://github.com/YanWei123/Pointnet_encoder_Foldingnet_decoder_quantization'}, {'title': 'yanxp/PointNet', 'url': 'https://github.com/yanxp/PointNet'}, {'title': 'amyllykoski/CycleGAN', 'url': 'https://github.com/amyllykoski/CycleGAN'}, {'title': 'ftdlyc/pointnet_pytorch', 'url': 'https://github.com/ftdlyc/pointnet_pytorch'}, {'title': 'alpemek/ais3d', 'url': 'https://github.com/alpemek/ais3d'}, {'title': 'Dir-b/PointNet', 'url': 'https://github.com/Dir-b/PointNet'}, {'title': 'PaParaZz1/PointNet', 'url': 'https://github.com/PaParaZz1/PointNet'}, {'title': 'witignite/Frustum-PointNet', 'url': 'https://github.com/witignite/Frustum-PointNet'}, {'title': 'sanantoniochili/PointCloud_KNN', 'url': 'https://github.com/sanantoniochili/PointCloud_KNN'}, {'title': 'zgx0534/pointnet_win', 'url': 'https://github.com/zgx0534/pointnet_win'}, {'title': 'KhusDM/PointNetTree', 'url': 'https://github.com/KhusDM/PointNetTree'}, {'title': 'timothylimyl/PointNet-Pytorch', 'url': 'https://github.com/timothylimyl/PointNet-Pytorch'}, {'title': 'minhncedutw/pointnet1_keras', 'url': 'https://github.com/minhncedutw/pointnet1_keras'}, {'title': 'kenakai16/pointconv_pytorch', 'url': 'https://github.com/kenakai16/pointconv_pytorch'}, {'title': 'xurui1217/pointnet.pytorch-master', 'url': 'https://github.com/xurui1217/pointnet.pytorch-master'}, {'title': 'GOD-GOD-Autonomous-Vehicle/self-pointnet', 'url': 'https://github.com/GOD-GOD-Autonomous-Vehicle/self-pointnet'}, {'title': 'Young98CN/pointconv_pytorch', 'url': 'https://github.com/Young98CN/pointconv_pytorch'}, {'title': 'm-117/PointNet-ein-Implementationsbeispiel-mit-Jupyter-Notebooks', 'url': 'https://github.com/m-117/PointNet-ein-Implementationsbeispiel-mit-Jupyter-Notebooks'}, {'title': 'hnVfly/pointnet.mxnet', 'url': 'https://github.com/hnVfly/pointnet.mxnet'}, {'title': 'LebronGG/PointNet', 'url': 'https://github.com/LebronGG/PointNet'}, {'title': 'ytng001/sensemaking', 'url': 'https://github.com/ytng001/sensemaking'}, {'title': 'lingzhang1/pointnet_tensorflow', 'url': 'https://github.com/lingzhang1/pointnet_tensorflow'}, {'title': 'coconutzs/PointNet_zs', 'url': 'https://github.com/coconutzs/PointNet_zs'}, {'title': 'Fnjn/UCSD-CSE-291I', 'url': 'https://github.com/Fnjn/UCSD-CSE-291I'}, {'title': 'aviros/roatationPointnet', 'url': 'https://github.com/aviros/roatationPointnet'}, {'title': 'ShadowShadowWong/update-pointnet-work', 'url': 'https://github.com/ShadowShadowWong/update-pointnet-work'}, {'title': 'Yang2446/pointnet', 'url': 'https://github.com/Yang2446/pointnet'}, {'title': 'dwtstore/sfm1', 'url': 'https://github.com/dwtstore/sfm1'}, {'title': 'ModelBunker/PointNet-TensorFlow', 'url': 'https://github.com/ModelBunker/PointNet-TensorFlow'}, {'title': 'Lw510107/pointnet-2018.6.27-', 'url': 'https://github.com/Lw510107/pointnet-2018.6.27-'}, {'title': 'freddieee/pn_6d_single', 'url': 'https://github.com/freddieee/pn_6d_single'}, {'title': 'mengxingshifen1218/pointnet.pytorch', 'url': 'https://github.com/mengxingshifen1218/pointnet.pytorch'}, {'title': 'LONG-9621/Extract_Point_3D', 'url': 'https://github.com/LONG-9621/Extract_Point_3D'}, {'title': 'lingzhang1/pointnet_pytorch', 'url': 'https://github.com/lingzhang1/pointnet_pytorch'}, {'title': 'THHHomas/mls', 'url': 'https://github.com/THHHomas/mls'}, {'title': 'ahmed-anas/thesis-pointnet', 'url': 'https://github.com/ahmed-anas/thesis-pointnet'}, {'title': 'liuch37/pointnet', 'url': 'https://github.com/liuch37/pointnet'}, {'title': 'CheesyB/cpointnet', 'url': 'https://github.com/CheesyB/cpointnet'}, {'title': 'aviros/pointnet_totations', 'url': 'https://github.com/aviros/pointnet_totations'}, {'title': 'Taeuk-Jang/pointcompletion', 'url': 'https://github.com/Taeuk-Jang/pointcompletion'}, {'title': 'bt77/pointnet', 'url': 'https://github.com/bt77/pointnet'}, {'title': 'yanx27/Pointnet', 'url': 'https://github.com/yanx27/Pointnet'}, {'title': 'monacv/pointnet', 'url': 'https://github.com/monacv/pointnet'}, {'title': 'merazlab/3D_Deep_Learning_Link', 'url': 'https://github.com/merazlab/3D_Deep_Learning_Link'}, {'title': 'ajertec/PointNetKeras', 'url': 'https://github.com/ajertec/PointNetKeras'}, {'title': 'AlfredoZermini/PointNet', 'url': 'https://github.com/AlfredoZermini/PointNet'}, {'title': 'YiruS/pointnet_adversarial', 'url': 'https://github.com/YiruS/pointnet_adversarial'}, {'title': 'wonderland-dsg/pointnet-grid', 'url': 'https://github.com/wonderland-dsg/pointnet-grid'}, {'title': 'KiranAkadas/My_Pointnet_v2', 'url': 'https://github.com/KiranAkadas/My_Pointnet_v2'}, {'title': 'Q-Qgao/pointnet', 'url': 'https://github.com/Q-Qgao/pointnet'}, {'title': 'LONG-9621/PointNet', 'url': 'https://github.com/LONG-9621/PointNet'}, {'title': 'wuryantoAji/POINTNET', 'url': 'https://github.com/wuryantoAji/POINTNET'}, {'title': 'WLK12580/12', 'url': 'https://github.com/WLK12580/12'}, {'title': 'SBPL-Cruz/perch_pose_sampler', 'url': 'https://github.com/SBPL-Cruz/perch_pose_sampler'}, {'title': 'Harut0726/my-pointnet-tensorflow', 'url': 'https://github.com/Harut0726/my-pointnet-tensorflow'}, {'title': 'BPMJG/annotated_pointnet', 'url': 'https://github.com/BPMJG/annotated_pointnet'}, {'title': 'SonuDileep/3-D-Object-Detection-using-PointNet', 'url': 'https://github.com/SonuDileep/3-D-Object-Detection-using-PointNet'}, {'title': 'prasadsawant5/PointNet', 'url': 'https://github.com/prasadsawant5/PointNet'}, {'title': 'zhijie-yang/pointnet.pytorch', 'url': 'https://github.com/zhijie-yang/pointnet.pytorch'}, {'title': 'KaidongLi/tf-3d-alpha', 'url': 'https://github.com/KaidongLi/tf-3d-alpha'}, {'title': 'hz-ants/pointnet.pytorch', 'url': 'https://github.com/hz-ants/pointnet.pytorch'}, {'title': 'AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation', 'url': 'https://github.com/AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation'}, {'title': 'VaanHUANG/CSCI5210HW1', 'url': 'https://github.com/VaanHUANG/CSCI5210HW1'}], 'metrics': {'Accuracy': '69.1%'}, 'model_links': [], 'model_name': 'PointNet (5-shot)', 'paper_date': '2016-12-02', 'paper_title': 'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation', 'paper_url': 'http://arxiv.org/abs/1612.00593v2', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}], 'metrics': {'Accuracy': '68.1%'}, 'model_links': [], 'model_name': 'P-LSTM (5-shot)', 'paper_date': '2016-04-11', 'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis', 'paper_url': 'http://arxiv.org/abs/1604.02808v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'First-Person Hand Action Benchmark', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-first'}], 'description': '', 'sota': {'metrics': ['1:3 Accuracy', '1:1 Accuracy', '3:1 Accuracy', 'Cross-person Accuracy'], 'rows': [{'code_links': [{'title': 'AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition', 'url': 'https://github.com/AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition'}], 'metrics': {'1:1 Accuracy': '95.93', '1:3 Accuracy': '92.9', '3:1 Accuracy': '96.76', 'Cross-person Accuracy': '88.70'}, 'model_links': [], 'model_name': 'TCN-Summ', 'paper_date': '2021-03-03', 'paper_title': 'Domain and View-point Agnostic Hand Action Recognition', 'paper_url': 'https://arxiv.org/abs/2103.02303v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UAV-Human', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-uav'}], 'description': '', 'sota': {'metrics': ['Average Accuracy'], 'rows': [{'code_links': [{'title': 'kchengiva/Shift-GCN', 'url': 'https://github.com/kchengiva/Shift-GCN'}], 'metrics': {'Average Accuracy': '37.98'}, 'model_links': [], 'model_name': 'Shift-GCN', 'paper_date': '2020-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Average Accuracy': '36.97'}, 'model_links': [], 'model_name': 'HARD-Net', 'paper_date': None, 'paper_title': 'HARD-Net: Hardness-AwaRe Discrimination Network for 3D Early Activity Prediction', 'paper_url': 'https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1360_ECCV_2020_paper.php', 'uses_additional_data': False}, {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal', 'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'}, {'title': 'lshiwjx/2s-AGCN', 'url': 'https://github.com/lshiwjx/2s-AGCN'}, {'title': 'iamjeff7/j-va-aagcn', 'url': 'https://github.com/iamjeff7/j-va-aagcn'}], 'metrics': {'Average Accuracy': '34.84'}, 'model_links': [], 'model_name': '2S-AGCN', 'paper_date': '2018-05-20', 'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'https://arxiv.org/abs/1805.07694v3', 'uses_additional_data': False}, {'code_links': [{'title': 'open-mmlab/mmskeleton', 'url': 'https://github.com/open-mmlab/mmskeleton'}, {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'}, {'title': 'ericksiavichay/cs230-final-project', 'url': 'https://github.com/ericksiavichay/cs230-final-project'}, {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'}, {'title': 'stillarrow/S2VT_ACT', 'url': 'https://github.com/stillarrow/S2VT_ACT'}, {'title': 'ZhangNYG/ST-GCN', 'url': 'https://github.com/ZhangNYG/ST-GCN'}, {'title': 'DixinFan/st-gcn', 'url': 'https://github.com/DixinFan/st-gcn'}, {'title': 'ken724049/action-recognition', 'url': 'https://github.com/ken724049/action-recognition'}, {'title': 'antoniolq/st-gcn', 'url': 'https://github.com/antoniolq/st-gcn'}, {'title': 'github-zbx/ST-GCN', 'url': 'https://github.com/github-zbx/ST-GCN'}, {'title': 'GeyuanZhang/st-gcn-master', 'url': 'https://github.com/GeyuanZhang/st-gcn-master'}, {'title': 'KrisLee512/ST-GCN', 'url': 'https://github.com/KrisLee512/ST-GCN'}, {'title': 'AbiterVX/ST-GCN', 'url': 'https://github.com/AbiterVX/ST-GCN'}], 'metrics': {'Average Accuracy': '30.25'}, 'model_links': [], 'model_name': 'ST-GCN', 'paper_date': '2018-01-23', 'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition', 'paper_url': 'http://arxiv.org/abs/1801.07455v2', 'uses_additional_data': False}, {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch', 'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}], 'metrics': {'Average Accuracy': '29.90'}, 'model_links': [], 'model_name': 'DGNN', 'paper_date': '2019-06-01', 'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'J-HMDB', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-j-hmdb'}], 'description': '', 'sota': {'metrics': ['Accuracy (RGB+pose)', 'Accuracy (pose)'], 'rows': [{'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '90.4', 'Accuracy (pose)': '67.9'}, 'model_links': [], 'model_name': 'Potion', 'paper_date': '2018-06-01', 'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '86.1'}, 'model_links': [], 'model_name': 'PA3D+RPAN', 'paper_date': '2019-06-01', 'paper_title': 'PA3D: Pose-Action 3D Machine for Video Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '85.5'}, 'model_links': [], 'model_name': 'I3D + Potion', 'paper_date': '2018-06-01', 'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html', 'uses_additional_data': False}, {'code_links': [{'title': 'deepmind/kinetics-i3d', 'url': 'https://github.com/deepmind/kinetics-i3d'}, {'title': 'open-mmlab/mmaction2', 'url': 'https://github.com/open-mmlab/mmaction2'}, {'title': 'piergiaj/pytorch-i3d', 'url': 'https://github.com/piergiaj/pytorch-i3d'}, {'title': 'hassony2/kinetics_i3d_pytorch', 'url': 'https://github.com/hassony2/kinetics_i3d_pytorch'}, {'title': 'yaohungt/GSTEG_CVPR_2019', 'url': 'https://github.com/yaohungt/GSTEG_CVPR_2019'}, {'title': 'dlpbc/keras-kinetics-i3d', 'url': 'https://github.com/dlpbc/keras-kinetics-i3d'}, {'title': 'StanfordVL/RubiksNet', 'url': 'https://github.com/StanfordVL/RubiksNet'}, {'title': 'FrederikSchorr/sign-language', 'url': 'https://github.com/FrederikSchorr/sign-language'}, {'title': 'CMU-CREATE-Lab/deep-smoke-machine', 'url': 'https://github.com/CMU-CREATE-Lab/deep-smoke-machine'}, {'title': 'JeffCHEN2017/WSSTG', 'url': 'https://github.com/JeffCHEN2017/WSSTG'}, {'title': 'OanaIgnat/i3d_keras', 'url': 'https://github.com/OanaIgnat/i3d_keras'}, {'title': 'ahsaniqbal/Kinetics-FeatureExtractor', 'url': 'https://github.com/ahsaniqbal/Kinetics-FeatureExtractor'}, {'title': 'prinshul/GWSDR', 'url': 'https://github.com/prinshul/GWSDR'}, {'title': 'PPPrior/i3d-pytorch', 'url': 'https://github.com/PPPrior/i3d-pytorch'}, {'title': 'sebastiantiesmeyer/deeplabchop3d', 'url': 'https://github.com/sebastiantiesmeyer/deeplabchop3d'}, {'title': 'anonymous-p/Flickering_Adversarial_Video', 'url': 'https://github.com/anonymous-p/Flickering_Adversarial_Video'}, {'title': 'vijayvee/behavior-recognition', 'url': 'https://github.com/vijayvee/behavior-recognition'}, {'title': 'LukasHedegaard/co3d', 'url': 'https://github.com/LukasHedegaard/co3d'}, {'title': 'vijayvee/behavior_recognition', 'url': 'https://github.com/vijayvee/behavior_recognition'}, {'title': 'AbdurrahmanNadi/activity_recognition_web_service', 'url': 'https://github.com/AbdurrahmanNadi/activity_recognition_web_service'}, {'title': 'hjchoi-minds/i3dnia', 'url': 'https://github.com/hjchoi-minds/i3dnia'}, {'title': 'Alexyuda/action_recognition', 'url': 'https://github.com/Alexyuda/action_recognition'}, {'title': 'helloxy96/CS5242_Project2020', 'url': 'https://github.com/helloxy96/CS5242_Project2020'}, {'title': 'ShobhitMaheshwari/sign-language1', 'url': 'https://github.com/ShobhitMaheshwari/sign-language1'}, {'title': 'mHealthBuet/SegCodeNet', 'url': 'https://github.com/mHealthBuet/SegCodeNet'}], 'metrics': {'Accuracy (RGB+pose)': '84.1'}, 'model_links': [], 'model_name': 'I3D', 'paper_date': '2017-05-22', 'paper_title': 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset', 'paper_url': 'http://arxiv.org/abs/1705.07750v3', 'uses_additional_data': False}, {'code_links': [{'title': 'agethen/RPAN', 'url': 'https://github.com/agethen/RPAN'}], 'metrics': {'Accuracy (RGB+pose)': '83.9'}, 'model_links': [], 'model_name': 'RPAN', 'paper_date': '2017-10-22', 'paper_title': 'RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos', 'paper_url': 'https://doi.org/10.1109/ICCV.2017.402', 'uses_additional_data': False}, {'code_links': [{'title': 'mzolfaghari/chained-multistream-networks', 'url': 'https://github.com/mzolfaghari/chained-multistream-networks'}], 'metrics': {'Accuracy (RGB+pose)': '76.1', 'Accuracy (pose)': '56.8'}, 'model_links': [], 'model_name': 'Chained (RGB+Flow +Pose)', 'paper_date': '2017-04-03', 'paper_title': 'Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection', 'paper_url': 'http://arxiv.org/abs/1704.00616v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '71.1'}, 'model_links': [], 'model_name': 'MR Two-Sream R-CNN', 'paper_date': '2016-09-17', 'paper_title': 'Multi-region two-stream R-CNN for action detection', 'paper_url': 'https://doi.org/10.1007/978-3-319-46493-0_45', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '69.5'}, 'model_links': [], 'model_name': 'PA3D', 'paper_date': '2019-06-01', 'paper_title': 'PA3D: Pose-Action 3D Machine for Video Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy (RGB+pose)': '64.3'}, 'model_links': [], 'model_name': 'STAR-Net', 'paper_date': '2019-02-26', 'paper_title': 'STAR-Net: Action Recognition using Spatio-Temporal Activation Reprojection', 'paper_url': 'http://arxiv.org/abs/1902.10024v1', 'uses_additional_data': False}, {'code_links': [{'title': 'JeffCHEN2017/WSSTG', 'url': 'https://github.com/JeffCHEN2017/WSSTG'}], 'metrics': {'Accuracy (RGB+pose)': '62.5'}, 'model_links': [], 'model_name': 'Action Tubes', 'paper_date': '2014-11-21', 'paper_title': 'Finding Action Tubes', 'paper_url': 'http://arxiv.org/abs/1411.6031v1', 'uses_additional_data': False}, {'code_links': [{'title': 'fandulu/DD-Net', 'url': 'https://github.com/fandulu/DD-Net'}, {'title': 'BlurryLight/DD-Net-Pytorch', 'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}], 'metrics': {'Accuracy (RGB+pose)': '-', 'Accuracy (pose)': '77.2'}, 'model_links': [], 'model_name': 'DD-Net', 'paper_date': '2019-07-23', 'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better', 'paper_url': 'https://arxiv.org/abs/1907.09658v8', 'uses_additional_data': False}, {'code_links': [{'title': 'noboevbo/ehpi_action_recognition', 'url': 'https://github.com/noboevbo/ehpi_action_recognition'}], 'metrics': {'Accuracy (RGB+pose)': '-', 'Accuracy (pose)': '65.5'}, 'model_links': [], 'model_name': 'EHPI', 'paper_date': '2019-04-19', 'paper_title': 'Simple yet efficient real-time pose-based action recognition', 'paper_url': 'http://arxiv.org/abs/1904.09140v1', 'uses_additional_data': False}]}, 'subdatasets': []}, {'dataset': 'UPenn Action', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-upenn'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'google-research/google-research', 'url': 'https://github.com/google-research/google-research/tree/master/poem'}], 'metrics': {'Accuracy': '97.5'}, 'model_links': [], 'model_name': 'Pr-VIPE', 'paper_date': '2019-12-02', 'paper_title': 'View-Invariant Probabilistic Embedding for Human Pose', 'paper_url': 'https://arxiv.org/abs/1912.01001v4', 'uses_additional_data': False}, {'code_links': [{'title': 'rort1989/HDM', 'url': 'https://github.com/rort1989/HDM'}], 'metrics': {'Accuracy': '93.4'}, 'model_links': [], 'model_name': 'HDM-BG', 'paper_date': '2019-06-01', 'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html', 'uses_additional_data': False}]}, 'subdatasets': []}], 'description': '<span style=\"color:grey; opacity: 0.6\">( Image credit: [View Adaptive Neural Networks for High\n",
       "Performance Skeleton-based Human Action\n",
       "Recognition](https://arxiv.org/pdf/1804.07453v3.pdf) )</span>', 'source_link': None, 'subtasks': [], 'synonyms': [], 'task': 'Skeleton Based Action Recognition'}, {'categories': [], 'datasets': [{'dataset': 'NTU RGB+D 120', 'dataset_citations': [], 'dataset_links': [{'title': 'Papers with Code Leaderboard URL', 'url': 'https://paperswithcode.com/sota/one-shot-3d-action-recognition-on-ntu-rgbd'}], 'description': '', 'sota': {'metrics': ['Accuracy'], 'rows': [{'code_links': [{'title': 'raphaelmemmesheimer/sl-dml', 'url': 'https://github.com/raphaelmemmesheimer/sl-dml'}], 'metrics': {'Accuracy': '49.6%'}, 'model_links': [], 'model_name': 'Deep Metric Learning (Triplet Loss, Signals)', 'paper_date': '2020-04-23', 'paper_title': 'SL-DML: Signal Level Deep Metric Learning for Multimodal One-Shot Action Recognition', 'paper_url': 'https://arxiv.org/abs/2004.11085v4', 'uses_additional_data': False}, {'code_links': [{'title': 'AlbertoSabater/Skeleton-based-One-shot-Action-Recognition', 'url': 'https://github.com/AlbertoSabater/Skeleton-based-One-shot-Action-Recognition'}], 'metrics': {'Accuracy': '46.5%'}, 'model_links': [], 'model_name': 'TCN_OneShot', 'paper_date': '2021-02-17', 'paper_title': 'One-shot action recognition in challenging therapy scenarios', 'paper_url': 'https://arxiv.org/abs/2102.08997v3', 'uses_additional_data': False}, {'code_links': [{'title': 'shahroudy/NTURGB-D', 'url': 'https://github.com/shahroudy/NTURGB-D'}, {'title': 'LinguoLi/CrosSCLR', 'url': 'https://github.com/LinguoLi/CrosSCLR'}], 'metrics': {'Accuracy': '45.3%'}, 'model_links': [], 'model_name': 'APSR', 'paper_date': '2019-05-12', 'paper_title': 'NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding', 'paper_url': 'https://arxiv.org/abs/1905.04757v2', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '42.9%'}, 'model_links': [], 'model_name': 'Average Pooling', 'paper_date': '2017-06-26', 'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates', 'paper_url': 'http://arxiv.org/abs/1706.08276v1', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '42.1%'}, 'model_links': [], 'model_name': 'Fully Connected', 'paper_date': '2017-07-01', 'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html', 'uses_additional_data': False}, {'code_links': [], 'metrics': {'Accuracy': '41.0%'}, 'model_links': [], 'model_name': 'Attention Network', 'paper_date': '2017-07-01', 'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition', 'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html', 'uses_additional_data': False}]}, 'subdatasets': []}], 'description': '', 'source_link': None, 'subtasks': [], 'synonyms': [], 'task': 'One-Shot 3D Action Recognition'}, {'categories': [], 'datasets': [], 'description': 'Detect if two people are looking at each other', 'source_link': None, 'subtasks': [], 'synonyms': [], 'task': 'Mutual Gaze'}]   \n",
       "\n",
       "    synonyms                           task  \n",
       "100       []  Human Interaction Recognition  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display it\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab7a4c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get all the subtasks of this sample\n",
    "subtasks = sample[\"subtasks\"].iloc[0]\n",
    "\n",
    "len(subtasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b583d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skeleton Based Action Recognition\n",
      "One-Shot 3D Action Recognition\n",
      "Mutual Gaze\n"
     ]
    }
   ],
   "source": [
    "#print different tasks\n",
    "print(subtasks[0][\"task\"])\n",
    "print(subtasks[1][\"task\"])\n",
    "print(subtasks[2][\"task\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49040a37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categories': [],\n",
       " 'datasets': [{'dataset': 'SYSU 3D',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sysu-3d'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'microsoft/SGN',\n",
       "        'url': 'https://github.com/microsoft/SGN'}],\n",
       "      'metrics': {'Accuracy': '86.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SGN',\n",
       "      'paper_date': '2019-04-02',\n",
       "      'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1904.01189v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "        'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy': '86.7%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VA-fusion (aug.)',\n",
       "      'paper_date': '2018-04-20',\n",
       "      'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '85.7%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EleAtt-GRU (aug.)',\n",
       "      'paper_date': '2019-09-03',\n",
       "      'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/1909.01939v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '83.14%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Local+LGN',\n",
       "      'paper_date': '2019-09-02',\n",
       "      'paper_title': 'Learning Latent Global Network for Skeleton-based Action Prediction',\n",
       "      'paper_url': 'https://doi.org/10.1109/TIP.2019.2937757',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '77.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Complete GR-GCN',\n",
       "      'paper_date': '2018-11-29',\n",
       "      'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "      'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '77.5%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VA-LSTM',\n",
       "      'paper_date': '2017-03-24',\n",
       "      'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data',\n",
       "      'paper_url': 'http://arxiv.org/abs/1703.08274v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '76.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DPRL',\n",
       "      'paper_date': '2018-06-01',\n",
       "      'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '75.5%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Dynamic Skeletons',\n",
       "      'paper_date': '2016-12-15',\n",
       "      'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '73.4%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-LSTM (Tree)',\n",
       "      'paper_date': '2017-06-26',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates',\n",
       "      'paper_url': 'http://arxiv.org/abs/1706.08276v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'UT-Kinect',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ut'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "        'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "      'metrics': {'Accuracy': '99.50%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Temporal Subspace Clustering',\n",
       "      'paper_date': '2020-06-21',\n",
       "      'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "      'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '98.5%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Complete GR-GCN',\n",
       "      'paper_date': '2018-11-29',\n",
       "      'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "      'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '98.5%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DPRL',\n",
       "      'paper_date': '2018-06-01',\n",
       "      'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'fdsa1860/skeletal',\n",
       "        'url': 'https://github.com/fdsa1860/skeletal'}],\n",
       "      'metrics': {'Accuracy': '97.1%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Lie Group',\n",
       "      'paper_date': '2014-06-23',\n",
       "      'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group',\n",
       "      'paper_url': 'https://doi.org/10.1109/CVPR.2014.82',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '96%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GFT',\n",
       "      'paper_date': '2019-08-26',\n",
       "      'paper_title': 'Graph Based Skeleton Modeling for Human Activity Analysis',\n",
       "      'paper_url': 'https://doi.org/10.1109/ICIP.2019.8803186',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'N-UCLA',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-n-ucla'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [],\n",
       "      'metrics': {'Accuracy': '93.99'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Hierarchical Action Classification (RGB + Pose)',\n",
       "      'paper_date': '2020-07-30',\n",
       "      'paper_title': 'Hierarchical Action Classification with Network Pruning',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.15244v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'srijandas07/VPN',\n",
       "        'url': 'https://github.com/srijandas07/VPN'}],\n",
       "      'metrics': {'Accuracy': '93.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VPN (RGB + Pose)',\n",
       "      'paper_date': '2020-07-06',\n",
       "      'paper_title': 'VPN: Learning Video-Pose Embedding for Activities of Daily Living',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.03056v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'srijandas07/vpnplusplus',\n",
       "        'url': 'https://github.com/srijandas07/vpnplusplus'}],\n",
       "      'metrics': {'Accuracy': '93.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VPN++ (RGB + Pose)',\n",
       "      'paper_date': '2021-05-17',\n",
       "      'paper_title': 'VPN++: Rethinking Video-Pose embeddings for understanding Activities of Daily Living',\n",
       "      'paper_url': 'https://arxiv.org/abs/2105.08141v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'microsoft/SGN',\n",
       "        'url': 'https://github.com/microsoft/SGN'}],\n",
       "      'metrics': {'Accuracy': '92.5%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SGN',\n",
       "      'paper_date': '2019-04-02',\n",
       "      'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1904.01189v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '92.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Action Machine',\n",
       "      'paper_date': '2018-12-14',\n",
       "      'paper_title': 'Action Machine: Rethinking Action Recognition in Trimmed Videos',\n",
       "      'paper_url': 'http://arxiv.org/abs/1812.05770v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '90.7%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EleAtt-GRU (aug.)',\n",
       "      'paper_date': '2019-09-03',\n",
       "      'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/1909.01939v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "        'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy': '88.1%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VA-fusion (aug.)',\n",
       "      'paper_date': '2018-04-20',\n",
       "      'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'fabienbaradel/glimpse_clouds',\n",
       "        'url': 'https://github.com/fabienbaradel/glimpse_clouds'}],\n",
       "      'metrics': {'Accuracy': '87.6%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Glimpse Clouds',\n",
       "      'paper_date': '2018-02-22',\n",
       "      'paper_title': 'Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points',\n",
       "      'paper_url': 'http://arxiv.org/abs/1802.07898v4',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'SBU',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sbu'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'Sy-Zhang/Geometric-Feature-Release',\n",
       "        'url': 'https://github.com/Sy-Zhang/Geometric-Feature-Release'}],\n",
       "      'metrics': {'Accuracy': '99.02%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Joint Line Distance',\n",
       "      'paper_date': '2017-03-01',\n",
       "      'paper_title': 'On Geometric Features for Skeleton-Based Action Recognition using Multilayer LSTM Networks',\n",
       "      'paper_url': 'https://doi.org/10.1109/WACV.2017.24',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '98.60%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MLGCN',\n",
       "      'paper_date': '2019-09-11',\n",
       "      'paper_title': 'MLGCN: Multi-Laplacian Graph Convolutional Networks for Human Action Recognition',\n",
       "      'paper_url': 'https://bmvc2019.org/wp-content/uploads/papers/1103-paper.pdf',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "        'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy': '98.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VA-fusion (aug.)',\n",
       "      'paper_date': '2018-04-20',\n",
       "      'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'mdeff/cnn_graph',\n",
       "        'url': 'https://github.com/mdeff/cnn_graph'},\n",
       "       {'title': 'xbresson/spectral_graph_convnets',\n",
       "        'url': 'https://github.com/xbresson/spectral_graph_convnets'},\n",
       "       {'title': 'jasonseu/cnn_graph.pytorch',\n",
       "        'url': 'https://github.com/jasonseu/cnn_graph.pytorch'},\n",
       "       {'title': 'andrejmiscic/gcn-pytorch',\n",
       "        'url': 'https://github.com/andrejmiscic/gcn-pytorch'},\n",
       "       {'title': 'mdeff/paper-cnn-graph-nips2016',\n",
       "        'url': 'https://github.com/mdeff/paper-cnn-graph-nips2016'},\n",
       "       {'title': 'hazdzz/ChebyNet',\n",
       "        'url': 'https://github.com/hazdzz/ChebyNet'},\n",
       "       {'title': 'stsfaroz/Graph-Convolutional-Networks-with-Cora-Dataset',\n",
       "        'url': 'https://github.com/stsfaroz/Graph-Convolutional-Networks-with-Cora-Dataset'}],\n",
       "      'metrics': {'Accuracy': '96.00%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ChebyNet',\n",
       "      'paper_date': '2016-06-30',\n",
       "      'paper_title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering',\n",
       "      'paper_url': 'http://arxiv.org/abs/1606.09375v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '96.00%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ArmaConv',\n",
       "      'paper_date': '2019-01-05',\n",
       "      'paper_title': 'Graph Neural Networks with convolutional ARMA filters',\n",
       "      'paper_url': 'https://arxiv.org/abs/1901.01343v7',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'Maghoumi/DeepGRU',\n",
       "        'url': 'https://github.com/Maghoumi/DeepGRU'}],\n",
       "      'metrics': {'Accuracy': '95.7%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DeepGRU',\n",
       "      'paper_date': '2018-10-30',\n",
       "      'paper_title': 'DeepGRU: Deep Gesture Recognition Utility',\n",
       "      'paper_url': 'https://arxiv.org/abs/1810.12514v4',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'Tiiiger/SGC',\n",
       "        'url': 'https://github.com/Tiiiger/SGC'},\n",
       "       {'title': 'pulkit1joshi/SGC',\n",
       "        'url': 'https://github.com/pulkit1joshi/SGC'},\n",
       "       {'title': 'hazdzz/SGC', 'url': 'https://github.com/hazdzz/SGC'}],\n",
       "      'metrics': {'Accuracy': '94.0%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SGCConv',\n",
       "      'paper_date': '2019-02-19',\n",
       "      'paper_title': 'Simplifying Graph Convolutional Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/1902.07153v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '93.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-LSTM + Trust Gate',\n",
       "      'paper_date': '2016-07-24',\n",
       "      'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'tkipf/gcn',\n",
       "        'url': 'https://github.com/tkipf/gcn'},\n",
       "       {'title': 'tkipf/pygcn', 'url': 'https://github.com/tkipf/pygcn'},\n",
       "       {'title': 'tkipf/keras-gcn',\n",
       "        'url': 'https://github.com/tkipf/keras-gcn'},\n",
       "       {'title': 'LeeWooJung/GCN_reproduce',\n",
       "        'url': 'https://github.com/LeeWooJung/GCN_reproduce'},\n",
       "       {'title': 'hazdzz/GCN', 'url': 'https://github.com/hazdzz/GCN'},\n",
       "       {'title': 'giuseppefutia/link-prediction-code',\n",
       "        'url': 'https://github.com/giuseppefutia/link-prediction-code'},\n",
       "       {'title': 'andrejmiscic/gcn-pytorch',\n",
       "        'url': 'https://github.com/andrejmiscic/gcn-pytorch'},\n",
       "       {'title': 'switiz/gnn-gcn-gat',\n",
       "        'url': 'https://github.com/switiz/gnn-gcn-gat'},\n",
       "       {'title': 'dtriepke/Graph_Convolutional_Network',\n",
       "        'url': 'https://github.com/dtriepke/Graph_Convolutional_Network'},\n",
       "       {'title': 'bcsrn/gcn', 'url': 'https://github.com/bcsrn/gcn'},\n",
       "       {'title': 'Anieca/GCN', 'url': 'https://github.com/Anieca/GCN'},\n",
       "       {'title': 'HoganZhang/pygcn_python3',\n",
       "        'url': 'https://github.com/HoganZhang/pygcn_python3'},\n",
       "       {'title': 'lipingcoding/pygcn',\n",
       "        'url': 'https://github.com/lipingcoding/pygcn'},\n",
       "       {'title': 'KimMeen/GCN', 'url': 'https://github.com/KimMeen/GCN'},\n",
       "       {'title': 'darnbi/pygcn', 'url': 'https://github.com/darnbi/pygcn'},\n",
       "       {'title': 'thanhtrunghuynh93/pygcn',\n",
       "        'url': 'https://github.com/thanhtrunghuynh93/pygcn'},\n",
       "       {'title': 'ChengSashankh/gcn-graph-classification',\n",
       "        'url': 'https://github.com/ChengSashankh/gcn-graph-classification'},\n",
       "       {'title': 'LouisDumont/GCN---re-implementation',\n",
       "        'url': 'https://github.com/LouisDumont/GCN---re-implementation'}],\n",
       "      'metrics': {'Accuracy': '90.00%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GCNConv',\n",
       "      'paper_date': '2016-09-09',\n",
       "      'paper_title': 'Semi-Supervised Classification with Graph Convolutional Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1609.02907v4',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'JHMDB Pose Tracking',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-jhmdb'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['PCK@0.1', 'PCK@0.2', 'PCK@0.3', 'PCK@0.4', 'PCK@0.5'],\n",
       "    'rows': [{'code_links': [{'title': 'aimerykong/predictive-filter-flow',\n",
       "        'url': 'https://github.com/aimerykong/predictive-filter-flow'},\n",
       "       {'title': 'bestaar/predictiveFilterFlow',\n",
       "        'url': 'https://github.com/bestaar/predictiveFilterFlow'}],\n",
       "      'metrics': {'PCK@0.1': '58.4',\n",
       "       'PCK@0.2': '78.1',\n",
       "       'PCK@0.3': '85.9',\n",
       "       'PCK@0.4': '89.8',\n",
       "       'PCK@0.5': '92.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'mgPFF+ft 1st',\n",
       "      'paper_date': '2019-04-02',\n",
       "      'paper_title': 'Multigrid Predictive Filter Flow for Unsupervised Learning on Videos',\n",
       "      'paper_url': 'http://arxiv.org/abs/1904.01693v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'hyperparameters/tracking_via_colorization',\n",
       "        'url': 'https://github.com/hyperparameters/tracking_via_colorization'}],\n",
       "      'metrics': {'PCK@0.1': '45.2',\n",
       "       'PCK@0.2': '69.6',\n",
       "       'PCK@0.3': '80.8',\n",
       "       'PCK@0.4': '87.5',\n",
       "       'PCK@0.5': '91.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ColorPointer',\n",
       "      'paper_date': '2018-06-25',\n",
       "      'paper_title': 'Tracking Emerges by Colorizing Videos',\n",
       "      'paper_url': 'http://arxiv.org/abs/1806.09594v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'NVIDIA/flownet2-pytorch',\n",
       "        'url': 'https://github.com/NVIDIA/flownet2-pytorch'},\n",
       "       {'title': 'philferriere/tfoptflow',\n",
       "        'url': 'https://github.com/philferriere/tfoptflow'},\n",
       "       {'title': 'ElliotHYLee/VisualOdometry3D',\n",
       "        'url': 'https://github.com/ElliotHYLee/VisualOdometry3D'},\n",
       "       {'title': 'mcgridles/LENS', 'url': 'https://github.com/mcgridles/LENS'},\n",
       "       {'title': 'rickyHong/tfoptflow-repl',\n",
       "        'url': 'https://github.com/rickyHong/tfoptflow-repl'}],\n",
       "      'metrics': {'PCK@0.1': '45.2',\n",
       "       'PCK@0.2': '62.9',\n",
       "       'PCK@0.3': '73.5',\n",
       "       'PCK@0.4': '80.6',\n",
       "       'PCK@0.5': '85.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'FlowNet2',\n",
       "      'paper_date': '2016-12-06',\n",
       "      'paper_title': 'FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1612.01925v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'NTU RGB+D 120',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu-rgbd-1'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy (Cross-Subject)', 'Accuracy (Cross-Setup)'],\n",
       "    'rows': [{'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '89.1',\n",
       "       'Accuracy (Cross-Subject)': '88.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EfficientGCN-B4',\n",
       "      'paper_date': '2021-06-29',\n",
       "      'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '89.2%',\n",
       "       'Accuracy (Cross-Subject)': '88.2%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'AngNet-JA + BA + JBA + VJBA',\n",
       "      'paper_date': '2021-05-04',\n",
       "      'paper_title': 'Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2105.01563v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '88.0',\n",
       "       'Accuracy (Cross-Subject)': '87.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EfficientGCN-B2',\n",
       "      'paper_date': '2021-06-29',\n",
       "      'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/ResGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/ResGCNv1'},\n",
       "       {'title': 'yfsong0709/EfficientGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '88.3%',\n",
       "       'Accuracy (Cross-Subject)': '87.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PA-ResGCN-B19',\n",
       "      'paper_date': '2020-10-20',\n",
       "      'paper_title': 'Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2010.09978v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'skelemoa/quovadis',\n",
       "        'url': 'https://github.com/skelemoa/quovadis'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '88.8%',\n",
       "       'Accuracy (Cross-Subject)': '87.22%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Ensemble-top5 (MS-G3D Net + 4s Shift-GCN + VA-CNN (ResNeXt101) + 2s SDGCN + GCN-NAS (retrained))',\n",
       "      'paper_date': '2020-07-04',\n",
       "      'paper_title': 'Quo Vadis, Skeleton Action Recognition ?',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.02072v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'open-mmlab/mmaction2',\n",
       "        'url': 'https://github.com/open-mmlab/mmaction2'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '90.3',\n",
       "       'Accuracy (Cross-Subject)': '86.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PoseC3D (w. HRNet 2D Skeleton)',\n",
       "      'paper_date': '2021-04-28',\n",
       "      'paper_title': 'Revisiting Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2104.13586v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kenziyuliu/ms-g3d',\n",
       "        'url': 'https://github.com/kenziyuliu/ms-g3d'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '88.4%',\n",
       "       'Accuracy (Cross-Subject)': '86.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MS-G3D Net',\n",
       "      'paper_date': '2020-03-31',\n",
       "      'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.14111v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'lshiwjx/DSTA-Net',\n",
       "        'url': 'https://github.com/lshiwjx/DSTA-Net'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '89.0 %',\n",
       "       'Accuracy (Cross-Subject)': '86.6%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DSTA-Net',\n",
       "      'paper_date': '2020-07-07',\n",
       "      'paper_title': 'Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.03263v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'srijandas07/VPN',\n",
       "        'url': 'https://github.com/srijandas07/VPN'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '87.8',\n",
       "       'Accuracy (Cross-Subject)': '86.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VPN',\n",
       "      'paper_date': '2020-07-06',\n",
       "      'paper_title': 'VPN: Learning Video-Pose Embedding for Activities of Daily Living',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.03056v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kchengiva/Shift-GCN',\n",
       "        'url': 'https://github.com/kchengiva/Shift-GCN'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '87.6%',\n",
       "       'Accuracy (Cross-Subject)': '85.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': '4s Shift-GCN',\n",
       "      'paper_date': '2020-06-01',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '84.3',\n",
       "       'Accuracy (Cross-Subject)': '85.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EfficientGCN-B0',\n",
       "      'paper_date': '2021-06-29',\n",
       "      'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '87.4%',\n",
       "       'Accuracy (Cross-Subject)': '85.4%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'FGCN ',\n",
       "      'paper_date': '2020-03-17',\n",
       "      'paper_title': 'Feedback Graph Convolutional Network for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.07564v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '86.90',\n",
       "       'Accuracy (Cross-Subject)': '84.88'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VA-CNN (ResNeXt-101)',\n",
       "      'paper_date': None,\n",
       "      'paper_title': '',\n",
       "      'paper_url': '',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'Chiaraplizz/ST-TR',\n",
       "        'url': 'https://github.com/Chiaraplizz/ST-TR'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '84.7%',\n",
       "       'Accuracy (Cross-Subject)': '82.7%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-TR-agcn',\n",
       "      'paper_date': '2020-08-17',\n",
       "      'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/2008.07404v4',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/RA-GCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/RA-GCNv1'},\n",
       "       {'title': 'yfsong0709/RA-GCNv2',\n",
       "        'url': 'https://github.com/yfsong0709/RA-GCNv2'},\n",
       "       {'title': 'peter-yys-yoon/pegcnv2',\n",
       "        'url': 'https://github.com/peter-yys-yoon/pegcnv2'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '82.7%',\n",
       "       'Accuracy (Cross-Subject)': '81.1%'},\n",
       "      'model_links': [],\n",
       "      'model_name': '3s RA-GCN',\n",
       "      'paper_date': '2020-08-09',\n",
       "      'paper_title': 'Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2008.03791v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '83.2%',\n",
       "       'Accuracy (Cross-Subject)': '80.5%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Mix-Dimension',\n",
       "      'paper_date': '2020-07-30',\n",
       "      'paper_title': 'Mix Dimension in Poincar√© Geometry for 3D Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.15678v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '79.8%',\n",
       "       'Accuracy (Cross-Subject)': '78.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GVFE + AS-GCN with DH-TCN',\n",
       "      'paper_date': '2019-12-20',\n",
       "      'paper_title': 'Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.09745v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'airglow/gimme_signals_action_recognition',\n",
       "        'url': 'https://github.com/airglow/gimme_signals_action_recognition'},\n",
       "       {'title': 'raphaelmemmesheimer/gimme_signals_action_recognition',\n",
       "        'url': 'https://github.com/raphaelmemmesheimer/gimme_signals_action_recognition'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '71.6%',\n",
       "       'Accuracy (Cross-Subject)': '70.8%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Gimme Signals (Skeleton, AIS)',\n",
       "      'paper_date': '2020-03-13',\n",
       "      'paper_title': 'Gimme Signals: Discriminative signal encoding for multimodal activity recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.06156v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '67.2%',\n",
       "       'Accuracy (Cross-Subject)': '68.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Logsig-RNN',\n",
       "      'paper_date': '2019-08-22',\n",
       "      'paper_title': 'Learning stochastic differential equations using RNN with log signature features',\n",
       "      'paper_url': 'https://arxiv.org/abs/1908.08286v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "        'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '62.8%',\n",
       "       'Accuracy (Cross-Subject)': '67.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'TSRJI (Late Fusion) + HCN',\n",
       "      'paper_date': '2019-09-11',\n",
       "      'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints',\n",
       "      'paper_url': 'https://arxiv.org/abs/1909.05704v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "        'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '66.9%',\n",
       "       'Accuracy (Cross-Subject)': '67.7%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SkeleMotion + Yang et al. (2018)',\n",
       "      'paper_date': '2019-07-30',\n",
       "      'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1907.13025v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "        'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '59.7%',\n",
       "       'Accuracy (Cross-Subject)': '65.5%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'TSRJI (Late Fusion)',\n",
       "      'paper_date': '2019-09-11',\n",
       "      'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints',\n",
       "      'paper_url': 'https://arxiv.org/abs/1909.05704v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '66.9%',\n",
       "       'Accuracy (Cross-Subject)': '64.6%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Body Pose Evolution Map',\n",
       "      'paper_date': '2018-06-01',\n",
       "      'paper_title': 'Recognizing Human Actions as the Evolution of Pose Estimation Maps',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Recognizing_Human_Actions_CVPR_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "        'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '63.0%',\n",
       "       'Accuracy (Cross-Subject)': '62.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SkeleMotion [Magnitude-Orientation (TSA)]',\n",
       "      'paper_date': '2019-07-30',\n",
       "      'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1907.13025v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '61.8%',\n",
       "       'Accuracy (Cross-Subject)': '62.2%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Multi-Task CNN with RotClips',\n",
       "      'paper_date': '2018-03-05',\n",
       "      'paper_title': 'Learning clip representations for skeleton-based 3d action recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/TIP.2018.2812099',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '63.3%',\n",
       "       'Accuracy (Cross-Subject)': '61.2%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Two-Stream Attention LSTM',\n",
       "      'paper_date': '2017-07-18',\n",
       "      'paper_title': 'Skeleton-Based Human Action Recognition with Global Context-Aware Attention LSTM Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1707.05740v5',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '63.2%',\n",
       "       'Accuracy (Cross-Subject)': '60.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Skeleton Visualization (Single Stream)',\n",
       "      'paper_date': '2017-08-01',\n",
       "      'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition',\n",
       "      'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '62.4%',\n",
       "       'Accuracy (Cross-Subject)': '59.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'FSNet',\n",
       "      'paper_date': '2019-02-08',\n",
       "      'paper_title': 'Skeleton-Based Online Action Prediction Using Scale Selection Network',\n",
       "      'paper_url': 'http://arxiv.org/abs/1902.03084v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '57.9%',\n",
       "       'Accuracy (Cross-Subject)': '58.4%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Multi-Task Learning Network',\n",
       "      'paper_date': '2017-03-09',\n",
       "      'paper_title': 'A New Representation of Skeleton Sequences for 3D Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1703.03492v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '59.2%',\n",
       "       'Accuracy (Cross-Subject)': '58.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GCA-LSTM',\n",
       "      'paper_date': '2017-07-01',\n",
       "      'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '60.9%',\n",
       "       'Accuracy (Cross-Subject)': '58.2%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Internal Feature Fusion',\n",
       "      'paper_date': '2017-06-26',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates',\n",
       "      'paper_url': 'http://arxiv.org/abs/1706.08276v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '57.9%',\n",
       "       'Accuracy (Cross-Subject)': '55.7%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Spatio-Temporal LSTM',\n",
       "      'paper_date': '2016-07-24',\n",
       "      'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '54.7%',\n",
       "       'Accuracy (Cross-Subject)': '50.8%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Dynamic Skeletons',\n",
       "      'paper_date': '2016-12-15',\n",
       "      'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '44.9%',\n",
       "       'Accuracy (Cross-Subject)': '36.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Soft RNN',\n",
       "      'paper_date': '2018-08-06',\n",
       "      'paper_title': 'Early action prediction by soft regression',\n",
       "      'paper_url': 'https://doi.org/10.1109/TPAMI.2018.2863279',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "        'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "      'metrics': {'Accuracy (Cross-Setup)': '26.3%',\n",
       "       'Accuracy (Cross-Subject)': '25.5%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Part-Aware LSTM',\n",
       "      'paper_date': '2016-04-11',\n",
       "      'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "      'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'HDM05',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-hdm05'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "        'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "      'metrics': {'Accuracy': '89.80%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Temporal Subspace Clustering',\n",
       "      'paper_date': '2020-06-21',\n",
       "      'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "      'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'Gaming 3D (G3D)',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-gaming'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [],\n",
       "      'metrics': {'Accuracy': '96.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'CNN',\n",
       "      'paper_date': '2016-12-30',\n",
       "      'paper_title': 'Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1612.09401v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "        'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "      'metrics': {'Accuracy': '92.91%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Temporal K-Means Clustering + Temporal Covariance Subspace Clustering',\n",
       "      'paper_date': '2020-06-21',\n",
       "      'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "      'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'rort1989/HDM',\n",
       "        'url': 'https://github.com/rort1989/HDM'}],\n",
       "      'metrics': {'Accuracy': '92.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'HDM-BG',\n",
       "      'paper_date': '2019-06-01',\n",
       "      'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '90.94'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Rolling Rotations (FTP)',\n",
       "      'paper_date': '2016-06-27',\n",
       "      'paper_title': 'Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data',\n",
       "      'paper_url': 'https://doi.org/10.1109/CVPR.2016.484',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'UWA3D',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-uwa3d'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "        'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy': '81.4%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VA-fusion (aug.)',\n",
       "      'paper_date': '2018-04-20',\n",
       "      'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '73.8%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ESV (Synthesized + Pre-trained)',\n",
       "      'paper_date': '2017-08-01',\n",
       "      'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition',\n",
       "      'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '17.7%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'HOJ3D',\n",
       "      'paper_date': '2012-07-16',\n",
       "      'paper_title': 'View invariant human action recognition using histograms of 3D joints',\n",
       "      'paper_url': 'https://doi.org/10.1109/CVPRW.2012.6239233',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'MSRC-12',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msrc-12'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "        'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "      'metrics': {'Accuracy': '99.08%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Temporal Subspace Clustering',\n",
       "      'paper_date': '2020-06-21',\n",
       "      'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "      'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'Kinetics-Skeleton dataset',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-kinetics'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'open-mmlab/mmaction2',\n",
       "        'url': 'https://github.com/open-mmlab/mmaction2'}],\n",
       "      'metrics': {'Accuracy': '47.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PoseC3D (w. HRNet 2D skeleton)',\n",
       "      'paper_date': '2021-04-28',\n",
       "      'paper_title': 'Revisiting Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2104.13586v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '38.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': '2s-AGCN+TEM',\n",
       "      'paper_date': '2020-03-19',\n",
       "      'paper_title': 'Temporal Extension Module for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.08951v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kenziyuliu/ms-g3d',\n",
       "        'url': 'https://github.com/kenziyuliu/ms-g3d'}],\n",
       "      'metrics': {'Accuracy': '38.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MS-G3D',\n",
       "      'paper_date': '2020-03-31',\n",
       "      'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.14111v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'open-mmlab/mmaction2',\n",
       "        'url': 'https://github.com/open-mmlab/mmaction2'}],\n",
       "      'metrics': {'Accuracy': '38.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PoseC3D',\n",
       "      'paper_date': '2021-04-28',\n",
       "      'paper_title': 'Revisiting Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2104.13586v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '37.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Dynamic GCN',\n",
       "      'paper_date': '2020-07-29',\n",
       "      'paper_title': 'Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.14690v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "        'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy': '37.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MS-AAGCN',\n",
       "      'paper_date': '2019-12-15',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.06971v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '37.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'CGCN',\n",
       "      'paper_date': '2020-03-06',\n",
       "      'paper_title': 'Centrality Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.03007v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "        'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy': '37.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'JB-AAGCN',\n",
       "      'paper_date': '2019-12-15',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.06971v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'Chiaraplizz/ST-TR',\n",
       "        'url': 'https://github.com/Chiaraplizz/ST-TR'}],\n",
       "      'metrics': {'Accuracy': '37.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-TR-agcn',\n",
       "      'paper_date': '2020-08-17',\n",
       "      'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/2008.07404v4',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'xiaoiker/GCN-NAS',\n",
       "        'url': 'https://github.com/xiaoiker/GCN-NAS'}],\n",
       "      'metrics': {'Accuracy': '37.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GCN-NAS',\n",
       "      'paper_date': '2019-11-11',\n",
       "      'paper_title': 'Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching',\n",
       "      'paper_url': 'https://arxiv.org/abs/1911.04131v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch',\n",
       "        'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}],\n",
       "      'metrics': {'Accuracy': '36.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DGNN',\n",
       "      'paper_date': '2019-06-01',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '36.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SLnL-rFA',\n",
       "      'paper_date': '2018-11-10',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention',\n",
       "      'paper_url': 'https://arxiv.org/abs/1811.04237v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal',\n",
       "        'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'},\n",
       "       {'title': 'lshiwjx/2s-AGCN',\n",
       "        'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy': '36.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': '2s-AGCN',\n",
       "      'paper_date': '2018-05-20',\n",
       "      'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1805.07694v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'limaosen0/AS-GCN',\n",
       "        'url': 'https://github.com/limaosen0/AS-GCN'}],\n",
       "      'metrics': {'Accuracy': '34.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'AS-GCN',\n",
       "      'paper_date': '2019-04-26',\n",
       "      'paper_title': 'Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1904.12659v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'andreYoo/PeGCNs',\n",
       "        'url': 'https://github.com/andreYoo/PeGCNs'}],\n",
       "      'metrics': {'Accuracy': '34.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PeGCN',\n",
       "      'paper_date': '2020-03-17',\n",
       "      'paper_title': 'Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.07514v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '33.7 '},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PR-GCN',\n",
       "      'paper_date': '2020-10-14',\n",
       "      'paper_title': 'Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2010.07367v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '33.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-GR',\n",
       "      'paper_date': '2019-07-17',\n",
       "      'paper_title': 'Spatiotemporal graph routing for skeleton-based action recognition',\n",
       "      'paper_url': 'https://doi.org/10.1609/aaai.v33i01.33018561',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '33.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'AR-GCN',\n",
       "      'paper_date': '2019-11-27',\n",
       "      'paper_title': 'An Attention-Enhanced Recurrent Graph Convolutional Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://doi.org/10.1145/3372806.3372814',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'raymondyeh07/chirality_nets',\n",
       "        'url': 'https://github.com/raymondyeh07/chirality_nets'}],\n",
       "      'metrics': {'Accuracy': '30.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Ours-Conv-Chiral',\n",
       "      'paper_date': '2019-10-31',\n",
       "      'paper_title': 'Chirality Nets for Human Pose Regression',\n",
       "      'paper_url': 'https://arxiv.org/abs/1911.00029v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'open-mmlab/mmskeleton',\n",
       "        'url': 'https://github.com/open-mmlab/mmskeleton'},\n",
       "       {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'},\n",
       "       {'title': 'ericksiavichay/cs230-final-project',\n",
       "        'url': 'https://github.com/ericksiavichay/cs230-final-project'},\n",
       "       {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'},\n",
       "       {'title': 'stillarrow/S2VT_ACT',\n",
       "        'url': 'https://github.com/stillarrow/S2VT_ACT'},\n",
       "       {'title': 'ZhangNYG/ST-GCN',\n",
       "        'url': 'https://github.com/ZhangNYG/ST-GCN'},\n",
       "       {'title': 'DixinFan/st-gcn',\n",
       "        'url': 'https://github.com/DixinFan/st-gcn'},\n",
       "       {'title': 'ken724049/action-recognition',\n",
       "        'url': 'https://github.com/ken724049/action-recognition'},\n",
       "       {'title': 'antoniolq/st-gcn',\n",
       "        'url': 'https://github.com/antoniolq/st-gcn'},\n",
       "       {'title': 'github-zbx/ST-GCN',\n",
       "        'url': 'https://github.com/github-zbx/ST-GCN'},\n",
       "       {'title': 'GeyuanZhang/st-gcn-master',\n",
       "        'url': 'https://github.com/GeyuanZhang/st-gcn-master'},\n",
       "       {'title': 'KrisLee512/ST-GCN',\n",
       "        'url': 'https://github.com/KrisLee512/ST-GCN'},\n",
       "       {'title': 'AbiterVX/ST-GCN',\n",
       "        'url': 'https://github.com/AbiterVX/ST-GCN'}],\n",
       "      'metrics': {'Accuracy': '30.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-GCN',\n",
       "      'paper_date': '2018-01-23',\n",
       "      'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1801.07455v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition',\n",
       "        'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}],\n",
       "      'metrics': {'Accuracy': '20.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Res-TCN',\n",
       "      'paper_date': '2017-04-14',\n",
       "      'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.04516v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'J-HMBD Early Action',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-j-hmbd'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['10%'],\n",
       "    'rows': [{'code_links': [{'title': 'ser-art/RAE-vs-AE',\n",
       "        'url': 'https://github.com/ser-art/RAE-vs-AE'},\n",
       "       {'title': 'rk68657/AutoEncoders',\n",
       "        'url': 'https://github.com/rk68657/AutoEncoders'}],\n",
       "      'metrics': {'10%': '60.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DR^2N',\n",
       "      'paper_date': '2018-02-09',\n",
       "      'paper_title': 'Relational Autoencoder for Feature Extraction',\n",
       "      'paper_url': 'http://arxiv.org/abs/1802.03145v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'labmlai/annotated_deep_learning_paper_implementations',\n",
       "        'url': 'https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/graphs/gat'},\n",
       "       {'title': 'PetarV-/GAT', 'url': 'https://github.com/PetarV-/GAT'},\n",
       "       {'title': 'Diego999/pyGAT', 'url': 'https://github.com/Diego999/pyGAT'},\n",
       "       {'title': 'gordicaleksa/pytorch-GAT',\n",
       "        'url': 'https://github.com/gordicaleksa/pytorch-GAT'},\n",
       "       {'title': 'shenweichen/GraphNeuralNetwork',\n",
       "        'url': 'https://github.com/shenweichen/GraphNeuralNetwork'},\n",
       "       {'title': 'danielegrattarola/keras-gat',\n",
       "        'url': 'https://github.com/danielegrattarola/keras-gat'},\n",
       "       {'title': 'HazyResearch/hgcn',\n",
       "        'url': 'https://github.com/HazyResearch/hgcn'},\n",
       "       {'title': 'GraphSAINT/GraphSAINT',\n",
       "        'url': 'https://github.com/GraphSAINT/GraphSAINT'},\n",
       "       {'title': 'lukecavabarrett/pna',\n",
       "        'url': 'https://github.com/lukecavabarrett/pna'},\n",
       "       {'title': 'Kaimaoge/IGNNK', 'url': 'https://github.com/Kaimaoge/IGNNK'},\n",
       "       {'title': 'zhao-tong/GNNs-easy-to-use',\n",
       "        'url': 'https://github.com/zhao-tong/GNNs-easy-to-use'},\n",
       "       {'title': 'snowkylin/gnn', 'url': 'https://github.com/snowkylin/gnn'},\n",
       "       {'title': 'marblet/GNN_models_pytorch_geometric',\n",
       "        'url': 'https://github.com/marblet/GNN_models_pytorch_geometric'},\n",
       "       {'title': 'marble0117/GNN_models_pytorch',\n",
       "        'url': 'https://github.com/marble0117/GNN_models_pytorch'},\n",
       "       {'title': 'marble0117/GNN_models_pytorch_geometric',\n",
       "        'url': 'https://github.com/marble0117/GNN_models_pytorch_geometric'},\n",
       "       {'title': 'HeapHop30/graph-attention-nets',\n",
       "        'url': 'https://github.com/HeapHop30/graph-attention-nets'},\n",
       "       {'title': 'weiyangfb/PyTorchSparseGAT',\n",
       "        'url': 'https://github.com/weiyangfb/PyTorchSparseGAT'},\n",
       "       {'title': 'marblet/gat-pytorch',\n",
       "        'url': 'https://github.com/marblet/gat-pytorch'},\n",
       "       {'title': 'ds4dm/sparse-gcn',\n",
       "        'url': 'https://github.com/ds4dm/sparse-gcn'},\n",
       "       {'title': 'ds4dm/sGat', 'url': 'https://github.com/ds4dm/sGat'},\n",
       "       {'title': 'gcucurull/jax-gat',\n",
       "        'url': 'https://github.com/gcucurull/jax-gat'},\n",
       "       {'title': 'calciver/Graph-Attention-Networks',\n",
       "        'url': 'https://github.com/calciver/Graph-Attention-Networks'},\n",
       "       {'title': 'noahtren/Graph-Attention-Networks-TensorFlow-2',\n",
       "        'url': 'https://github.com/noahtren/Graph-Attention-Networks-TensorFlow-2'},\n",
       "       {'title': 'Yindong-Zhang/myGAT',\n",
       "        'url': 'https://github.com/Yindong-Zhang/myGAT'},\n",
       "       {'title': 'liu6zijian/simplified-gcn-model',\n",
       "        'url': 'https://github.com/liu6zijian/simplified-gcn-model'},\n",
       "       {'title': 'taishan1994/pytorch_gat',\n",
       "        'url': 'https://github.com/taishan1994/pytorch_gat'},\n",
       "       {'title': 'BIG-S2/keras-gnm',\n",
       "        'url': 'https://github.com/BIG-S2/keras-gnm'},\n",
       "       {'title': 'Aveek-Saha/Graph-Attention-Net',\n",
       "        'url': 'https://github.com/Aveek-Saha/Graph-Attention-Net'},\n",
       "       {'title': 'zxhhh97/ABot', 'url': 'https://github.com/zxhhh97/ABot'},\n",
       "       {'title': 'qema/orca-py', 'url': 'https://github.com/qema/orca-py'},\n",
       "       {'title': 'mitya8128/experiments_notes',\n",
       "        'url': 'https://github.com/mitya8128/experiments_notes'},\n",
       "       {'title': 'giuseppefutia/link-prediction-code',\n",
       "        'url': 'https://github.com/giuseppefutia/link-prediction-code'},\n",
       "       {'title': 'AngusMonroe/GAT-pytorch',\n",
       "        'url': 'https://github.com/AngusMonroe/GAT-pytorch'},\n",
       "       {'title': 'handasontam/GAT-with-edgewise-attention',\n",
       "        'url': 'https://github.com/handasontam/GAT-with-edgewise-attention'},\n",
       "       {'title': 'fongyk/graph-attention',\n",
       "        'url': 'https://github.com/fongyk/graph-attention'},\n",
       "       {'title': 'mlzxzhou/keras-gnm',\n",
       "        'url': 'https://github.com/mlzxzhou/keras-gnm'},\n",
       "       {'title': 'YunseobShin/wiki_GAT',\n",
       "        'url': 'https://github.com/YunseobShin/wiki_GAT'},\n",
       "       {'title': 'dzb1998/pyGAT', 'url': 'https://github.com/dzb1998/pyGAT'},\n",
       "       {'title': 'Anou9531/GAT', 'url': 'https://github.com/Anou9531/GAT'},\n",
       "       {'title': 'WantingZhao/my_GAT',\n",
       "        'url': 'https://github.com/WantingZhao/my_GAT'},\n",
       "       {'title': 'TyngJiunKuo/deep-learning-project',\n",
       "        'url': 'https://github.com/TyngJiunKuo/deep-learning-project'},\n",
       "       {'title': 'iwzy7071/graph_neural_network',\n",
       "        'url': 'https://github.com/iwzy7071/graph_neural_network'},\n",
       "       {'title': 'PumpkinYing/GAT',\n",
       "        'url': 'https://github.com/PumpkinYing/GAT'},\n",
       "       {'title': 'anish-lu-yihe/SVRT-by-GAT',\n",
       "        'url': 'https://github.com/anish-lu-yihe/SVRT-by-GAT'},\n",
       "       {'title': 'ChengSashankh/trying-GAT',\n",
       "        'url': 'https://github.com/ChengSashankh/trying-GAT'},\n",
       "       {'title': 'subercui/pyGConvAT',\n",
       "        'url': 'https://github.com/subercui/pyGConvAT'},\n",
       "       {'title': 'ChengyuSun/gat', 'url': 'https://github.com/ChengyuSun/gat'},\n",
       "       {'title': 'blueberryc/pyGAT',\n",
       "        'url': 'https://github.com/blueberryc/pyGAT'},\n",
       "       {'title': 'zhangbo2008/GAT_network',\n",
       "        'url': 'https://github.com/zhangbo2008/GAT_network'},\n",
       "       {'title': 'whut2962575697/gat_sementic_segmentation',\n",
       "        'url': 'https://github.com/whut2962575697/gat_sementic_segmentation'},\n",
       "       {'title': 'RJ12138/Multilevel',\n",
       "        'url': 'https://github.com/RJ12138/Multilevel'},\n",
       "       {'title': 'galkampel/HyperNetworks',\n",
       "        'url': 'https://github.com/galkampel/HyperNetworks'},\n",
       "       {'title': 'tlmakinen/GAT-noise',\n",
       "        'url': 'https://github.com/tlmakinen/GAT-noise'},\n",
       "       {'title': 'Anak2016/GAT', 'url': 'https://github.com/Anak2016/GAT'},\n",
       "       {'title': 'snownus/COOP', 'url': 'https://github.com/snownus/COOP'},\n",
       "       {'title': 'ColdenChan/GAT_D',\n",
       "        'url': 'https://github.com/ColdenChan/GAT_D'}],\n",
       "      'metrics': {'10%': '58.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GAT',\n",
       "      'paper_date': '2017-10-30',\n",
       "      'paper_title': 'Graph Attention Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1710.10903v3',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'Varying-view RGB-D Action-Skeleton',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-varying'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy (CS)',\n",
       "     'Accuracy (CV I)',\n",
       "     'Accuracy (CV II)',\n",
       "     'Accuracy (AV I)',\n",
       "     'Accuracy (AV II)'],\n",
       "    'rows': [{'code_links': [],\n",
       "      'metrics': {'Accuracy (AV I)': '57%',\n",
       "       'Accuracy (AV II)': '75%',\n",
       "       'Accuracy (CS)': '76%',\n",
       "       'Accuracy (CV I)': '29%',\n",
       "       'Accuracy (CV II)': '71%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VS-CNN',\n",
       "      'paper_date': '2019-04-24',\n",
       "      'paper_title': 'A Large-scale Varying-view RGB-D Action Dataset for Arbitrary-view Human Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1904.10681v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'open-mmlab/mmskeleton',\n",
       "        'url': 'https://github.com/open-mmlab/mmskeleton'},\n",
       "       {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'},\n",
       "       {'title': 'ericksiavichay/cs230-final-project',\n",
       "        'url': 'https://github.com/ericksiavichay/cs230-final-project'},\n",
       "       {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'},\n",
       "       {'title': 'stillarrow/S2VT_ACT',\n",
       "        'url': 'https://github.com/stillarrow/S2VT_ACT'},\n",
       "       {'title': 'ZhangNYG/ST-GCN',\n",
       "        'url': 'https://github.com/ZhangNYG/ST-GCN'},\n",
       "       {'title': 'DixinFan/st-gcn',\n",
       "        'url': 'https://github.com/DixinFan/st-gcn'},\n",
       "       {'title': 'ken724049/action-recognition',\n",
       "        'url': 'https://github.com/ken724049/action-recognition'},\n",
       "       {'title': 'antoniolq/st-gcn',\n",
       "        'url': 'https://github.com/antoniolq/st-gcn'},\n",
       "       {'title': 'github-zbx/ST-GCN',\n",
       "        'url': 'https://github.com/github-zbx/ST-GCN'},\n",
       "       {'title': 'GeyuanZhang/st-gcn-master',\n",
       "        'url': 'https://github.com/GeyuanZhang/st-gcn-master'},\n",
       "       {'title': 'KrisLee512/ST-GCN',\n",
       "        'url': 'https://github.com/KrisLee512/ST-GCN'},\n",
       "       {'title': 'AbiterVX/ST-GCN',\n",
       "        'url': 'https://github.com/AbiterVX/ST-GCN'}],\n",
       "      'metrics': {'Accuracy (AV I)': '53%',\n",
       "       'Accuracy (AV II)': '43%',\n",
       "       'Accuracy (CS)': '71%',\n",
       "       'Accuracy (CV I)': '25%',\n",
       "       'Accuracy (CV II)': '56%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-GCN',\n",
       "      'paper_date': '2018-01-23',\n",
       "      'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1801.07455v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition',\n",
       "        'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}],\n",
       "      'metrics': {'Accuracy (AV I)': '48%',\n",
       "       'Accuracy (AV II)': '68%',\n",
       "       'Accuracy (CS)': '63%',\n",
       "       'Accuracy (CV I)': '14%',\n",
       "       'Accuracy (CV II)': '48%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Res-TCN',\n",
       "      'paper_date': '2017-04-14',\n",
       "      'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.04516v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "        'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "      'metrics': {'Accuracy (AV I)': '33%',\n",
       "       'Accuracy (AV II)': '50%',\n",
       "       'Accuracy (CS)': '60%',\n",
       "       'Accuracy (CV I)': '13%',\n",
       "       'Accuracy (CV II)': '33%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'P-LSTM',\n",
       "      'paper_date': '2016-04-11',\n",
       "      'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "      'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (AV I)': '43%',\n",
       "       'Accuracy (AV II)': '77%',\n",
       "       'Accuracy (CS)': '59%',\n",
       "       'Accuracy (CV I)': '26%',\n",
       "       'Accuracy (CV II)': '68%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SK-CNN',\n",
       "      'paper_date': '2017-08-01',\n",
       "      'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition',\n",
       "      'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'coderSkyChen/Action_Recognition_Zoo',\n",
       "        'url': 'https://github.com/coderSkyChen/Action_Recognition_Zoo'},\n",
       "       {'title': 'colincsl/TemporalConvolutionalNetworks',\n",
       "        'url': 'https://github.com/colincsl/TemporalConvolutionalNetworks'},\n",
       "       {'title': 'yz-cnsdqz/TemporalActionParsing-FineGrained',\n",
       "        'url': 'https://github.com/yz-cnsdqz/TemporalActionParsing-FineGrained'},\n",
       "       {'title': 'sadari1/TumorDetectionDeepLearning',\n",
       "        'url': 'https://github.com/sadari1/TumorDetectionDeepLearning'},\n",
       "       {'title': 'BehnooshParsa/HumanActionRecognition_with_ErgonomicRisk',\n",
       "        'url': 'https://github.com/BehnooshParsa/HumanActionRecognition_with_ErgonomicRisk'}],\n",
       "      'metrics': {'Accuracy (AV I)': '43%',\n",
       "       'Accuracy (AV II)': '64%',\n",
       "       'Accuracy (CS)': '56%',\n",
       "       'Accuracy (CV I)': '16%',\n",
       "       'Accuracy (CV II)': '43%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'TCN',\n",
       "      'paper_date': '2016-11-16',\n",
       "      'paper_title': 'Temporal Convolutional Networks for Action Segmentation and Detection',\n",
       "      'paper_url': 'http://arxiv.org/abs/1611.05267v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "        'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "      'metrics': {'Accuracy (AV I)': '31%',\n",
       "       'Accuracy (AV II)': '68%',\n",
       "       'Accuracy (CS)': '56%',\n",
       "       'Accuracy (CV I)': '16%',\n",
       "       'Accuracy (CV II)': '31%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'LSTM',\n",
       "      'paper_date': '2016-04-11',\n",
       "      'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "      'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'Florence 3D',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-florence'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [],\n",
       "      'metrics': {'Accuracy': '99.1%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Deep STGC_K',\n",
       "      'paper_date': '2018-02-27',\n",
       "      'paper_title': 'Spatio-Temporal Graph Convolution for Skeleton Based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1802.09834v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '98.4%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Complete GR-GCN',\n",
       "      'paper_date': '2018-11-29',\n",
       "      'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "      'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "        'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "      'metrics': {'Accuracy': '95.81%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Temporal Spectral Clustering + Temporal Subspace Clustering',\n",
       "      'paper_date': '2020-06-21',\n",
       "      'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "      'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '91.40%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Rolling Rotations (FTP)',\n",
       "      'paper_date': '2016-06-27',\n",
       "      'paper_title': 'Rolling Rotations for Recognizing Human Actions from 3D Skeletal Data',\n",
       "      'paper_url': 'https://doi.org/10.1109/CVPR.2016.484',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'fdsa1860/skeletal',\n",
       "        'url': 'https://github.com/fdsa1860/skeletal'}],\n",
       "      'metrics': {'Accuracy': '90.9%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Lie Group',\n",
       "      'paper_date': '2014-06-23',\n",
       "      'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group',\n",
       "      'paper_url': 'https://doi.org/10.1109/CVPR.2014.82',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'NTU60-X',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu60-x'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy (Body + Fingers joints)',\n",
       "     'Accuracy (Body joints)',\n",
       "     'Accuracy (Body + Fingers + Face joints)'],\n",
       "    'rows': [{'code_links': [{'title': 'skelemoa/ntu-x',\n",
       "        'url': 'https://github.com/skelemoa/ntu-x'}],\n",
       "      'metrics': {'Accuracy (Body + Fingers + Face joints)': '89.64',\n",
       "       'Accuracy (Body + Fingers joints)': '91.78',\n",
       "       'Accuracy (Body joints)': '89.56'},\n",
       "      'model_links': [],\n",
       "      'model_name': '4s-ShiftGCN',\n",
       "      'paper_date': '2021-01-27',\n",
       "      'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions',\n",
       "      'paper_url': 'https://arxiv.org/abs/2101.11529v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'skelemoa/ntu-x',\n",
       "        'url': 'https://github.com/skelemoa/ntu-x'}],\n",
       "      'metrics': {'Accuracy (Body + Fingers + Face joints)': '91.12',\n",
       "       'Accuracy (Body + Fingers joints)': '91.76',\n",
       "       'Accuracy (Body joints)': '91.26'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MS-G3D',\n",
       "      'paper_date': '2021-01-27',\n",
       "      'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions',\n",
       "      'paper_url': 'https://arxiv.org/abs/2101.11529v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'skelemoa/ntu-x',\n",
       "        'url': 'https://github.com/skelemoa/ntu-x'}],\n",
       "      'metrics': {'Accuracy (Body + Fingers + Face joints)': '89.79',\n",
       "       'Accuracy (Body + Fingers joints)': '91.64',\n",
       "       'Accuracy (Body joints)': '89.98'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PA-ResGCN',\n",
       "      'paper_date': '2021-01-27',\n",
       "      'paper_title': 'NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions',\n",
       "      'paper_url': 'https://arxiv.org/abs/2101.11529v2',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'TCG-dataset',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-tcg'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Acc'],\n",
       "    'rows': [{'code_links': [{'title': 'againerju/tcg_recognition',\n",
       "        'url': 'https://github.com/againerju/tcg_recognition'}],\n",
       "      'metrics': {'Acc': '87.24'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Bidirectional LSTM',\n",
       "      'paper_date': '2020-07-31',\n",
       "      'paper_title': 'Traffic Control Gesture Recognition for Autonomous Vehicles',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.16072v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'SHREC 2017 track on 3D Hand Gesture Recognition',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-shrec'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['28 gestures accuracy',\n",
       "     'Accuracy',\n",
       "     '14 gestures accuracy',\n",
       "     'No. parameters',\n",
       "     'Speed  (FPS)'],\n",
       "    'rows': [{'code_links': [{'title': 'fandulu/DD-Net',\n",
       "        'url': 'https://github.com/fandulu/DD-Net'},\n",
       "       {'title': 'BlurryLight/DD-Net-Pytorch',\n",
       "        'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}],\n",
       "      'metrics': {'14 gestures accuracy': '94.6',\n",
       "       '28 gestures accuracy': '91.9',\n",
       "       'Accuracy': '94.6 (14  gestures) , 91.9 (28 gestures )',\n",
       "       'No. parameters': '1.82M',\n",
       "       'Speed  (FPS)': '2,200'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DD-Net',\n",
       "      'paper_date': '2019-07-23',\n",
       "      'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better',\n",
       "      'paper_url': 'https://arxiv.org/abs/1907.09658v8',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition',\n",
       "        'url': 'https://github.com/AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition'}],\n",
       "      'metrics': {'14 gestures accuracy': '93.57',\n",
       "       '28 gestures accuracy': '91.43'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'TCN-Summ',\n",
       "      'paper_date': '2021-03-03',\n",
       "      'paper_title': 'Domain and View-point Agnostic Hand Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2103.02303v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'14 gestures accuracy': '93.6',\n",
       "       '28 gestures accuracy': '90.7',\n",
       "       'Speed  (FPS)': '161'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'STA-Res-TCN',\n",
       "      'paper_date': '2019-01-23',\n",
       "      'paper_title': 'Spatial-Temporal Attention Res-TCN for Skeleton-Based Dynamic Hand Gesture Recognition',\n",
       "      'paper_url': 'https://link.springer.com/chapter/10.1007/978-3-030-11024-6_18',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'14 gestures accuracy': '91.3',\n",
       "       '28 gestures accuracy': '86.6',\n",
       "       'Speed  (FPS)': '361'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MFA-Net',\n",
       "      'paper_date': '2019-01-10',\n",
       "      'paper_title': 'Motion feature augmented network for dynamic hand gesture recognition from skeletal data',\n",
       "      'paper_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6359639/',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'NTU RGB+D',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-ntu-rgbd'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy (CV)', 'Accuracy (CS)'],\n",
       "    'rows': [{'code_links': [{'title': 'open-mmlab/mmaction2',\n",
       "        'url': 'https://github.com/open-mmlab/mmaction2'}],\n",
       "      'metrics': {'Accuracy (CS)': '94.1', 'Accuracy (CV)': '97.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PoseC3D (w. HRNet 2D skeleton)',\n",
       "      'paper_date': '2021-04-28',\n",
       "      'paper_title': 'Revisiting Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2104.13586v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '91.0', 'Accuracy (CV)': '96.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MS-AAGCN+TEM',\n",
       "      'paper_date': '2020-03-19',\n",
       "      'paper_title': 'Temporal Extension Module for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.08951v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kchengiva/Shift-GCN',\n",
       "        'url': 'https://github.com/kchengiva/Shift-GCN'}],\n",
       "      'metrics': {'Accuracy (CS)': '90.7', 'Accuracy (CV)': '96.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': '4s Shift-GCN',\n",
       "      'paper_date': '2020-06-01',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '91.7', 'Accuracy (CV)': '96.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'AngNet-JA + BA + JBA + VJBA',\n",
       "      'paper_date': '2021-05-04',\n",
       "      'paper_title': 'Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2105.01563v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'lshiwjx/DSTA-Net',\n",
       "        'url': 'https://github.com/lshiwjx/DSTA-Net'}],\n",
       "      'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DSTA-Net',\n",
       "      'paper_date': '2020-07-07',\n",
       "      'paper_title': 'Decoupled Spatial-Temporal Attention Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.03263v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '90.3', 'Accuracy (CV)': '96.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'CGCN',\n",
       "      'paper_date': '2020-03-06',\n",
       "      'paper_title': 'Centrality Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.03007v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '90.1', 'Accuracy (CV)': '96.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Sym-GNN',\n",
       "      'paper_date': '2019-10-05',\n",
       "      'paper_title': 'Symbiotic Graph Neural Networks for 3D Skeleton-based Human Action Recognition and Motion Prediction',\n",
       "      'paper_url': 'https://arxiv.org/abs/1910.02212v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '90.3', 'Accuracy (CV)': '96.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'BAGCN',\n",
       "      'paper_date': '2019-12-24',\n",
       "      'paper_title': 'Focusing and Diffusion: Bidirectional Attentive Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.11521v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '90.2', 'Accuracy (CV)': '96.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'FGCN-spatial+FGCN-motion',\n",
       "      'paper_date': '2020-03-17',\n",
       "      'paper_title': 'Feedback Graph Convolutional Network for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.07564v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kenziyuliu/ms-g3d',\n",
       "        'url': 'https://github.com/kenziyuliu/ms-g3d'}],\n",
       "      'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MS-G3D Net',\n",
       "      'paper_date': '2020-03-31',\n",
       "      'paper_title': 'Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.14111v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "        'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy (CS)': '90.0', 'Accuracy (CV)': '96.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MS-AAGCN',\n",
       "      'paper_date': '2019-12-15',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.06971v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch',\n",
       "        'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}],\n",
       "      'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '96.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DGNN',\n",
       "      'paper_date': '2019-06-01',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'Chiaraplizz/ST-TR',\n",
       "        'url': 'https://github.com/Chiaraplizz/ST-TR'}],\n",
       "      'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '96.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-TR',\n",
       "      'paper_date': '2020-08-17',\n",
       "      'paper_title': 'Skeleton-based Action Recognition via Spatial and Temporal Transformer Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/2008.07404v4',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '91.5', 'Accuracy (CV)': '96.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Dynamic GCN',\n",
       "      'paper_date': '2020-07-29',\n",
       "      'paper_title': 'Dynamic GCN: Context-enriched Topology Learning for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.14690v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/ResGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/ResGCNv1'},\n",
       "       {'title': 'yfsong0709/EfficientGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "      'metrics': {'Accuracy (CS)': '90.9', 'Accuracy (CV)': '96'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PA-ResGCN-B19',\n",
       "      'paper_date': '2020-10-20',\n",
       "      'paper_title': 'Stronger, Faster and More Explainable: A Graph Convolutional Baseline for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2010.09978v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '89.7', 'Accuracy (CV)': '96'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Mix-Dimension',\n",
       "      'paper_date': '2020-07-30',\n",
       "      'paper_title': 'Mix Dimension in Poincar√© Geometry for 3D Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.15678v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "        'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '96.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'JB-AAGCN',\n",
       "      'paper_date': '2019-12-15',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition with Multi-Stream Adaptive Graph Convolutional Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.06971v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '89.58', 'Accuracy (CV)': '95.74'},\n",
       "      'model_links': [],\n",
       "      'model_name': '2s-SDGCN',\n",
       "      'paper_date': '2019-10-27',\n",
       "      'paper_title': 'Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_ICCVW_2019/html/SGRL/Wu_Spatial_Residual_Layer_and_Dense_Connection_Block_Enhanced_Spatial_Temporal_ICCVW_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "      'metrics': {'Accuracy (CS)': '91.7', 'Accuracy (CV)': '95.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EfficientGCN-B4',\n",
       "      'paper_date': '2021-06-29',\n",
       "      'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'xiaoiker/GCN-NAS',\n",
       "        'url': 'https://github.com/xiaoiker/GCN-NAS'}],\n",
       "      'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '95.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GCN-NAS',\n",
       "      'paper_date': '2019-11-11',\n",
       "      'paper_title': 'Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching',\n",
       "      'paper_url': 'https://arxiv.org/abs/1911.04131v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "      'metrics': {'Accuracy (CS)': '90.9', 'Accuracy (CV)': '95.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EfficientGCN-B2',\n",
       "      'paper_date': '2021-06-29',\n",
       "      'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal',\n",
       "        'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'},\n",
       "       {'title': 'lshiwjx/2s-AGCN',\n",
       "        'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy (CS)': '88.5', 'Accuracy (CV)': '95.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': '2s-AGCN',\n",
       "      'paper_date': '2018-05-20',\n",
       "      'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1805.07694v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'lshiwjx/2s-AGCN',\n",
       "        'url': 'https://github.com/lshiwjx/2s-AGCN'}],\n",
       "      'metrics': {'Accuracy (CS)': '88.5', 'Accuracy (CV)': '95.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': '2s-NLGCN',\n",
       "      'paper_date': '2019-07-04',\n",
       "      'paper_title': 'Non-Local Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1805.07694v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "        'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Accuracy (CS)': '89.4', 'Accuracy (CV)': '95.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VA-fusion (aug.)',\n",
       "      'paper_date': '2018-04-20',\n",
       "      'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '89.2', 'Accuracy (CV)': '95.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'AGC-LSTM (Joint&Part)',\n",
       "      'paper_date': '2019-02-25',\n",
       "      'paper_title': 'An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1902.09130v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '89.1', 'Accuracy (CV)': '94.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SLnL-rFA',\n",
       "      'paper_date': '2018-11-10',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition with Synchronous Local and Non-local Spatio-temporal Learning and Frequency Attention',\n",
       "      'paper_url': 'https://arxiv.org/abs/1811.04237v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/EfficientGCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/EfficientGCNv1'}],\n",
       "      'metrics': {'Accuracy (CS)': '89.9', 'Accuracy (CV)': '94.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EfficientGCN-B0',\n",
       "      'paper_date': '2021-06-29',\n",
       "      'paper_title': 'Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2106.15125v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '87.5', 'Accuracy (CV)': '94.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GR-GCN',\n",
       "      'paper_date': '2018-11-29',\n",
       "      'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "      'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'limaosen0/AS-GCN',\n",
       "        'url': 'https://github.com/limaosen0/AS-GCN'}],\n",
       "      'metrics': {'Accuracy (CS)': '86.8', 'Accuracy (CV)': '94.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'AS-GCN',\n",
       "      'paper_date': '2019-04-26',\n",
       "      'paper_title': 'Actional-Structural Graph Convolutional Networks for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1904.12659v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '86.2', 'Accuracy (CV)': '94.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Sem-GCN',\n",
       "      'paper_date': '2020-05-01',\n",
       "      'paper_title': 'A Semantics-Guided Graph Convolutional Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://doi.org/10.1145/3390557.3394129',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'Sunnydreamrain/IndRNN_pytorch',\n",
       "        'url': 'https://github.com/Sunnydreamrain/IndRNN_pytorch'}],\n",
       "      'metrics': {'Accuracy (CS)': '86.70', 'Accuracy (CV)': '93.97'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Dense IndRNN',\n",
       "      'paper_date': '2019-10-11',\n",
       "      'paper_title': 'Deep Independently Recurrent Neural Network (IndRNN)',\n",
       "      'paper_url': 'https://arxiv.org/abs/1910.06251v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '88.6', 'Accuracy (CV)': '93.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': '3SCNN',\n",
       "      'paper_date': '2019-06-16',\n",
       "      'paper_title': 'Three-Stream Convolutional Neural Network With Multi-Task and Ensemble Learning for 3D Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPRW_2019/html/PBVS/Liang_Three-Stream_Convolutional_Neural_Network_With_Multi-Task_and_Ensemble_Learning_for_CVPRW_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '88.0', 'Accuracy (CV)': '93.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PGCN-TCA',\n",
       "      'paper_date': '2020-01-06',\n",
       "      'paper_title': 'PGCN-TCA: Pseudo Graph Convolutional Network With Temporal and Channel-Wise Attention for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/ACCESS.2020.2964115',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/RA-GCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/RA-GCNv1'},\n",
       "       {'title': 'yfsong0709/RA-GCNv2',\n",
       "        'url': 'https://github.com/yfsong0709/RA-GCNv2'},\n",
       "       {'title': 'peter-yys-yoon/pegcnv2',\n",
       "        'url': 'https://github.com/peter-yys-yoon/pegcnv2'}],\n",
       "      'metrics': {'Accuracy (CS)': '87.3', 'Accuracy (CV)': '93.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': '3s RA-GCN',\n",
       "      'paper_date': '2020-08-09',\n",
       "      'paper_title': 'Richly Activated Graph Convolutional Network for Robust Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2008.03791v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/RA-GCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/RA-GCNv1'},\n",
       "       {'title': 'yfsong0709/RA-GCNv2',\n",
       "        'url': 'https://github.com/yfsong0709/RA-GCNv2'},\n",
       "       {'title': 'peter-yys-yoon/pegcnv2',\n",
       "        'url': 'https://github.com/peter-yys-yoon/pegcnv2'}],\n",
       "      'metrics': {'Accuracy (CS)': '85.9', 'Accuracy (CV)': '93.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': '3s RA-GCN',\n",
       "      'paper_date': '2019-05-16',\n",
       "      'paper_title': 'Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons',\n",
       "      'paper_url': 'https://arxiv.org/abs/1905.06774v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'microsoft/SGN',\n",
       "        'url': 'https://github.com/microsoft/SGN'}],\n",
       "      'metrics': {'Accuracy (CS)': '86.6', 'Accuracy (CV)': '93.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SGN',\n",
       "      'paper_date': '2019-04-02',\n",
       "      'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1904.01189v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'andreYoo/PeGCNs',\n",
       "        'url': 'https://github.com/andreYoo/PeGCNs'}],\n",
       "      'metrics': {'Accuracy (CS)': '85.6', 'Accuracy (CV)': '93.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PeGCN',\n",
       "      'paper_date': '2020-03-17',\n",
       "      'paper_title': 'Predictively Encoded Graph Convolutional Network for Noise-Robust Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2003.07514v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'memory-attention-networks/MANs',\n",
       "        'url': 'https://github.com/memory-attention-networks/MANs'}],\n",
       "      'metrics': {'Accuracy (CS)': '82.67', 'Accuracy (CV)': '93.22'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MANs (DenseNet-161)',\n",
       "      'paper_date': '2018-04-23',\n",
       "      'paper_title': 'Memory Attention Networks for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1804.08254v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kalpitthakkar/pb-gcn',\n",
       "        'url': 'https://github.com/kalpitthakkar/pb-gcn'}],\n",
       "      'metrics': {'Accuracy (CS)': '87.5', 'Accuracy (CV)': '93.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PB-GCN',\n",
       "      'paper_date': '2018-09-13',\n",
       "      'paper_title': 'Part-based Graph Convolutional Network for Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1809.04983v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '85.1', 'Accuracy (CV)': '93.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'AR-GCN',\n",
       "      'paper_date': '2019-11-27',\n",
       "      'paper_title': 'An Attention-Enhanced Recurrent Graph Convolutional Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://doi.org/10.1145/3372806.3372814',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'yfsong0709/RA-GCNv1',\n",
       "        'url': 'https://github.com/yfsong0709/RA-GCNv1'},\n",
       "       {'title': 'yfsong0709/RA-GCNv2',\n",
       "        'url': 'https://github.com/yfsong0709/RA-GCNv2'},\n",
       "       {'title': 'peter-yys-yoon/pegcnv2',\n",
       "        'url': 'https://github.com/peter-yys-yoon/pegcnv2'}],\n",
       "      'metrics': {'Accuracy (CS)': '85.8', 'Accuracy (CV)': '93.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': '2s RA-GCN',\n",
       "      'paper_date': '2019-05-16',\n",
       "      'paper_title': 'Richly Activated Graph Convolutional Network for Action Recognition with Incomplete Skeletons',\n",
       "      'paper_url': 'https://arxiv.org/abs/1905.06774v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '85.3', 'Accuracy (CV)': '92.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GVFE+ AS-GCN with DH-TCN',\n",
       "      'paper_date': '2019-12-20',\n",
       "      'paper_title': 'Vertex Feature Encoding and Hierarchical Temporal Modeling in a Spatial-Temporal Graph Convolutional Network for Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.09745v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '87.2', 'Accuracy (CV)': '92.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'TS-SAN',\n",
       "      'paper_date': '2019-12-18',\n",
       "      'paper_title': 'Self-Attention Network for Skeleton-based Human Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.08435v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '84.8', 'Accuracy (CV)': '92.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SR-TSL',\n",
       "      'paper_date': '2018-05-07',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning',\n",
       "      'paper_url': 'http://arxiv.org/abs/1805.02335v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '85.0', 'Accuracy (CV)': '92.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': '3scale ResNet152',\n",
       "      'paper_date': '2017-04-19',\n",
       "      'paper_title': 'Skeleton based action recognition using translation-scale invariant image mapping and multi-scale deep cnn',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.05645v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '85.2', 'Accuracy (CV)': '91.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PR-GCN',\n",
       "      'paper_date': '2020-10-14',\n",
       "      'paper_title': 'Pose Refinement Graph Convolutional Network for Skeleton-based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2010.07367v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '86.8', 'Accuracy (CV)': '91.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'RF-Action',\n",
       "      'paper_date': '2019-09-20',\n",
       "      'paper_title': 'Making the Invisible Visible: Action Recognition Through Walls and Occlusions',\n",
       "      'paper_url': 'https://arxiv.org/abs/1909.09300v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'huguyuehuhu/HCN-pytorch',\n",
       "        'url': 'https://github.com/huguyuehuhu/HCN-pytorch'},\n",
       "       {'title': 'fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition',\n",
       "        'url': 'https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition'},\n",
       "       {'title': 'maxstrobel/HCN-PrototypeLoss-PyTorch',\n",
       "        'url': 'https://github.com/maxstrobel/HCN-PrototypeLoss-PyTorch'},\n",
       "       {'title': 'hhe-distance/AIF-CNN',\n",
       "        'url': 'https://github.com/hhe-distance/AIF-CNN'},\n",
       "       {'title': 'natepuppy/HCN-pytorch',\n",
       "        'url': 'https://github.com/natepuppy/HCN-pytorch'}],\n",
       "      'metrics': {'Accuracy (CS)': '86.5', 'Accuracy (CV)': '91.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'HCN',\n",
       "      'paper_date': '2018-04-17',\n",
       "      'paper_title': 'Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation',\n",
       "      'paper_url': 'http://arxiv.org/abs/1804.06055v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '82.83', 'Accuracy (CV)': '90.05'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'FO-GASTM',\n",
       "      'paper_date': '2019-07-08',\n",
       "      'paper_title': 'Learning Shape-Motion Representations from Geometric Algebra Spatio-Temporal Model for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/ICME.2019.00187',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '83.5', 'Accuracy (CV)': '89.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DPRL',\n",
       "      'paper_date': '2018-06-01',\n",
       "      'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'magnux/DMNN',\n",
       "        'url': 'https://github.com/magnux/DMNN'}],\n",
       "      'metrics': {'Accuracy (CS)': '82.0', 'Accuracy (CV)': '89.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DM-3DCNN',\n",
       "      'paper_date': '2017-10-23',\n",
       "      'paper_title': '3D CNNs on Distance Matrices for Human Action Recognition',\n",
       "      'paper_url': 'https://doi.org/10.1145/3123266.3123299',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '83.2', 'Accuracy (CV)': '89.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'CNN+Motion+Trans',\n",
       "      'paper_date': '2017-04-25',\n",
       "      'paper_title': 'Skeleton-based Action Recognition with Convolutional Neural Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.07595v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '84.23', 'Accuracy (CV)': '89.27'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'RGB+Skeleton (cross-attention)',\n",
       "      'paper_date': '2020-01-20',\n",
       "      'paper_title': 'Context-Aware Cross-Attention for Skeleton-Based Human Action Recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/ACCESS.2020.2968054',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '81.8', 'Accuracy (CV)': '89.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Bayesian GC-LSTM',\n",
       "      'paper_date': '2019-10-01',\n",
       "      'paper_title': 'Bayesian Graph Convolution LSTM for Skeleton Based Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_ICCV_2019/html/Zhao_Bayesian_Graph_Convolution_LSTM_for_Skeleton_Based_Action_Recognition_ICCV_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '83.36', 'Accuracy (CV)': '88.84'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-GCN-jpd',\n",
       "      'paper_date': '2019-06-24',\n",
       "      'paper_title': 'A Comparative Review of Recent Kinect-based Action Recognition Algorithms',\n",
       "      'paper_url': 'https://arxiv.org/abs/1906.09955v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '80.7', 'Accuracy (CV)': '88.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ARRN-LSTM',\n",
       "      'paper_date': '2018-05-07',\n",
       "      'paper_title': 'Relational Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1805.02556v4',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '80.7', 'Accuracy (CV)': '88.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EleAtt-GRU (aug.)',\n",
       "      'paper_date': '2019-09-03',\n",
       "      'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks',\n",
       "      'paper_url': 'https://arxiv.org/abs/1909.01939v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'open-mmlab/mmskeleton',\n",
       "        'url': 'https://github.com/open-mmlab/mmskeleton'},\n",
       "       {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'},\n",
       "       {'title': 'ericksiavichay/cs230-final-project',\n",
       "        'url': 'https://github.com/ericksiavichay/cs230-final-project'},\n",
       "       {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'},\n",
       "       {'title': 'stillarrow/S2VT_ACT',\n",
       "        'url': 'https://github.com/stillarrow/S2VT_ACT'},\n",
       "       {'title': 'ZhangNYG/ST-GCN',\n",
       "        'url': 'https://github.com/ZhangNYG/ST-GCN'},\n",
       "       {'title': 'DixinFan/st-gcn',\n",
       "        'url': 'https://github.com/DixinFan/st-gcn'},\n",
       "       {'title': 'ken724049/action-recognition',\n",
       "        'url': 'https://github.com/ken724049/action-recognition'},\n",
       "       {'title': 'antoniolq/st-gcn',\n",
       "        'url': 'https://github.com/antoniolq/st-gcn'},\n",
       "       {'title': 'github-zbx/ST-GCN',\n",
       "        'url': 'https://github.com/github-zbx/ST-GCN'},\n",
       "       {'title': 'GeyuanZhang/st-gcn-master',\n",
       "        'url': 'https://github.com/GeyuanZhang/st-gcn-master'},\n",
       "       {'title': 'KrisLee512/ST-GCN',\n",
       "        'url': 'https://github.com/KrisLee512/ST-GCN'},\n",
       "       {'title': 'AbiterVX/ST-GCN',\n",
       "        'url': 'https://github.com/AbiterVX/ST-GCN'}],\n",
       "      'metrics': {'Accuracy (CS)': '81.5', 'Accuracy (CV)': '88.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-GCN',\n",
       "      'paper_date': '2018-01-23',\n",
       "      'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1801.07455v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'TobiasLee/Text-Classification',\n",
       "        'url': 'https://github.com/TobiasLee/Text-Classification'},\n",
       "       {'title': 'batzner/indrnn', 'url': 'https://github.com/batzner/indrnn'},\n",
       "       {'title': 'lmnt-com/haste', 'url': 'https://github.com/lmnt-com/haste'},\n",
       "       {'title': 'StefOe/indrnn-pytorch',\n",
       "        'url': 'https://github.com/StefOe/indrnn-pytorch'},\n",
       "       {'title': 'Sunnydreamrain/IndRNN_pytorch',\n",
       "        'url': 'https://github.com/Sunnydreamrain/IndRNN_pytorch'},\n",
       "       {'title': 'Sunnydreamrain/IndRNN_Theano_Lasagne',\n",
       "        'url': 'https://github.com/Sunnydreamrain/IndRNN_Theano_Lasagne'},\n",
       "       {'title': 'trevor-richardson/rnn_zoo',\n",
       "        'url': 'https://github.com/trevor-richardson/rnn_zoo'},\n",
       "       {'title': 'amcs1729/Predicting-cloud-CPU-usage-on-Azure-data',\n",
       "        'url': 'https://github.com/amcs1729/Predicting-cloud-CPU-usage-on-Azure-data'},\n",
       "       {'title': 'Sunnydreamrain/IndRNN',\n",
       "        'url': 'https://github.com/Sunnydreamrain/IndRNN'},\n",
       "       {'title': 'secretlyvogon/IndRNNTF',\n",
       "        'url': 'https://github.com/secretlyvogon/IndRNNTF'},\n",
       "       {'title': 'secretlyvogon/Neural-Network-Implementations',\n",
       "        'url': 'https://github.com/secretlyvogon/Neural-Network-Implementations'}],\n",
       "      'metrics': {'Accuracy (CS)': '81.8', 'Accuracy (CV)': '88.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Ind-RNN',\n",
       "      'paper_date': '2018-03-13',\n",
       "      'paper_title': 'Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN',\n",
       "      'paper_url': 'http://arxiv.org/abs/1803.04831v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '79.4', 'Accuracy (CV)': '87.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'VA-LSTM',\n",
       "      'paper_date': '2017-03-24',\n",
       "      'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data',\n",
       "      'paper_url': 'http://arxiv.org/abs/1703.08274v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '80', 'Accuracy (CV)': '87.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Synthesized CNN',\n",
       "      'paper_date': '2017-08-01',\n",
       "      'paper_title': 'Enhanced skeleton visualization for view invariant human action recognition',\n",
       "      'paper_url': 'https://doi.org/10.1016/j.patcog.2017.02.030',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '79.8', 'Accuracy (CV)': '87.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EleAtt-GRU',\n",
       "      'paper_date': '2018-07-12',\n",
       "      'paper_title': 'Adding Attentiveness to the Neurons in Recurrent Neural Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1807.04445v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '80.9', 'Accuracy (CV)': '86.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'HPM_RGB+HPM_3D+Traj',\n",
       "      'paper_date': '2017-07-04',\n",
       "      'paper_title': 'Learning Human Pose Models from Synthesized Data for Robust RGB-D Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1707.00823v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '79.6', 'Accuracy (CV)': '84.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Clips+CNN+MTLN',\n",
       "      'paper_date': '2017-03-09',\n",
       "      'paper_title': 'A New Representation of Skeleton Sequences for 3D Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1703.03492v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "        'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "      'metrics': {'Accuracy (CS)': '76.5', 'Accuracy (CV)': '84.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Skelemotion + Yang et al.',\n",
       "      'paper_date': '2019-07-30',\n",
       "      'paper_title': 'SkeleMotion: A New Representation of Skeleton Joint Sequences Based on Motion Information for 3D Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1907.13025v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '79.6', 'Accuracy (CV)': '84.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'F2CSkeleton',\n",
       "      'paper_date': '2018-05-30',\n",
       "      'paper_title': 'A Fine-to-Coarse Convolutional Neural Network for 3D Human Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1805.11790v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '76.10', 'Accuracy (CV)': '84.00'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GCA-LSTM',\n",
       "      'paper_date': '2017-07-01',\n",
       "      'paper_title': 'Global Context-Aware Attention LSTM Networks for 3D Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '74.6', 'Accuracy (CV)': '83.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'URNN-2L-T',\n",
       "      'paper_date': '2017-10-22',\n",
       "      'paper_title': 'Adaptive RNN Tree for Large-Scale Human Action Recognition',\n",
       "      'paper_url': 'https://ieeexplore.ieee.org/document/8237423',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'TaeSoo-Kim/TCNActionRecognition',\n",
       "        'url': 'https://github.com/TaeSoo-Kim/TCNActionRecognition'}],\n",
       "      'metrics': {'Accuracy (CS)': '74.3', 'Accuracy (CV)': '83.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'TCN',\n",
       "      'paper_date': '2017-04-14',\n",
       "      'paper_title': 'Interpretable 3D Human Action Analysis with Temporal Convolutional Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.04516v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'dzwallkilled/IEforAR',\n",
       "        'url': 'https://github.com/dzwallkilled/IEforAR'}],\n",
       "      'metrics': {'Accuracy (CV)': '82.31'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Five Spatial Skeleton Features',\n",
       "      'paper_date': '2017-05-02',\n",
       "      'paper_title': 'Investigation of Different Skeleton Features for CNN-based 3D Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1705.00835v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '74.60', 'Accuracy (CV)': '81.25'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Ensemble TS-LSTM v2',\n",
       "      'paper_date': '2017-10-01',\n",
       "      'paper_title': 'Ensemble Deep Learning for Skeleton-Based Action Recognition Using Temporal Sliding LSTM Networks',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_iccv_2017/html/Lee_Ensemble_Deep_Learning_ICCV_2017_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '75.9', 'Accuracy (CV)': '81.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'SkeletonNet',\n",
       "      'paper_date': '2017-03-31',\n",
       "      'paper_title': 'Skeletonnet: Mining deep part features for 3-d action recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/LSP.2017.2690339',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '73.4', 'Accuracy (CV)': '81.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'STA-LSTM',\n",
       "      'paper_date': '2016-11-18',\n",
       "      'paper_title': 'An End-to-End Spatio-Temporal Attention Model for Human Action Recognition from Skeleton Data',\n",
       "      'paper_url': 'http://arxiv.org/abs/1611.06067v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'carloscaetano/skeleton-images',\n",
       "        'url': 'https://github.com/carloscaetano/skeleton-images'}],\n",
       "      'metrics': {'Accuracy (CS)': '73.3', 'Accuracy (CV)': '80.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'TSRJI (Late Fusion)',\n",
       "      'paper_date': '2019-09-11',\n",
       "      'paper_title': 'Skeleton Image Representation for 3D Action Recognition based on Tree Structure and Reference Joints',\n",
       "      'paper_url': 'https://arxiv.org/abs/1909.05704v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '71.3', 'Accuracy (CV)': '79.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Two-Stream RNN',\n",
       "      'paper_date': '2017-04-09',\n",
       "      'paper_title': 'Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.02581v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '69.2', 'Accuracy (CV)': '77.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Trust Gate ST-LSTM',\n",
       "      'paper_date': '2016-07-24',\n",
       "      'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '61.70', 'Accuracy (CV)': '75.50'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-LSTM',\n",
       "      'paper_date': '2016-07-24',\n",
       "      'paper_title': 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1607.07043v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '66.8', 'Accuracy (CV)': '72.6'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Two-Stream 3DCNN',\n",
       "      'paper_date': '2017-05-23',\n",
       "      'paper_title': 'Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1705.08106v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "        'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "      'metrics': {'Accuracy (CS)': '62.93', 'Accuracy (CV)': '70.27'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Part-aware LSTM',\n",
       "      'paper_date': '2016-04-11',\n",
       "      'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "      'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "        'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "      'metrics': {'Accuracy (CS)': '60.7', 'Accuracy (CV)': '67.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Deep LSTM',\n",
       "      'paper_date': '2016-04-11',\n",
       "      'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "      'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '60.2', 'Accuracy (CV)': '65.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Dynamic Skeletons',\n",
       "      'paper_date': '2016-12-15',\n",
       "      'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '59.1', 'Accuracy (CV)': '64.0'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'H-RNN',\n",
       "      'paper_date': '2015-06-07',\n",
       "      'paper_title': 'Hierarchical recurrent neural network for skeleton based action recognition',\n",
       "      'paper_url': 'https://doi.org/10.1109/CVPR.2015.7298714',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'fdsa1860/skeletal',\n",
       "        'url': 'https://github.com/fdsa1860/skeletal'}],\n",
       "      'metrics': {'Accuracy (CS)': '50.1', 'Accuracy (CV)': '52.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Lie Group',\n",
       "      'paper_date': '2014-06-23',\n",
       "      'paper_title': 'Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group',\n",
       "      'paper_url': 'https://doi.org/10.1109/CVPR.2014.82',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (CS)': '38.6', 'Accuracy (CV)': '41.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Skeleton Quads',\n",
       "      'paper_date': '2014-08-24',\n",
       "      'paper_title': 'Skeletal quads: Human action recognition using joint quadruples',\n",
       "      'paper_url': 'https://doi.org/10.1109/ICPR.2014.772',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'JHMDB (2D poses only)',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-jhmdb-2d'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Average accuracy of 3 splits',\n",
       "     'Accuracy',\n",
       "     'No. parameters'],\n",
       "    'rows': [{'code_links': [{'title': 'fandulu/DD-Net',\n",
       "        'url': 'https://github.com/fandulu/DD-Net'},\n",
       "       {'title': 'BlurryLight/DD-Net-Pytorch',\n",
       "        'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}],\n",
       "      'metrics': {'Accuracy': '78.0 (average of 3 split train/test)',\n",
       "       'Average accuracy of 3 splits': '77.2',\n",
       "       'No. parameters': '1.82 M'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DD-Net',\n",
       "      'paper_date': '2019-07-23',\n",
       "      'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better',\n",
       "      'paper_url': 'https://arxiv.org/abs/1907.09658v8',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Average accuracy of 3 splits': '67.9',\n",
       "       'No. parameters': '-'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PoTion',\n",
       "      'paper_date': '2018-06-01',\n",
       "      'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'noboevbo/ehpi_action_recognition',\n",
       "        'url': 'https://github.com/noboevbo/ehpi_action_recognition'}],\n",
       "      'metrics': {'Average accuracy of 3 splits': '65.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EHPI',\n",
       "      'paper_date': '2019-04-19',\n",
       "      'paper_title': 'Simple yet efficient real-time pose-based action recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1904.09140v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'mzolfaghari/chained-multistream-networks',\n",
       "        'url': 'https://github.com/mzolfaghari/chained-multistream-networks'}],\n",
       "      'metrics': {'Average accuracy of 3 splits': '56.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Chained',\n",
       "      'paper_date': '2017-04-03',\n",
       "      'paper_title': 'Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.00616v2',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'PKU-MMD',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-pku-mmd'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['mAP@0.50 (CV)', 'mAP@0.50 (CS)'],\n",
       "    'rows': [{'code_links': [],\n",
       "      'metrics': {'mAP@0.50 (CS)': '92.9', 'mAP@0.50 (CV)': '94.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'RF-Action',\n",
       "      'paper_date': '2019-09-20',\n",
       "      'paper_title': 'Making the Invisible Visible: Action Recognition Through Walls and Occlusions',\n",
       "      'paper_url': 'https://arxiv.org/abs/1909.09300v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'huguyuehuhu/HCN-pytorch',\n",
       "        'url': 'https://github.com/huguyuehuhu/HCN-pytorch'},\n",
       "       {'title': 'fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition',\n",
       "        'url': 'https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition'},\n",
       "       {'title': 'maxstrobel/HCN-PrototypeLoss-PyTorch',\n",
       "        'url': 'https://github.com/maxstrobel/HCN-PrototypeLoss-PyTorch'},\n",
       "       {'title': 'hhe-distance/AIF-CNN',\n",
       "        'url': 'https://github.com/hhe-distance/AIF-CNN'},\n",
       "       {'title': 'natepuppy/HCN-pytorch',\n",
       "        'url': 'https://github.com/natepuppy/HCN-pytorch'}],\n",
       "      'metrics': {'mAP@0.50 (CS)': '92.6', 'mAP@0.50 (CV)': '94.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'HCN',\n",
       "      'paper_date': '2018-04-17',\n",
       "      'paper_title': 'Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation',\n",
       "      'paper_url': 'http://arxiv.org/abs/1804.06055v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'mAP@0.50 (CS)': '90.4', 'mAP@0.50 (CV)': '93.7'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Li et al. [[Li et al.2017b]]',\n",
       "      'paper_date': '2017-04-25',\n",
       "      'paper_title': 'Skeleton-based Action Recognition with Convolutional Neural Networks',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.07595v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'Skeletics-152',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy (Cross-Subject)'],\n",
       "    'rows': [{'code_links': [{'title': 'skelemoa/quovadis',\n",
       "        'url': 'https://github.com/skelemoa/quovadis'}],\n",
       "      'metrics': {'Accuracy (Cross-Subject)': '57.01 %'},\n",
       "      'model_links': [],\n",
       "      'model_name': '4s-ShiftGCN',\n",
       "      'paper_date': '2020-07-04',\n",
       "      'paper_title': 'Quo Vadis, Skeleton Action Recognition ?',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.02072v2',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'Skeleton-Mimetics',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-skeleton'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy (%)'],\n",
       "    'rows': [{'code_links': [{'title': 'skelemoa/quovadis',\n",
       "        'url': 'https://github.com/skelemoa/quovadis'}],\n",
       "      'metrics': {'Accuracy (%)': '57.37 %'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MS-G3D',\n",
       "      'paper_date': '2020-07-04',\n",
       "      'paper_title': 'Quo Vadis, Skeleton Action Recognition ?',\n",
       "      'paper_url': 'https://arxiv.org/abs/2007.02072v2',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'MSR Action3D',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msr'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "        'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "      'metrics': {'Accuracy': '88.51%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Temporal K-Means Clustering + Temporal Subspace Clustering',\n",
       "      'paper_date': '2020-06-21',\n",
       "      'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "      'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'rort1989/HDM',\n",
       "        'url': 'https://github.com/rort1989/HDM'}],\n",
       "      'metrics': {'Accuracy': '86.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'HDM-BG',\n",
       "      'paper_date': '2019-06-01',\n",
       "      'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '74'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'GFT',\n",
       "      'paper_date': '2019-08-26',\n",
       "      'paper_title': 'Graph Based Skeleton Modeling for Human Activity Analysis',\n",
       "      'paper_url': 'https://doi.org/10.1109/ICIP.2019.8803186',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'MSR ActionPairs',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-msr-1'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'IIT-PAVIS/subspace-clustering-action-recognition',\n",
       "        'url': 'https://github.com/IIT-PAVIS/subspace-clustering-action-recognition'}],\n",
       "      'metrics': {'Accuracy': '98.02%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Temporal Subspace Clustering',\n",
       "      'paper_date': '2020-06-21',\n",
       "      'paper_title': 'Subspace Clustering for Action Recognition with Covariance Representations and Temporal Pruning',\n",
       "      'paper_url': 'https://arxiv.org/abs/2006.11812v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'CAD-120',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-cad-120'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [],\n",
       "      'metrics': {'Accuracy': '91.1%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'NGM (5-shot)',\n",
       "      'paper_date': '2018-09-01',\n",
       "      'paper_title': 'Neural Graph Matching Networks for Fewshot 3D Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '89.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'All Features (w ground truth)',\n",
       "      'paper_date': '2013-02-01',\n",
       "      'paper_title': 'Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation',\n",
       "      'paper_url': 'http://proceedings.mlr.press/v28/koppula13.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '86.0%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'KGS',\n",
       "      'paper_date': '2012-10-04',\n",
       "      'paper_title': 'Learning Human Activities and Object Affordances from RGB-D Videos',\n",
       "      'paper_url': 'http://arxiv.org/abs/1210.1207v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'asheshjain399/RNNexp',\n",
       "        'url': 'https://github.com/asheshjain399/RNNexp'},\n",
       "       {'title': 'zhaolongkzz/human_motion',\n",
       "        'url': 'https://github.com/zhaolongkzz/human_motion'}],\n",
       "      'metrics': {'Accuracy': '85.4%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'S-RNN (5-shot)',\n",
       "      'paper_date': '2015-11-17',\n",
       "      'paper_title': 'Structural-RNN: Deep Learning on Spatio-Temporal Graphs',\n",
       "      'paper_url': 'http://arxiv.org/abs/1511.05298v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '85.0%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'NGM w/o Edges  (5-shot)',\n",
       "      'paper_date': '2018-09-01',\n",
       "      'paper_title': 'Neural Graph Matching Networks for Fewshot 3D Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_ECCV_2018/html/Michelle_Guo_Neural_Graph_Matching_ECCV_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy': '70.3%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Our DP seg. + moves + heuristic seg.',\n",
       "      'paper_date': '2013-02-01',\n",
       "      'paper_title': 'Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation',\n",
       "      'paper_url': 'http://proceedings.mlr.press/v28/koppula13.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'charlesq34/pointnet',\n",
       "        'url': 'https://github.com/charlesq34/pointnet'},\n",
       "       {'title': 'fxia22/pointnet.pytorch',\n",
       "        'url': 'https://github.com/fxia22/pointnet.pytorch'},\n",
       "       {'title': 'vinits5/learning3d',\n",
       "        'url': 'https://github.com/vinits5/learning3d'},\n",
       "       {'title': 'maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification',\n",
       "        'url': 'https://github.com/maggie0106/Graph-CNN-in-3D-Point-Cloud-Classification'},\n",
       "       {'title': 'DylanWusee/pointconv_pytorch',\n",
       "        'url': 'https://github.com/DylanWusee/pointconv_pytorch'},\n",
       "       {'title': 'nikitakaraevv/pointnet',\n",
       "        'url': 'https://github.com/nikitakaraevv/pointnet'},\n",
       "       {'title': 'princeton-vl/SimpleView',\n",
       "        'url': 'https://github.com/princeton-vl/SimpleView'},\n",
       "       {'title': 'ZhihaoZhu/PointNet-Implementation-Tensorflow',\n",
       "        'url': 'https://github.com/ZhihaoZhu/PointNet-Implementation-Tensorflow'},\n",
       "       {'title': 'sarthakTUM/roofn3d',\n",
       "        'url': 'https://github.com/sarthakTUM/roofn3d'},\n",
       "       {'title': 'ajhamdi/AdvPC', 'url': 'https://github.com/ajhamdi/AdvPC'},\n",
       "       {'title': 'romaintha/pytorch_pointnet',\n",
       "        'url': 'https://github.com/romaintha/pytorch_pointnet'},\n",
       "       {'title': 'AI-Guru/pointcloud_experiments',\n",
       "        'url': 'https://github.com/AI-Guru/pointcloud_experiments'},\n",
       "       {'title': 'YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti',\n",
       "        'url': 'https://github.com/YanWei123/Pytorch-implementation-of-FoldingNet-encoder-and-decoder-with-graph-pooling-covariance-add-quanti'},\n",
       "       {'title': 'YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024',\n",
       "        'url': 'https://github.com/YanWei123/PointNet-encoder-and-FoldingNet-decoder-add-quantization-change-latent-code-size-from-512-to-1024'},\n",
       "       {'title': 'saaries/PointNet',\n",
       "        'url': 'https://github.com/saaries/PointNet'},\n",
       "       {'title': 'y2kmz/pointnetv2',\n",
       "        'url': 'https://github.com/y2kmz/pointnetv2'},\n",
       "       {'title': 'Fragjacker/Pointcloud-grad-CAM',\n",
       "        'url': 'https://github.com/Fragjacker/Pointcloud-grad-CAM'},\n",
       "       {'title': 'abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds',\n",
       "        'url': 'https://github.com/abdullahozer11/Segmentation-and-Classification-of-Objects-in-Point-Clouds'},\n",
       "       {'title': 'donshen/pointnet.phasedetection',\n",
       "        'url': 'https://github.com/donshen/pointnet.phasedetection'},\n",
       "       {'title': 'Yuto0107/pointnet',\n",
       "        'url': 'https://github.com/Yuto0107/pointnet'},\n",
       "       {'title': 'opeco17/pointnet',\n",
       "        'url': 'https://github.com/opeco17/pointnet'},\n",
       "       {'title': 'YanWei123/Pointnet_encoder_Foldingnet_decoder_quantization',\n",
       "        'url': 'https://github.com/YanWei123/Pointnet_encoder_Foldingnet_decoder_quantization'},\n",
       "       {'title': 'yanxp/PointNet', 'url': 'https://github.com/yanxp/PointNet'},\n",
       "       {'title': 'amyllykoski/CycleGAN',\n",
       "        'url': 'https://github.com/amyllykoski/CycleGAN'},\n",
       "       {'title': 'ftdlyc/pointnet_pytorch',\n",
       "        'url': 'https://github.com/ftdlyc/pointnet_pytorch'},\n",
       "       {'title': 'alpemek/ais3d', 'url': 'https://github.com/alpemek/ais3d'},\n",
       "       {'title': 'Dir-b/PointNet', 'url': 'https://github.com/Dir-b/PointNet'},\n",
       "       {'title': 'PaParaZz1/PointNet',\n",
       "        'url': 'https://github.com/PaParaZz1/PointNet'},\n",
       "       {'title': 'witignite/Frustum-PointNet',\n",
       "        'url': 'https://github.com/witignite/Frustum-PointNet'},\n",
       "       {'title': 'sanantoniochili/PointCloud_KNN',\n",
       "        'url': 'https://github.com/sanantoniochili/PointCloud_KNN'},\n",
       "       {'title': 'zgx0534/pointnet_win',\n",
       "        'url': 'https://github.com/zgx0534/pointnet_win'},\n",
       "       {'title': 'KhusDM/PointNetTree',\n",
       "        'url': 'https://github.com/KhusDM/PointNetTree'},\n",
       "       {'title': 'timothylimyl/PointNet-Pytorch',\n",
       "        'url': 'https://github.com/timothylimyl/PointNet-Pytorch'},\n",
       "       {'title': 'minhncedutw/pointnet1_keras',\n",
       "        'url': 'https://github.com/minhncedutw/pointnet1_keras'},\n",
       "       {'title': 'kenakai16/pointconv_pytorch',\n",
       "        'url': 'https://github.com/kenakai16/pointconv_pytorch'},\n",
       "       {'title': 'xurui1217/pointnet.pytorch-master',\n",
       "        'url': 'https://github.com/xurui1217/pointnet.pytorch-master'},\n",
       "       {'title': 'GOD-GOD-Autonomous-Vehicle/self-pointnet',\n",
       "        'url': 'https://github.com/GOD-GOD-Autonomous-Vehicle/self-pointnet'},\n",
       "       {'title': 'Young98CN/pointconv_pytorch',\n",
       "        'url': 'https://github.com/Young98CN/pointconv_pytorch'},\n",
       "       {'title': 'm-117/PointNet-ein-Implementationsbeispiel-mit-Jupyter-Notebooks',\n",
       "        'url': 'https://github.com/m-117/PointNet-ein-Implementationsbeispiel-mit-Jupyter-Notebooks'},\n",
       "       {'title': 'hnVfly/pointnet.mxnet',\n",
       "        'url': 'https://github.com/hnVfly/pointnet.mxnet'},\n",
       "       {'title': 'LebronGG/PointNet',\n",
       "        'url': 'https://github.com/LebronGG/PointNet'},\n",
       "       {'title': 'ytng001/sensemaking',\n",
       "        'url': 'https://github.com/ytng001/sensemaking'},\n",
       "       {'title': 'lingzhang1/pointnet_tensorflow',\n",
       "        'url': 'https://github.com/lingzhang1/pointnet_tensorflow'},\n",
       "       {'title': 'coconutzs/PointNet_zs',\n",
       "        'url': 'https://github.com/coconutzs/PointNet_zs'},\n",
       "       {'title': 'Fnjn/UCSD-CSE-291I',\n",
       "        'url': 'https://github.com/Fnjn/UCSD-CSE-291I'},\n",
       "       {'title': 'aviros/roatationPointnet',\n",
       "        'url': 'https://github.com/aviros/roatationPointnet'},\n",
       "       {'title': 'ShadowShadowWong/update-pointnet-work',\n",
       "        'url': 'https://github.com/ShadowShadowWong/update-pointnet-work'},\n",
       "       {'title': 'Yang2446/pointnet',\n",
       "        'url': 'https://github.com/Yang2446/pointnet'},\n",
       "       {'title': 'dwtstore/sfm1', 'url': 'https://github.com/dwtstore/sfm1'},\n",
       "       {'title': 'ModelBunker/PointNet-TensorFlow',\n",
       "        'url': 'https://github.com/ModelBunker/PointNet-TensorFlow'},\n",
       "       {'title': 'Lw510107/pointnet-2018.6.27-',\n",
       "        'url': 'https://github.com/Lw510107/pointnet-2018.6.27-'},\n",
       "       {'title': 'freddieee/pn_6d_single',\n",
       "        'url': 'https://github.com/freddieee/pn_6d_single'},\n",
       "       {'title': 'mengxingshifen1218/pointnet.pytorch',\n",
       "        'url': 'https://github.com/mengxingshifen1218/pointnet.pytorch'},\n",
       "       {'title': 'LONG-9621/Extract_Point_3D',\n",
       "        'url': 'https://github.com/LONG-9621/Extract_Point_3D'},\n",
       "       {'title': 'lingzhang1/pointnet_pytorch',\n",
       "        'url': 'https://github.com/lingzhang1/pointnet_pytorch'},\n",
       "       {'title': 'THHHomas/mls', 'url': 'https://github.com/THHHomas/mls'},\n",
       "       {'title': 'ahmed-anas/thesis-pointnet',\n",
       "        'url': 'https://github.com/ahmed-anas/thesis-pointnet'},\n",
       "       {'title': 'liuch37/pointnet',\n",
       "        'url': 'https://github.com/liuch37/pointnet'},\n",
       "       {'title': 'CheesyB/cpointnet',\n",
       "        'url': 'https://github.com/CheesyB/cpointnet'},\n",
       "       {'title': 'aviros/pointnet_totations',\n",
       "        'url': 'https://github.com/aviros/pointnet_totations'},\n",
       "       {'title': 'Taeuk-Jang/pointcompletion',\n",
       "        'url': 'https://github.com/Taeuk-Jang/pointcompletion'},\n",
       "       {'title': 'bt77/pointnet', 'url': 'https://github.com/bt77/pointnet'},\n",
       "       {'title': 'yanx27/Pointnet',\n",
       "        'url': 'https://github.com/yanx27/Pointnet'},\n",
       "       {'title': 'monacv/pointnet',\n",
       "        'url': 'https://github.com/monacv/pointnet'},\n",
       "       {'title': 'merazlab/3D_Deep_Learning_Link',\n",
       "        'url': 'https://github.com/merazlab/3D_Deep_Learning_Link'},\n",
       "       {'title': 'ajertec/PointNetKeras',\n",
       "        'url': 'https://github.com/ajertec/PointNetKeras'},\n",
       "       {'title': 'AlfredoZermini/PointNet',\n",
       "        'url': 'https://github.com/AlfredoZermini/PointNet'},\n",
       "       {'title': 'YiruS/pointnet_adversarial',\n",
       "        'url': 'https://github.com/YiruS/pointnet_adversarial'},\n",
       "       {'title': 'wonderland-dsg/pointnet-grid',\n",
       "        'url': 'https://github.com/wonderland-dsg/pointnet-grid'},\n",
       "       {'title': 'KiranAkadas/My_Pointnet_v2',\n",
       "        'url': 'https://github.com/KiranAkadas/My_Pointnet_v2'},\n",
       "       {'title': 'Q-Qgao/pointnet',\n",
       "        'url': 'https://github.com/Q-Qgao/pointnet'},\n",
       "       {'title': 'LONG-9621/PointNet',\n",
       "        'url': 'https://github.com/LONG-9621/PointNet'},\n",
       "       {'title': 'wuryantoAji/POINTNET',\n",
       "        'url': 'https://github.com/wuryantoAji/POINTNET'},\n",
       "       {'title': 'WLK12580/12', 'url': 'https://github.com/WLK12580/12'},\n",
       "       {'title': 'SBPL-Cruz/perch_pose_sampler',\n",
       "        'url': 'https://github.com/SBPL-Cruz/perch_pose_sampler'},\n",
       "       {'title': 'Harut0726/my-pointnet-tensorflow',\n",
       "        'url': 'https://github.com/Harut0726/my-pointnet-tensorflow'},\n",
       "       {'title': 'BPMJG/annotated_pointnet',\n",
       "        'url': 'https://github.com/BPMJG/annotated_pointnet'},\n",
       "       {'title': 'SonuDileep/3-D-Object-Detection-using-PointNet',\n",
       "        'url': 'https://github.com/SonuDileep/3-D-Object-Detection-using-PointNet'},\n",
       "       {'title': 'prasadsawant5/PointNet',\n",
       "        'url': 'https://github.com/prasadsawant5/PointNet'},\n",
       "       {'title': 'zhijie-yang/pointnet.pytorch',\n",
       "        'url': 'https://github.com/zhijie-yang/pointnet.pytorch'},\n",
       "       {'title': 'KaidongLi/tf-3d-alpha',\n",
       "        'url': 'https://github.com/KaidongLi/tf-3d-alpha'},\n",
       "       {'title': 'hz-ants/pointnet.pytorch',\n",
       "        'url': 'https://github.com/hz-ants/pointnet.pytorch'},\n",
       "       {'title': 'AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation',\n",
       "        'url': 'https://github.com/AlvesRobot/Deep-Learning-on-Point-Cloud-for-3D-Classification-and-Segmentation'},\n",
       "       {'title': 'VaanHUANG/CSCI5210HW1',\n",
       "        'url': 'https://github.com/VaanHUANG/CSCI5210HW1'}],\n",
       "      'metrics': {'Accuracy': '69.1%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PointNet (5-shot)',\n",
       "      'paper_date': '2016-12-02',\n",
       "      'paper_title': 'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation',\n",
       "      'paper_url': 'http://arxiv.org/abs/1612.00593v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'shahroudy/NTURGB-D',\n",
       "        'url': 'https://github.com/shahroudy/NTURGB-D'}],\n",
       "      'metrics': {'Accuracy': '68.1%'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'P-LSTM (5-shot)',\n",
       "      'paper_date': '2016-04-11',\n",
       "      'paper_title': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       "      'paper_url': 'http://arxiv.org/abs/1604.02808v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'First-Person Hand Action Benchmark',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-first'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['1:3 Accuracy',\n",
       "     '1:1 Accuracy',\n",
       "     '3:1 Accuracy',\n",
       "     'Cross-person Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition',\n",
       "        'url': 'https://github.com/AlbertoSabater/Domain-and-View-point-Agnostic-Hand-Action-Recognition'}],\n",
       "      'metrics': {'1:1 Accuracy': '95.93',\n",
       "       '1:3 Accuracy': '92.9',\n",
       "       '3:1 Accuracy': '96.76',\n",
       "       'Cross-person Accuracy': '88.70'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'TCN-Summ',\n",
       "      'paper_date': '2021-03-03',\n",
       "      'paper_title': 'Domain and View-point Agnostic Hand Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/2103.02303v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'UAV-Human',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-uav'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Average Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'kchengiva/Shift-GCN',\n",
       "        'url': 'https://github.com/kchengiva/Shift-GCN'}],\n",
       "      'metrics': {'Average Accuracy': '37.98'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Shift-GCN',\n",
       "      'paper_date': '2020-06-01',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition With Shift Graph Convolutional Network',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Skeleton-Based_Action_Recognition_With_Shift_Graph_Convolutional_Network_CVPR_2020_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Average Accuracy': '36.97'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'HARD-Net',\n",
       "      'paper_date': None,\n",
       "      'paper_title': 'HARD-Net: Hardness-AwaRe Discrimination Network for 3D Early Activity Prediction',\n",
       "      'paper_url': 'https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1360_ECCV_2020_paper.php',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'benedekrozemberczki/pytorch_geometric_temporal',\n",
       "        'url': 'https://github.com/benedekrozemberczki/pytorch_geometric_temporal'},\n",
       "       {'title': 'lshiwjx/2s-AGCN',\n",
       "        'url': 'https://github.com/lshiwjx/2s-AGCN'},\n",
       "       {'title': 'iamjeff7/j-va-aagcn',\n",
       "        'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "      'metrics': {'Average Accuracy': '34.84'},\n",
       "      'model_links': [],\n",
       "      'model_name': '2S-AGCN',\n",
       "      'paper_date': '2018-05-20',\n",
       "      'paper_title': 'Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'https://arxiv.org/abs/1805.07694v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'open-mmlab/mmskeleton',\n",
       "        'url': 'https://github.com/open-mmlab/mmskeleton'},\n",
       "       {'title': 'yysijie/st-gcn', 'url': 'https://github.com/yysijie/st-gcn'},\n",
       "       {'title': 'ericksiavichay/cs230-final-project',\n",
       "        'url': 'https://github.com/ericksiavichay/cs230-final-project'},\n",
       "       {'title': 'XinzeWu/st-GCN', 'url': 'https://github.com/XinzeWu/st-GCN'},\n",
       "       {'title': 'stillarrow/S2VT_ACT',\n",
       "        'url': 'https://github.com/stillarrow/S2VT_ACT'},\n",
       "       {'title': 'ZhangNYG/ST-GCN',\n",
       "        'url': 'https://github.com/ZhangNYG/ST-GCN'},\n",
       "       {'title': 'DixinFan/st-gcn',\n",
       "        'url': 'https://github.com/DixinFan/st-gcn'},\n",
       "       {'title': 'ken724049/action-recognition',\n",
       "        'url': 'https://github.com/ken724049/action-recognition'},\n",
       "       {'title': 'antoniolq/st-gcn',\n",
       "        'url': 'https://github.com/antoniolq/st-gcn'},\n",
       "       {'title': 'github-zbx/ST-GCN',\n",
       "        'url': 'https://github.com/github-zbx/ST-GCN'},\n",
       "       {'title': 'GeyuanZhang/st-gcn-master',\n",
       "        'url': 'https://github.com/GeyuanZhang/st-gcn-master'},\n",
       "       {'title': 'KrisLee512/ST-GCN',\n",
       "        'url': 'https://github.com/KrisLee512/ST-GCN'},\n",
       "       {'title': 'AbiterVX/ST-GCN',\n",
       "        'url': 'https://github.com/AbiterVX/ST-GCN'}],\n",
       "      'metrics': {'Average Accuracy': '30.25'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'ST-GCN',\n",
       "      'paper_date': '2018-01-23',\n",
       "      'paper_title': 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1801.07455v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'kenziyuliu/DGNN-PyTorch',\n",
       "        'url': 'https://github.com/kenziyuliu/DGNN-PyTorch'}],\n",
       "      'metrics': {'Average Accuracy': '29.90'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DGNN',\n",
       "      'paper_date': '2019-06-01',\n",
       "      'paper_title': 'Skeleton-Based Action Recognition With Directed Graph Neural Networks',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Skeleton-Based_Action_Recognition_With_Directed_Graph_Neural_Networks_CVPR_2019_paper.html',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'J-HMDB',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-j-hmdb'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy (RGB+pose)', 'Accuracy (pose)'],\n",
       "    'rows': [{'code_links': [],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '90.4', 'Accuracy (pose)': '67.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Potion',\n",
       "      'paper_date': '2018-06-01',\n",
       "      'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '86.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PA3D+RPAN',\n",
       "      'paper_date': '2019-06-01',\n",
       "      'paper_title': 'PA3D: Pose-Action 3D Machine for Video Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '85.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'I3D + Potion',\n",
       "      'paper_date': '2018-06-01',\n",
       "      'paper_title': 'PoTion: Pose MoTion Representation for Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'deepmind/kinetics-i3d',\n",
       "        'url': 'https://github.com/deepmind/kinetics-i3d'},\n",
       "       {'title': 'open-mmlab/mmaction2',\n",
       "        'url': 'https://github.com/open-mmlab/mmaction2'},\n",
       "       {'title': 'piergiaj/pytorch-i3d',\n",
       "        'url': 'https://github.com/piergiaj/pytorch-i3d'},\n",
       "       {'title': 'hassony2/kinetics_i3d_pytorch',\n",
       "        'url': 'https://github.com/hassony2/kinetics_i3d_pytorch'},\n",
       "       {'title': 'yaohungt/GSTEG_CVPR_2019',\n",
       "        'url': 'https://github.com/yaohungt/GSTEG_CVPR_2019'},\n",
       "       {'title': 'dlpbc/keras-kinetics-i3d',\n",
       "        'url': 'https://github.com/dlpbc/keras-kinetics-i3d'},\n",
       "       {'title': 'StanfordVL/RubiksNet',\n",
       "        'url': 'https://github.com/StanfordVL/RubiksNet'},\n",
       "       {'title': 'FrederikSchorr/sign-language',\n",
       "        'url': 'https://github.com/FrederikSchorr/sign-language'},\n",
       "       {'title': 'CMU-CREATE-Lab/deep-smoke-machine',\n",
       "        'url': 'https://github.com/CMU-CREATE-Lab/deep-smoke-machine'},\n",
       "       {'title': 'JeffCHEN2017/WSSTG',\n",
       "        'url': 'https://github.com/JeffCHEN2017/WSSTG'},\n",
       "       {'title': 'OanaIgnat/i3d_keras',\n",
       "        'url': 'https://github.com/OanaIgnat/i3d_keras'},\n",
       "       {'title': 'ahsaniqbal/Kinetics-FeatureExtractor',\n",
       "        'url': 'https://github.com/ahsaniqbal/Kinetics-FeatureExtractor'},\n",
       "       {'title': 'prinshul/GWSDR', 'url': 'https://github.com/prinshul/GWSDR'},\n",
       "       {'title': 'PPPrior/i3d-pytorch',\n",
       "        'url': 'https://github.com/PPPrior/i3d-pytorch'},\n",
       "       {'title': 'sebastiantiesmeyer/deeplabchop3d',\n",
       "        'url': 'https://github.com/sebastiantiesmeyer/deeplabchop3d'},\n",
       "       {'title': 'anonymous-p/Flickering_Adversarial_Video',\n",
       "        'url': 'https://github.com/anonymous-p/Flickering_Adversarial_Video'},\n",
       "       {'title': 'vijayvee/behavior-recognition',\n",
       "        'url': 'https://github.com/vijayvee/behavior-recognition'},\n",
       "       {'title': 'LukasHedegaard/co3d',\n",
       "        'url': 'https://github.com/LukasHedegaard/co3d'},\n",
       "       {'title': 'vijayvee/behavior_recognition',\n",
       "        'url': 'https://github.com/vijayvee/behavior_recognition'},\n",
       "       {'title': 'AbdurrahmanNadi/activity_recognition_web_service',\n",
       "        'url': 'https://github.com/AbdurrahmanNadi/activity_recognition_web_service'},\n",
       "       {'title': 'hjchoi-minds/i3dnia',\n",
       "        'url': 'https://github.com/hjchoi-minds/i3dnia'},\n",
       "       {'title': 'Alexyuda/action_recognition',\n",
       "        'url': 'https://github.com/Alexyuda/action_recognition'},\n",
       "       {'title': 'helloxy96/CS5242_Project2020',\n",
       "        'url': 'https://github.com/helloxy96/CS5242_Project2020'},\n",
       "       {'title': 'ShobhitMaheshwari/sign-language1',\n",
       "        'url': 'https://github.com/ShobhitMaheshwari/sign-language1'},\n",
       "       {'title': 'mHealthBuet/SegCodeNet',\n",
       "        'url': 'https://github.com/mHealthBuet/SegCodeNet'}],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '84.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'I3D',\n",
       "      'paper_date': '2017-05-22',\n",
       "      'paper_title': 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset',\n",
       "      'paper_url': 'http://arxiv.org/abs/1705.07750v3',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'agethen/RPAN',\n",
       "        'url': 'https://github.com/agethen/RPAN'}],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '83.9'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'RPAN',\n",
       "      'paper_date': '2017-10-22',\n",
       "      'paper_title': 'RPAN: An End-to-End Recurrent Pose-Attention Network for Action Recognition in Videos',\n",
       "      'paper_url': 'https://doi.org/10.1109/ICCV.2017.402',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'mzolfaghari/chained-multistream-networks',\n",
       "        'url': 'https://github.com/mzolfaghari/chained-multistream-networks'}],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '76.1', 'Accuracy (pose)': '56.8'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Chained (RGB+Flow +Pose)',\n",
       "      'paper_date': '2017-04-03',\n",
       "      'paper_title': 'Chained Multi-stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection',\n",
       "      'paper_url': 'http://arxiv.org/abs/1704.00616v2',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '71.1'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'MR Two-Sream R-CNN',\n",
       "      'paper_date': '2016-09-17',\n",
       "      'paper_title': 'Multi-region two-stream R-CNN for action detection',\n",
       "      'paper_url': 'https://doi.org/10.1007/978-3-319-46493-0_45',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '69.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'PA3D',\n",
       "      'paper_date': '2019-06-01',\n",
       "      'paper_title': 'PA3D: Pose-Action 3D Machine for Video Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Yan_PA3D_Pose-Action_3D_Machine_for_Video_Recognition_CVPR_2019_paper.html',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '64.3'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'STAR-Net',\n",
       "      'paper_date': '2019-02-26',\n",
       "      'paper_title': 'STAR-Net: Action Recognition using Spatio-Temporal Activation Reprojection',\n",
       "      'paper_url': 'http://arxiv.org/abs/1902.10024v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'JeffCHEN2017/WSSTG',\n",
       "        'url': 'https://github.com/JeffCHEN2017/WSSTG'}],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '62.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Action Tubes',\n",
       "      'paper_date': '2014-11-21',\n",
       "      'paper_title': 'Finding Action Tubes',\n",
       "      'paper_url': 'http://arxiv.org/abs/1411.6031v1',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'fandulu/DD-Net',\n",
       "        'url': 'https://github.com/fandulu/DD-Net'},\n",
       "       {'title': 'BlurryLight/DD-Net-Pytorch',\n",
       "        'url': 'https://github.com/BlurryLight/DD-Net-Pytorch'}],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '-', 'Accuracy (pose)': '77.2'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'DD-Net',\n",
       "      'paper_date': '2019-07-23',\n",
       "      'paper_title': 'Make Skeleton-based Action Recognition Model Smaller, Faster and Better',\n",
       "      'paper_url': 'https://arxiv.org/abs/1907.09658v8',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'noboevbo/ehpi_action_recognition',\n",
       "        'url': 'https://github.com/noboevbo/ehpi_action_recognition'}],\n",
       "      'metrics': {'Accuracy (RGB+pose)': '-', 'Accuracy (pose)': '65.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'EHPI',\n",
       "      'paper_date': '2019-04-19',\n",
       "      'paper_title': 'Simple yet efficient real-time pose-based action recognition',\n",
       "      'paper_url': 'http://arxiv.org/abs/1904.09140v1',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []},\n",
       "  {'dataset': 'UPenn Action',\n",
       "   'dataset_citations': [],\n",
       "   'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "     'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-upenn'}],\n",
       "   'description': '',\n",
       "   'sota': {'metrics': ['Accuracy'],\n",
       "    'rows': [{'code_links': [{'title': 'google-research/google-research',\n",
       "        'url': 'https://github.com/google-research/google-research/tree/master/poem'}],\n",
       "      'metrics': {'Accuracy': '97.5'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'Pr-VIPE',\n",
       "      'paper_date': '2019-12-02',\n",
       "      'paper_title': 'View-Invariant Probabilistic Embedding for Human Pose',\n",
       "      'paper_url': 'https://arxiv.org/abs/1912.01001v4',\n",
       "      'uses_additional_data': False},\n",
       "     {'code_links': [{'title': 'rort1989/HDM',\n",
       "        'url': 'https://github.com/rort1989/HDM'}],\n",
       "      'metrics': {'Accuracy': '93.4'},\n",
       "      'model_links': [],\n",
       "      'model_name': 'HDM-BG',\n",
       "      'paper_date': '2019-06-01',\n",
       "      'paper_title': 'Bayesian Hierarchical Dynamic Model for Human Action Recognition',\n",
       "      'paper_url': 'http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Bayesian_Hierarchical_Dynamic_Model_for_Human_Action_Recognition_CVPR_2019_paper.html',\n",
       "      'uses_additional_data': False}]},\n",
       "   'subdatasets': []}],\n",
       " 'description': '<span style=\"color:grey; opacity: 0.6\">( Image credit: [View Adaptive Neural Networks for High\\r\\nPerformance Skeleton-based Human Action\\r\\nRecognition](https://arxiv.org/pdf/1804.07453v3.pdf) )</span>',\n",
       " 'source_link': None,\n",
       " 'subtasks': [],\n",
       " 'synonyms': [],\n",
       " 'task': 'Skeleton Based Action Recognition'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get one subtask\n",
    "subtasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2097c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count how many datasets\n",
    "len(subtasks[0][\"datasets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b7a3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SYSU 3D'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the same of the dataset\n",
    "subtasks[0][\"datasets\"][0][\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "490c5d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'SYSU 3D',\n",
       " 'dataset_citations': [],\n",
       " 'dataset_links': [{'title': 'Papers with Code Leaderboard URL',\n",
       "   'url': 'https://paperswithcode.com/sota/skeleton-based-action-recognition-on-sysu-3d'}],\n",
       " 'description': '',\n",
       " 'sota': {'metrics': ['Accuracy'],\n",
       "  'rows': [{'code_links': [{'title': 'microsoft/SGN',\n",
       "      'url': 'https://github.com/microsoft/SGN'}],\n",
       "    'metrics': {'Accuracy': '86.9%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'SGN',\n",
       "    'paper_date': '2019-04-02',\n",
       "    'paper_title': 'Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition',\n",
       "    'paper_url': 'https://arxiv.org/abs/1904.01189v3',\n",
       "    'uses_additional_data': False},\n",
       "   {'code_links': [{'title': 'microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition',\n",
       "      'url': 'https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition'},\n",
       "     {'title': 'iamjeff7/j-va-aagcn',\n",
       "      'url': 'https://github.com/iamjeff7/j-va-aagcn'}],\n",
       "    'metrics': {'Accuracy': '86.7%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'VA-fusion (aug.)',\n",
       "    'paper_date': '2018-04-20',\n",
       "    'paper_title': 'View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition',\n",
       "    'paper_url': 'https://arxiv.org/abs/1804.07453v3',\n",
       "    'uses_additional_data': False},\n",
       "   {'code_links': [],\n",
       "    'metrics': {'Accuracy': '85.7%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'EleAtt-GRU (aug.)',\n",
       "    'paper_date': '2019-09-03',\n",
       "    'paper_title': 'EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks',\n",
       "    'paper_url': 'https://arxiv.org/abs/1909.01939v1',\n",
       "    'uses_additional_data': False},\n",
       "   {'code_links': [],\n",
       "    'metrics': {'Accuracy': '83.14%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'Local+LGN',\n",
       "    'paper_date': '2019-09-02',\n",
       "    'paper_title': 'Learning Latent Global Network for Skeleton-based Action Prediction',\n",
       "    'paper_url': 'https://doi.org/10.1109/TIP.2019.2937757',\n",
       "    'uses_additional_data': False},\n",
       "   {'code_links': [],\n",
       "    'metrics': {'Accuracy': '77.9%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'Complete GR-GCN',\n",
       "    'paper_date': '2018-11-29',\n",
       "    'paper_title': 'Optimized Skeleton-based Action Recognition via Sparsified Graph Regression',\n",
       "    'paper_url': 'http://arxiv.org/abs/1811.12013v2',\n",
       "    'uses_additional_data': False},\n",
       "   {'code_links': [],\n",
       "    'metrics': {'Accuracy': '77.5%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'VA-LSTM',\n",
       "    'paper_date': '2017-03-24',\n",
       "    'paper_title': 'View Adaptive Recurrent Neural Networks for High Performance Human Action Recognition from Skeleton Data',\n",
       "    'paper_url': 'http://arxiv.org/abs/1703.08274v2',\n",
       "    'uses_additional_data': False},\n",
       "   {'code_links': [],\n",
       "    'metrics': {'Accuracy': '76.9%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'DPRL',\n",
       "    'paper_date': '2018-06-01',\n",
       "    'paper_title': 'Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition',\n",
       "    'paper_url': 'http://openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html',\n",
       "    'uses_additional_data': False},\n",
       "   {'code_links': [],\n",
       "    'metrics': {'Accuracy': '75.5%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'Dynamic Skeletons',\n",
       "    'paper_date': '2016-12-15',\n",
       "    'paper_title': 'Jointly learning heterogeneous features for rgb-d activity recognition',\n",
       "    'paper_url': 'https://doi.org/10.1109/TPAMI.2016.2640292',\n",
       "    'uses_additional_data': False},\n",
       "   {'code_links': [],\n",
       "    'metrics': {'Accuracy': '73.4%'},\n",
       "    'model_links': [],\n",
       "    'model_name': 'ST-LSTM (Tree)',\n",
       "    'paper_date': '2017-06-26',\n",
       "    'paper_title': 'Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network with Trust Gates',\n",
       "    'paper_url': 'http://arxiv.org/abs/1706.08276v1',\n",
       "    'uses_additional_data': False}]},\n",
       " 'subdatasets': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is all the data for such dataset\n",
    "subtasks[0][\"datasets\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ed31128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show list of metrics per dataset\n",
    "subtasks[0][\"datasets\"][0][\"sota\"][\"metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41a9cd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': '86.9%'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show metric result for rank 1 ([\"rows\"][0])\n",
    "subtasks[0][\"datasets\"][0][\"sota\"][\"rows\"][0][\"metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f70270fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': '86.7%'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show metric result for rank 2 ([\"rows\"][1])\n",
    "subtasks[0][\"datasets\"][0][\"sota\"][\"rows\"][1][\"metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f230f737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': '49.6%'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sometimes it is just reported one metric \n",
    "subtasks[1][\"datasets\"][0][\"sota\"][\"rows\"][0][\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed182737",
   "metadata": {},
   "source": [
    "### create a function to retrieve all metrics of a subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "173094ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(subtasks):\n",
    "    #create report data frame for the subtasks of the taken sample\n",
    "    df_metric_all = pd.DataFrame(columns=[\"value\",\"metrics\",\"ranking\",\"datasets\",\"task\",\"categories\"])\n",
    "    #df_metric_all\n",
    "\n",
    "    s = 0\n",
    "    while s < len(subtasks):\n",
    "        d = 0\n",
    "        while d < len(subtasks[s][\"datasets\"]):\n",
    "            #iterate over the different metrics of a dataset\n",
    "            m = 0\n",
    "            while m < len(subtasks[s][\"datasets\"][d][\"sota\"][\"rows\"]):\n",
    "                df_metric = pd.DataFrame.from_dict(subtasks[s][\"datasets\"][d][\"sota\"][\"rows\"][m][\"metrics\"], orient='index')\n",
    "                df_metric.rename(columns = {0: 'value'}, inplace = True)\n",
    "                df_metric[\"metrics\"]=df_metric.index\n",
    "                df_metric[\"ranking\"]= m + 1\n",
    "                df_metric[\"datasets\"] = subtasks[s][\"datasets\"][d][\"dataset\"]\n",
    "                df_metric[\"task\"]= subtasks[s][\"task\"]\n",
    "                df_metric[\"categories\"]= str(sample[\"categories\"].iloc[0])\n",
    "                df_metric_all = df_metric_all.append(df_metric, ignore_index=True)\n",
    "                m = m +1 \n",
    "            d = d + 1\n",
    "        s = s + 1\n",
    "\n",
    "    #[\"value\",\"metric\",\"ranking\",\"datasets\",\"task\"])    \n",
    "    df_metric_all = df_metric_all[['categories','task', 'datasets', 'metrics','ranking', 'value']]\n",
    "\n",
    "    df_metric_all[\"value\"] = df_metric_all[\"value\"].replace(to_replace ='\\%', value = '', regex = True)\n",
    "    df_metric_all[\"value\"] = df_metric_all[\"value\"].replace(to_replace ='\\-', value = '', regex = True)\n",
    "    df_metric_all[\"value\"] = df_metric_all[\"value\"].replace(to_replace =' .*', value = '', regex = True)\n",
    "    df_metric_all[\"value\"] = df_metric_all[\"value\"].replace(to_replace =',', value = '', regex = True)\n",
    "\n",
    "\n",
    "    df_metric_all[\"categories\"] = df_metric_all[\"categories\"].replace(to_replace ='\\[\\'', value = '', regex = True)\n",
    "    df_metric_all[\"categories\"] = df_metric_all[\"categories\"].replace(to_replace ='\\'\\]', value = '', regex = True)\n",
    "    \n",
    "    return(df_metric_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c491f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many categories\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c42c3",
   "metadata": {},
   "source": [
    "## Attention!!\n",
    "\n",
    "This is the block that creates the file df_metric_all.csv required by the \"Complete collapsed workflow\" notebook.\n",
    "\n",
    "Change this block to code and run it if the file is not available."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a88a4f7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#Note: change to Code if you want to run it. It takes a while to process!\n",
    "\n",
    "#report df: df_metric_all\n",
    "df_metric_all = pd.DataFrame(columns=['categories','task', 'datasets', 'metrics','ranking', 'value'])\n",
    "\n",
    "#set here how many categories will be taken\n",
    "for x in range(len(df.index)):\n",
    "    sample= df[df.index==x]\n",
    "    #get all the subtasks of this sample\n",
    "    subtasks = sample[\"subtasks\"].iloc[0]\n",
    "    df_metric_all = df_metric_all.append(get_all_metrics(subtasks), ignore_index=True)\n",
    "    \n",
    "#df_metric_all[\"metrics\"].unique()\n",
    "\n",
    "df_metric_all = df_metric_all[df_metric_all[\"ranking\"]<=2]\n",
    "df_metric_all = df_metric_all[df_metric_all[\"ranking\"]>0]\n",
    "df_metric_all = df_metric_all[df_metric_all[\"metrics\"]!=\"No. parameters\"]\n",
    "\n",
    "#save to csv to save time\n",
    "df_metric_all.to_csv('df_metric_all.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe107ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>task</th>\n",
       "      <th>datasets</th>\n",
       "      <th>metrics</th>\n",
       "      <th>ranking</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer Vision', 'Natural Language Processing</td>\n",
       "      <td>Handwriting Recognition</td>\n",
       "      <td>BanglaLekha Isolated Dataset</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>96.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Computer Vision', 'Natural Language Processing</td>\n",
       "      <td>Handwriting Recognition</td>\n",
       "      <td>BanglaLekha Isolated Dataset</td>\n",
       "      <td>Cross Entropy Loss</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Computer Vision', 'Natural Language Processing</td>\n",
       "      <td>Handwriting Recognition</td>\n",
       "      <td>BanglaLekha Isolated Dataset</td>\n",
       "      <td>Epochs</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computer Vision', 'Natural Language Processing</td>\n",
       "      <td>Handwritten Digit Recognition</td>\n",
       "      <td>MNIST</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>96.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       categories  \\\n",
       "0  Computer Vision', 'Natural Language Processing   \n",
       "1  Computer Vision', 'Natural Language Processing   \n",
       "2  Computer Vision', 'Natural Language Processing   \n",
       "3  Computer Vision', 'Natural Language Processing   \n",
       "\n",
       "                            task                      datasets  \\\n",
       "0        Handwriting Recognition  BanglaLekha Isolated Dataset   \n",
       "1        Handwriting Recognition  BanglaLekha Isolated Dataset   \n",
       "2        Handwriting Recognition  BanglaLekha Isolated Dataset   \n",
       "3  Handwritten Digit Recognition                         MNIST   \n",
       "\n",
       "              metrics  ranking    value  \n",
       "0            Accuracy        1     96.8  \n",
       "1  Cross Entropy Loss        1  0.21612  \n",
       "2              Epochs        1       11  \n",
       "3            Accuracy        1    96.95  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reads pre-saved results for all the categories\n",
    "df_metric_all = pd.read_csv(\"df_metric_all.csv\")\n",
    "df_metric_all.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76d3dbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Computer Vision', 'Natural Language Processing\",\n",
       "       \"Computer Vision', 'Miscellaneous', 'Time Series\",\n",
       "       'Computer Vision', 'Time Series', \"Time Series', 'Methodology\",\n",
       "       \"Medical', 'Knowledge Base\", \"Computer Vision', 'Methodology\",\n",
       "       'Speech', \"Speech', 'Natural Language Processing\",\n",
       "       'Natural Language Processing', \"Computer Vision', 'Time Series\",\n",
       "       \"Medical', 'Computer Vision\",\n",
       "       \"Computer Vision', 'Miscellaneous', 'Methodology\", 'Methodology',\n",
       "       \"Computer Code', 'Natural Language Processing\", 'Knowledge Base',\n",
       "       \"Robots', 'Computer Vision\", 'Medical', 'Miscellaneous',\n",
       "       \"Robots', 'Miscellaneous\",\n",
       "       \"Computer Vision', 'Natural Language Processing', 'Speech', 'Miscellaneous\",\n",
       "       'Audio', \"Medical', 'Methodology\",\n",
       "       \"Miscellaneous', 'Computer Vision\",\n",
       "       \"Computer Vision', 'Playing Games\", \"Computer Code', 'Reasoning\",\n",
       "       \"Miscellaneous', 'Methodology\", 'Graphs', 'Playing Games',\n",
       "       \"Computer Vision', 'Graphs\",\n",
       "       \"Adversarial', 'Natural Language Processing\", 'Computer Code',\n",
       "       \"Computer Vision', 'Natural Language Processing', 'Methodology\",\n",
       "       \"Computer Vision', 'Miscellaneous\", \"Speech', 'Methodology\",\n",
       "       \"Adversarial', 'Methodology\",\n",
       "       \"Natural Language Processing', 'Methodology\",\n",
       "       \"Adversarial', 'Miscellaneous\", \"Miscellaneous', 'Graphs\",\n",
       "       \"Computer Vision', 'Speech\", \"Reasoning', 'Time Series\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metric_all[\"categories\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dcf5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only rakns 1 and 2\n",
    "#Filter out strange metrics\n",
    "\n",
    "df_metric_all = df_metric_all[df_metric_all[\"ranking\"]<=2]\n",
    "df_metric_all = df_metric_all[df_metric_all[\"ranking\"]>0]\n",
    "df_metric_all = df_metric_all[df_metric_all[\"metrics\"]!=\"No. parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ad094fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:COL\trank_1:5.31\trank_2:6.560\tpolarity:neg\n",
      "metricName:FDE\trank_1:1.14\trank_2:1.150\tpolarity:neg\n",
      "metricName:absolute relative error\trank_1:0.079\trank_2:0.087\tpolarity:neg\n",
      "metricName:Abs Rel\trank_1:0.377\trank_2:0.322\tpolarity:pos\n",
      "metricName:RMSE\trank_1:8.388\trank_2:7.417\tpolarity:pos\n",
      "metricName:Sq Rel\trank_1:4.9\trank_2:3.589\tpolarity:pos\n",
      "metricName:D3R\trank_1:0.3222\trank_2:0.4671\tpolarity:neg\n",
      "metricName:ORD\trank_1:0.3938\trank_2:0.5538\tpolarity:neg\n",
      "metricName:RMSE\trank_1:0.357\trank_2:0.364\tpolarity:neg\n",
      "metricName:RMSE\trank_1:0.1557\trank_2:0.1973\tpolarity:neg\n",
      "metricName:D3R\trank_1:0.1578\trank_2:0.2324\tpolarity:neg\n",
      "metricName:Œ¥1.25\trank_1:0.7406\trank_2:0.7891\tpolarity:neg\n",
      "metricName:ORD \trank_1:0.3467\trank_2:0.3879\tpolarity:neg\n",
      "metricName:absolute relative error\trank_1:0.058\trank_2:0.059\tpolarity:neg\n",
      "metricName:Abs Rel\trank_1:0.1425\trank_2:0.2410\tpolarity:neg\n",
      "metricName:RMSE\trank_1:8.8641\trank_2:12.599\tpolarity:neg\n",
      "metricName:RMSE log\trank_1:0.24571\trank_2:0.3618\tpolarity:neg\n",
      "metricName:SQ Rel\trank_1:3.6798\trank_2:5.5321\tpolarity:neg\n",
      "metricName: three pixel error\trank_1:2.43\trank_2:6.2\tpolarity:neg\n",
      "metricName:RMSE\trank_1:636.2\trank_2:725.43\tpolarity:neg\n",
      "metricName:PSNR\trank_1:25.06\trank_2:25.04\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.607\trank_2:0.6075\tpolarity:neg\n",
      "metricName:PSNR\trank_1:32.31\trank_2:32.21\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.901\trank_2:0.9001\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.94\trank_2:34.87\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9518\trank_2:0.9509\tpolarity:pos\n",
      "metricName:PSNR\trank_1:25.41\trank_2:25.4\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.657\trank_2:0.6547\tpolarity:pos\n",
      "metricName:NIQE\trank_1:2.51\trank_2:2.55\tpolarity:neg\n",
      "metricName:PSNR\trank_1:28.65\trank_2:27.42\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.816\trank_2:0.816\tpolarity:pos\n",
      "metricName:FID\trank_1:5.36\trank_2:74.43\tpolarity:neg\n",
      "metricName:MS-SSIM\trank_1:0.971\trank_2:0.958\tpolarity:pos\n",
      "metricName:PSNR\trank_1:29.41\trank_2:29.4\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.8116\trank_2:0.8125\tpolarity:neg\n",
      "metricName:PSNR\trank_1:33.04\trank_2:34.1\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.875\trank_2:0.906\tpolarity:neg\n",
      "metricName:FID\trank_1:1.978\trank_2:12.4\tpolarity:neg\n",
      "metricName:MS-SSIM\trank_1:0.975\trank_2:0.971\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.66\trank_2:31.78\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.9222\trank_2:0.9211\tpolarity:pos\n",
      "metricName:PSNR\trank_1:27.21\trank_2:27.11\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.75\trank_2:32.74\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9016\trank_2:0.9021\tpolarity:neg\n",
      "metricName:PSNR\trank_1:29.03\trank_2:27.51\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.8299\trank_2:0.793\tpolarity:pos\n",
      "metricName:PSNR\trank_1:29.28\trank_2:27.3\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.8711\trank_2:0.818\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.83\trank_2:32.47\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9262\trank_2:0.9032\tpolarity:pos\n",
      "metricName:PSNR\trank_1:35.24\trank_2:33.54\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9572\trank_2:0.9402\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.824\trank_2:30.188\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.838\trank_2:0.824\tpolarity:pos\n",
      "metricName:NIQE\trank_1:6.961\trank_2:13.636\tpolarity:neg\n",
      "metricName:FID\trank_1:1.898\trank_2:20.605\tpolarity:neg\n",
      "metricName:MS-SSIM\trank_1:0.971\trank_2:0.961\tpolarity:pos\n",
      "metricName:FED\trank_1:0.0716\trank_2:0.0843\tpolarity:neg\n",
      "metricName:LLE\trank_1:2.071\trank_2:2.003\tpolarity:pos\n",
      "metricName:LPIPS\trank_1:0.0723\trank_2:0.2475\tpolarity:neg\n",
      "metricName:PSNR\trank_1:35.61\trank_2:34.43\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9404\trank_2:0.9247\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.86\trank_2:34.85\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9307\trank_2:0.9300\tpolarity:pos\n",
      "metricName:PSNR\trank_1:25.71\trank_2:25.55\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.813\trank_2:0.8087\tpolarity:pos\n",
      "metricName:PSNR\trank_1:29.37\trank_2:29.21\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.8746\trank_2:0.8710\tpolarity:pos\n",
      "metricName:PSNR\trank_1:29.15\trank_2:27.87\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.8001\trank_2:0.7453\tpolarity:pos\n",
      "metricName:PSNR\trank_1:25.57\trank_2:24.10\tpolarity:pos\n",
      "metricName:PSNR\trank_1:23.24\trank_2:23.2\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.6523\trank_2:0.652\tpolarity:pos\n",
      "metricName:PSNR\trank_1:39.75\trank_2:39.62\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9792\trank_2:0.9787\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.31\trank_2:29.05\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.8382\trank_2:0.7921\tpolarity:pos\n",
      "metricName:LPIPS\trank_1:0.321\trank_2:0.345\tpolarity:neg\n",
      "metricName:PSNR\trank_1:30.8\trank_2:30.79\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.8498\trank_2:0.8487\tpolarity:pos\n",
      "metricName:PSNR\trank_1:38.94\trank_2:38.34\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9658\trank_2:0.9619\tpolarity:pos\n",
      "metricName:PSNR\trank_1:27.43\trank_2:27.31\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.835\trank_2:0.832\tpolarity:pos\n",
      "metricName:Average PSNR\trank_1:31.67\trank_2:31.47\tpolarity:pos\n",
      "metricName:PSNR\trank_1:26.23\trank_2:23.13\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.699\trank_2:0.651\tpolarity:pos\n",
      "metricName:TIoU\trank_1:0.879\trank_2:0.598\tpolarity:pos\n",
      "metricName:Average PSNR\trank_1:37.91\trank_2:37.52\tpolarity:pos\n",
      "metricName:PSNR\trank_1:26.83\trank_2:23.42\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.753\trank_2:0.671\tpolarity:pos\n",
      "metricName:TIoU\trank_1:0.684\trank_2:0.539\tpolarity:pos\n",
      "metricName:PSNR\trank_1:25.21\trank_2:23.22\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.674\trank_2:0.605\tpolarity:pos\n",
      "metricName:TIoU\trank_1:0.542\trank_2:0.542\tpolarity:pos\n",
      "metricName:3DIoU\trank_1:64.97\trank_2:64.64\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:87.2\trank_2:83.73\tpolarity:pos\n",
      "metricName:Average Accuracy (10 times)\trank_1:76.46\trank_2:53.97\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:98.986\trank_2:0.9602\tpolarity:pos\n",
      "metricName:Average Accuracy (10 times)\trank_1:83.79\trank_2:80.72\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:0.99833\trank_2:0.9973\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:85\trank_2:71\tpolarity:pos\n",
      "metricName:Average Accuracy (10 times)\trank_1:84.3\trank_2:80.6\tpolarity:pos\n",
      "metricName:AP\trank_1:0.9909\trank_2:0.990\tpolarity:pos\n",
      "metricName:AP\trank_1:0.991\trank_2:0.990\tpolarity:pos\n",
      "metricName:AP\trank_1:0.965\trank_2:0.960\tpolarity:pos\n",
      "metricName:AP\trank_1:0.924\trank_2:0.914\tpolarity:pos\n",
      "metricName:AP\trank_1:0.957\trank_2:0.953\tpolarity:pos\n",
      "metricName:AP\trank_1:0.9987\trank_2:0.9985\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:98.95\trank_2:98.48\tpolarity:pos\n",
      "metricName:TAR @ FAR=0.01\trank_1:98.5\trank_2:98.5\tpolarity:pos\n",
      "metricName:TAR @ FAR=0.001\trank_1:97.3\trank_2:96.9\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:0.985\trank_2:0.9307\tpolarity:pos\n",
      "metricName:TAR @ FAR=0.01\trank_1:96.5\trank_2:96.4\tpolarity:pos\n",
      "metricName:TAR @ FAR=0.01\trank_1:97.60\trank_2:97.5\tpolarity:pos\n",
      "metricName:TAR @ FAR=0.0001\trank_1:97.7\trank_2:97.30\tpolarity:pos\n",
      "metricName:TAR @ FAR=0.001\trank_1:99.8\trank_2:99.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:72.71\trank_2:61.61\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.85\trank_2:99.83\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:0.9815\trank_2:0.97333\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:98.12\trank_2:98.02\tpolarity:pos\n",
      "metricName:TAR @ FAR=0.01\trank_1:98.5\trank_2:97.2\tpolarity:pos\n",
      "metricName:TAR @ FAR=0.001\trank_1:92.9\trank_2:84.9\tpolarity:pos\n",
      "metricName:Fullset (public)\trank_1:3.07\trank_2:3.12\tpolarity:neg\n",
      "metricName:Mean Error Rate\trank_1:4.94\trank_2:5.04\tpolarity:neg\n",
      "metricName:Mean NME \trank_1:2.58\trank_2:3.51\tpolarity:neg\n",
      "metricName:AUC@0.1 (all)\trank_1:0.5913\trank_2:0.5755\tpolarity:pos\n",
      "metricName:FR@0.1(%, all)\trank_1:4.08\trank_2:3.55\tpolarity:pos\n",
      "metricName:ME (%, all) \trank_1:4.39\trank_2:4.39\tpolarity:pos\n",
      "metricName:Mean NME \trank_1:2.93\trank_2:3.86\tpolarity:neg\n",
      "metricName:Mean Error Rate\trank_1:6.73\trank_2:7.54\tpolarity:neg\n",
      "metricName:Mean NME\trank_1:4.43\trank_2:4.55\tpolarity:neg\n",
      "metricName:Error rate\trank_1:4.70\trank_2:5.38\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:89.75\trank_2:89.257\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:76.82\trank_2:74.14\tpolarity:pos\n",
      "metricName:Accuracy(on validation set)\trank_1:65.5\trank_2:65.5\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:98.63\trank_2:82.74\tpolarity:pos\n",
      "metricName:Accuracy (8 emotion)\trank_1:61.72\trank_2:61.60\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:92.8\trank_2:91.8\tpolarity:pos\n",
      "metricName:Accuracy (10-fold)\trank_1:99.7\trank_2:98.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:58.71\trank_2:56.4\tpolarity:pos\n",
      "metricName:Accuracy (10-fold)\trank_1:89.6\trank_2:84.59\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:58.14\trank_2:54.82\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:89.257\trank_2:87.15\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:Overall Accuracy\trank_1:92.05\trank_2:89.075\tpolarity:pos\n",
      "metricName:pose\trank_1:1.12\trank_2:1.22\tpolarity:neg\n",
      "metricName:NME\trank_1:3.13\trank_2:3.24\tpolarity:neg\n",
      "metricName:Mean NME \trank_1:1.91\trank_2:2.17\tpolarity:neg\n",
      "metricName:AUC0.08 private\trank_1:59.39\trank_2:58.22\tpolarity:pos\n",
      "metricName:Mean Reconstruction Error (mm)\trank_1:1.91\trank_2:2.08\tpolarity:neg\n",
      "metricName:Mean NME \trank_1:3.56\trank_2:3.9625\tpolarity:neg\n",
      "metricName:Mean NME \trank_1:3.56\trank_2:3.7551\tpolarity:neg\n",
      "metricName:Mean Reconstruction Error (mm)\trank_1:1.89\trank_2:1.91\tpolarity:neg\n",
      "metricName:Mean Reconstruction Error (mm)\trank_1:1.38\trank_2:1.53\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:73.56\trank_2:61.80\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:94.60\trank_2:91.4\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.10\trank_2:98.78\tpolarity:pos\n",
      "metricName:EER\trank_1:4.92\trank_2:2.22\tpolarity:pos\n",
      "metricName:EER\trank_1:2.14\trank_2:0.40\tpolarity:pos\n",
      "metricName:Equal Error Rate\trank_1:7.5\trank_2:10.8\tpolarity:neg\n",
      "metricName:MAE\trank_1:2.95\trank_2:3.62\tpolarity:neg\n",
      "metricName:MAE\trank_1:4.60\trank_2:5.35\tpolarity:neg\n",
      "metricName:MAE\trank_1:4.55\trank_2:5.39\tpolarity:neg\n",
      "metricName:MAE\trank_1:3.135\trank_2:3.51\tpolarity:neg\n",
      "metricName:Accuracy (%)\trank_1:92.93\trank_2:92.69\tpolarity:pos\n",
      "metricName:FID\trank_1:11.389\trank_2:50.901\tpolarity:neg\n",
      "metricName:LPIPS\trank_1:0.2449\trank_2:0.3928\tpolarity:neg\n",
      "metricName:NIQE\trank_1:6.767\trank_2:15.383\tpolarity:neg\n",
      "metricName:Accuracy (5-fold)\trank_1:67.47\trank_2:67.3\tpolarity:pos\n",
      "metricName:Accuracy (5-fold)\trank_1:89.66\trank_2:89.08\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:81.8\trank_2:56.1\tpolarity:pos\n",
      "metricName:F1\trank_1:57.7\trank_2:45.2\tpolarity:pos\n",
      "metricName:FSIM\trank_1:74.23\trank_2:73.61\tpolarity:pos\n",
      "metricName:SSIM\trank_1:63.28\trank_2:61.56\tpolarity:pos\n",
      "metricName:FSIM\trank_1:72.9\trank_2:72.7\tpolarity:pos\n",
      "metricName:FID\trank_1:18.2\trank_2:19.6\tpolarity:neg\n",
      "metricName:NLDA\trank_1:78\trank_2:78.1\tpolarity:neg\n",
      "metricName:FSIM\trank_1:72.56\trank_2:71.6\tpolarity:pos\n",
      "metricName:GAR @0.1% FAR Impersonation\trank_1:75.38\trank_2:69.27\tpolarity:pos\n",
      "metricName:GAR @0.1% FAR Obfuscation\trank_1:72.13\trank_2:93.08\tpolarity:neg\n",
      "metricName:GAR @0.1% FAR Overall\trank_1:72.72\trank_2:93.01\tpolarity:neg\n",
      "metricName:GAR @1% FAR Impersonation\trank_1:95.73\trank_2:99.01\tpolarity:neg\n",
      "metricName:GAR @1% FAR Obfuscation\trank_1:88.97\trank_2:95.93\tpolarity:neg\n",
      "metricName:GAR @1% FAR Overall\trank_1:89.3\trank_2:95.99\tpolarity:neg\n",
      "metricName:GAR @0.1% FAR Impersonation\trank_1:79.2\trank_2:76.40\tpolarity:pos\n",
      "metricName:GAR @0.1% FAR Obfuscation\trank_1:99.00\trank_2:96.84\tpolarity:pos\n",
      "metricName:GAR @0.1% FAR Overall\trank_1:98.63\trank_2:95.96\tpolarity:pos\n",
      "metricName:GAR @0.01% FAR Impersonation\trank_1:54.40\trank_2:52.80\tpolarity:pos\n",
      "metricName:GAR @0.01% FAR Obfuscation\trank_1:97.20\trank_2:94.02\tpolarity:pos\n",
      "metricName:GAR @0.01% FAR Overall\trank_1:96.18\trank_2:93.06\tpolarity:pos\n",
      "metricName:GAR @0.01% FAR Plastic Surgery\trank_1:96.00\trank_2:92.00\tpolarity:pos\n",
      "metricName:GAR @0.1% FAR Plastic Surgery\trank_1:98.80\trank_2:95.20\tpolarity:pos\n",
      "metricName:MAE\trank_1:0.2595\trank_2:0.3931\tpolarity:neg\n",
      "metricName:RC\trank_1:69.17\trank_2:56.36\tpolarity:pos\n",
      "metricName:RC\trank_1:86.91\trank_2:78.41\tpolarity:pos\n",
      "metricName:Driving Score\trank_1:31.37\trank_2:13.03\tpolarity:pos\n",
      "metricName:Route Completion\trank_1:57.65\trank_2:42.98\tpolarity:pos\n",
      "metricName:Reasonable Miss Rate\trank_1:1.76\trank_2:3.46\tpolarity:neg\n",
      "metricName:Bare MR^-2\trank_1:6.2\trank_2:4.9\tpolarity:pos\n",
      "metricName:Heavy MR^-2\trank_1:33.9\trank_2:42.5\tpolarity:neg\n",
      "metricName:Partial MR^-2\trank_1:5.7\trank_2:6.9\tpolarity:neg\n",
      "metricName:Reasonable MR^-2\trank_1:7.5\trank_2:7.6\tpolarity:neg\n",
      "metricName:F1 score\trank_1:79.48\trank_2:78.8\tpolarity:pos\n",
      "metricName:F1\trank_1:0.9601\trank_2:0.9374\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.92\trank_2:96.82\tpolarity:pos\n",
      "metricName:F1\trank_1:0.884\trank_2:0.866\tpolarity:pos\n",
      "metricName:F1\trank_1:0.869\trank_2:0.861\tpolarity:pos\n",
      "metricName:MAP\trank_1:0.32\trank_2:0.31\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.71\trank_2:99.68\tpolarity:pos\n",
      "metricName:mAP@0.50\trank_1:95.2\trank_2:94.3\tpolarity:pos\n",
      "metricName:mAP@0.50\trank_1:95.5\trank_2:95.2\tpolarity:pos\n",
      "metricName:mAP @0.5:0.95\trank_1:84.4\trank_2:82.0\tpolarity:pos\n",
      "metricName:MAP\trank_1:0.46\trank_2:0.45\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:78.84\trank_2:77.08\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:79.52\trank_2:76.13\tpolarity:pos\n",
      "metricName:Backpack\trank_1:63.9\trank_2:63.5\tpolarity:pos\n",
      "metricName:Gender\trank_1:75.0\trank_2:74.7\tpolarity:pos\n",
      "metricName:Hat\trank_1:67.2\trank_2:65.2\tpolarity:pos\n",
      "metricName:LCC\trank_1:54.6\trank_2:49.7\tpolarity:pos\n",
      "metricName:LCS\trank_1:68.9\trank_2:69.3\tpolarity:neg\n",
      "metricName:UCC\trank_1:49.8\trank_2:44.4\tpolarity:pos\n",
      "metricName:UCS\trank_1:73.0\trank_2:68.9\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:68.17\trank_2:65.39\tpolarity:pos\n",
      "metricName:Classification Accuracy\trank_1:90.4\trank_2:84.4\tpolarity:pos\n",
      "metricName:PSNR\trank_1:21.65\trank_2:18.57\tpolarity:pos\n",
      "metricName:mIoU\trank_1:57.5\trank_2:56.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:79.8\trank_2:74.8\tpolarity:pos\n",
      "metricName:MIoU (13 classes)\trank_1:62.0\trank_2:62.8\tpolarity:neg\n",
      "metricName:MIoU (16 classes)\trank_1:55.5\trank_2:55.0\tpolarity:pos\n",
      "metricName:Diversity\trank_1:0.175\trank_2:0.140\tpolarity:pos\n",
      "metricName:Quality\trank_1:50.0\trank_2:51.2\tpolarity:neg\n",
      "metricName:Diversity\trank_1:0.109\trank_2:0.104\tpolarity:pos\n",
      "metricName:Quality\trank_1:50.0\trank_2:56.7\tpolarity:neg\n",
      "metricName:FID\trank_1:16.2\trank_2:41.5\tpolarity:neg\n",
      "metricName:CIS\trank_1:1.039\trank_2:0.115\tpolarity:pos\n",
      "metricName:PSNR\trank_1:23.11\trank_2:17.38\tpolarity:pos\n",
      "metricName:FID\trank_1:13.73\trank_2:31.4\tpolarity:neg\n",
      "metricName:FID\trank_1:17.3\trank_2:20.7\tpolarity:neg\n",
      "metricName:Kernel Inception Distance\trank_1:0.00053\trank_2:0.00392\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.6865\trank_2:0.5171\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.5118\trank_2:0.3682\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.5938\trank_2:0.5457\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.3284\trank_2:0.2763\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.5366\trank_2:0.5323\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.6024\trank_2:0.2740\tpolarity:pos\n",
      "metricName:mAP\trank_1:30.6\trank_2:25.4\tpolarity:pos\n",
      "metricName:rank-1\trank_1:61.4\trank_2:51.6\tpolarity:pos\n",
      "metricName:rank-10\trank_1:78.2\trank_2:69.7\tpolarity:pos\n",
      "metricName:rank-5\trank_1:73.3\trank_2:64.3\tpolarity:pos\n",
      "metricName:mAP\trank_1:71.0\trank_2:68.8\tpolarity:pos\n",
      "metricName:rank-1\trank_1:83.4\trank_2:82.9\tpolarity:pos\n",
      "metricName:rank-10\trank_1:93.8\trank_2:92.5\tpolarity:pos\n",
      "metricName:rank-5\trank_1:91.7\trank_2:90.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:76.8\trank_2:76.8\tpolarity:pos\n",
      "metricName:mAP\trank_1:80.6\trank_2:76.7\tpolarity:pos\n",
      "metricName:rank-1\trank_1:92.9\trank_2:90.3\tpolarity:pos\n",
      "metricName:rank-10\trank_1:98.2\trank_2:97.7\tpolarity:pos\n",
      "metricName:rank-5\trank_1:97.2\trank_2:96.2\tpolarity:pos\n",
      "metricName:mAP\trank_1:30.7\trank_2:26.5\tpolarity:pos\n",
      "metricName:rank-1\trank_1:62.7\trank_2:53.1\tpolarity:pos\n",
      "metricName:rank-10\trank_1:79.0\trank_2:70.5\tpolarity:pos\n",
      "metricName:rank-5\trank_1:74.5\trank_2:65.8\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:45.3\trank_2:42.9\tpolarity:pos\n",
      "metricName:Average Per-Class Accuracy\trank_1:61.67\trank_2:58.4\tpolarity:pos\n",
      "metricName:AP@0.7\trank_1:17.1\trank_2:16.5\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:40.9\trank_2:38.8\tpolarity:pos\n",
      "metricName:Top 1 Error\trank_1:17.4\trank_2:19.7\tpolarity:neg\n",
      "metricName:mean Corruption Error (mCE)\trank_1:22.0\trank_2:23.0\tpolarity:neg\n",
      "metricName:mean Corruption Error (mCE)\trank_1:46.8\trank_2:49.4\tpolarity:neg\n",
      "metricName:Top-1 accuracy %\trank_1:28.5\trank_2:25.7\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:46.5\trank_2:43.63\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:79.1\trank_2:77.53\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:82.8\trank_2:78.9\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:70.6\trank_2:68.56\tpolarity:pos\n",
      "metricName:Top-1 Error Rate\trank_1:51.3\trank_2:52.3\tpolarity:neg\n",
      "metricName:Average Accuracy\trank_1:88.1\trank_2:87.83\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:98.38\trank_2:97.90\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:83.7\trank_2:81.86\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:78.3\trank_2:76.0\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:87.1\trank_2:85.7\tpolarity:pos\n",
      "metricName:Word Error Rate (WER)\trank_1:23.9\trank_2:29.8\tpolarity:neg\n",
      "metricName:Word Error Rate (WER)\trank_1:1.99\trank_2:2.74\tpolarity:neg\n",
      "metricName:Percentage error\trank_1:13.56\trank_2:28.46\tpolarity:neg\n",
      "metricName:Percentage error\trank_1:7.55\trank_2:15.01\tpolarity:neg\n",
      "metricName:Percentage error\trank_1:17.55\trank_2:31.20\tpolarity:neg\n",
      "metricName:Percentage error\trank_1:22.44\trank_2:45.35\tpolarity:neg\n",
      "metricName:Percentage error\trank_1:7.8\trank_2:11.4\tpolarity:neg\n",
      "metricName:Percentage error\trank_1:3.34\trank_2:6.3\tpolarity:neg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:Avg F1\trank_1:19.77\trank_2:19.09\tpolarity:pos\n",
      "metricName:BLEU\trank_1:14.4\trank_2:14.79\tpolarity:neg\n",
      "metricName:Entity F1\trank_1:62.7\trank_2:59.97\tpolarity:pos\n",
      "metricName:METEOR\trank_1:0.331\trank_2:0.089\tpolarity:pos\n",
      "metricName:Joint\trank_1:75.5\trank_2:74.5\tpolarity:pos\n",
      "metricName:Joint\trank_1:90.5\trank_2:88.9\tpolarity:pos\n",
      "metricName:MRR\trank_1:0.7124\trank_2:0.7041\tpolarity:pos\n",
      "metricName:Mean Rank\trank_1:2.96\trank_2:3.66\tpolarity:neg\n",
      "metricName:R@1\trank_1:58.28\trank_2:58.18\tpolarity:pos\n",
      "metricName:R@10\trank_1:94.45\trank_2:90.83\tpolarity:pos\n",
      "metricName:R@5\trank_1:87.55\trank_2:83.85\tpolarity:pos\n",
      "metricName:MRR\trank_1:68.92\trank_2:66.38\tpolarity:pos\n",
      "metricName:Mean Rank\trank_1:3.39\trank_2:4.04\tpolarity:neg\n",
      "metricName:R@1\trank_1:55.16\trank_2:53.33\tpolarity:pos\n",
      "metricName:R@10\trank_1:92.95\trank_2:90.38\tpolarity:pos\n",
      "metricName:R@5\trank_1:86.26\trank_2:82.42\tpolarity:pos\n",
      "metricName:R@1\trank_1:44.45\trank_2:44.75\tpolarity:neg\n",
      "metricName:R@10\trank_1:83.78\trank_2:82.75\tpolarity:pos\n",
      "metricName:R@5\trank_1:68.9\trank_2:68.4\tpolarity:pos\n",
      "metricName:MRR (x 100)\trank_1:56.2\trank_2:56.05\tpolarity:pos\n",
      "metricName:Mean\trank_1:5.41\trank_2:5.72\tpolarity:neg\n",
      "metricName:NDCG (x 100)\trank_1:77.92\trank_2:76.14\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:85.0\trank_2:82.9\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:92.4\trank_2:91.7\tpolarity:pos\n",
      "metricName:BLEU\trank_1:18.3\trank_2:17.2\tpolarity:pos\n",
      "metricName:MultiWOZ (Inform)\trank_1:78.1\trank_2:91.4\tpolarity:neg\n",
      "metricName:MultiWOZ (Success)\trank_1:67.1\trank_2:72.9\tpolarity:neg\n",
      "metricName:BLEU\trank_1:18.6\trank_2:17.2\tpolarity:pos\n",
      "metricName:MultiWOZ (Inform)\trank_1:76.3\trank_2:90.2\tpolarity:neg\n",
      "metricName:MultiWOZ (Success)\trank_1:60.4\trank_2:75.5\tpolarity:neg\n",
      "metricName:F\trank_1:46.8\trank_2:38.0\tpolarity:pos\n",
      "metricName:P\trank_1:44.3\trank_2:36.3\tpolarity:pos\n",
      "metricName:R\trank_1:49.6\trank_2:39.7\tpolarity:pos\n",
      "metricName:VI\trank_1:93.3\trank_2:91.5\tpolarity:pos\n",
      "metricName:1-1\trank_1:59.7\trank_2:59.7\tpolarity:pos\n",
      "metricName:1-1\trank_1:53.1\trank_2:52.1\tpolarity:pos\n",
      "metricName:Local\trank_1:81.9\trank_2:77.8\tpolarity:pos\n",
      "metricName:Shen F-1\trank_1:55.1\trank_2:53.8\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:83.2\trank_2:82.1\tpolarity:pos\n",
      "metricName:F1\trank_1:86.3\trank_2:86.1\tpolarity:pos\n",
      "metricName:Relation F1\trank_1:50.84\trank_2:48.40\tpolarity:pos\n",
      "metricName:P@10%\trank_1:92.3\trank_2:87.5\tpolarity:pos\n",
      "metricName:P@30%\trank_1:86.7\trank_2:74.1\tpolarity:pos\n",
      "metricName:F1 (v2)\trank_1:69.1\trank_2:67.1\tpolarity:pos\n",
      "metricName:F1c (v2)\trank_1:66.5\trank_2:61.1\tpolarity:pos\n",
      "metricName:Kendall's Tau\trank_1:0.7672\trank_2:0.7476\tpolarity:pos\n",
      "metricName:Average MPJPE (mm)\trank_1:44.1\trank_2:45.1\tpolarity:neg\n",
      "metricName:Frames Needed\trank_1:243\trank_2:243\tpolarity:pos\n",
      "metricName:MAE\trank_1:1.1101\trank_2:1.4038\tpolarity:neg\n",
      "metricName:MSE\trank_1:0.1199\trank_2:0.1434\tpolarity:neg\n",
      "metricName:MAE\trank_1:1.1651\trank_2:1.4063\tpolarity:neg\n",
      "metricName:MSE\trank_1:0.1210\trank_2:0.1354\tpolarity:neg\n",
      "metricName:Average MPJPE (mm)\trank_1:56.7\trank_2:60.8\tpolarity:neg\n",
      "metricName:Number of Frames Per View\trank_1:1\trank_2:1\tpolarity:pos\n",
      "metricName:Number of Views\trank_1:1\trank_2:1\tpolarity:pos\n",
      "metricName:Average MPJPE (mm)\trank_1:7.3\trank_2:17.68\tpolarity:neg\n",
      "metricName:3DPCK\trank_1:85.3\trank_2:75.8\tpolarity:pos\n",
      "metricName:PCP3D\trank_1:98.2\trank_2:97.6\tpolarity:pos\n",
      "metricName:PCP3D\trank_1:97.4\trank_2:96.79\tpolarity:pos\n",
      "metricName:MRPE\trank_1:88.1\trank_2:120.0\tpolarity:neg\n",
      "metricName:OOB Rate (10^‚àí3) \trank_1:1.733\trank_2:3.874\tpolarity:neg\n",
      "metricName:Path Difference\trank_1:0.581\trank_2:0.571\tpolarity:pos\n",
      "metricName:Path Length\trank_1:0.573\trank_2:0.702\tpolarity:neg\n",
      "metricName:Player Distance \trank_1:0.423\trank_2:0.417\tpolarity:pos\n",
      "metricName:Step Change (10^‚àí3)\trank_1:2.565\trank_2:4.811\tpolarity:neg\n",
      "metricName:MAE (PM2.5)\trank_1:11.56\trank_2:12.12\tpolarity:neg\n",
      "metricName:MAE (10% missing)\trank_1:0.219\trank_2:0.248\tpolarity:neg\n",
      "metricName:MSE (10^2, 50% missing)\trank_1:0.285\trank_2:0.447\tpolarity:neg\n",
      "metricName:L2 Loss (10^-4)\trank_1:3.54\trank_2:4.51\tpolarity:neg\n",
      "metricName:MSE (10% missing)\trank_1:0.334\trank_2:0.355\tpolarity:neg\n",
      "metricName:MAE (10% of data as GT)\trank_1:0.281\trank_2:0.390\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:83.0\trank_2:82.5\tpolarity:pos\n",
      "metricName:User Study Score\trank_1:3.24\trank_2:3.17\tpolarity:pos\n",
      "metricName:User Study Score\trank_1:4.13\trank_2:3.75\tpolarity:pos\n",
      "metricName:User Study Score\trank_1:3.52\trank_2:3.50\tpolarity:pos\n",
      "metricName:LPIPS\trank_1:0.2733\trank_2:0.440\tpolarity:neg\n",
      "metricName:MPS\trank_1:0.6956\trank_2:0.578\tpolarity:pos\n",
      "metricName:PSNR\trank_1:17.62\trank_2:17.59\tpolarity:pos\n",
      "metricName:Runtime(s)\trank_1:0.53\trank_2:0.5\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.6645\trank_2:0.596\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.05\trank_2:32.91\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.919\trank_2:0.916\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.91\trank_2:33.64\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.941\trank_2:0.938\tpolarity:pos\n",
      "metricName:PSNR\trank_1:37.28\trank_2:36.40\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.97\trank_2:0.965\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.29\trank_2:30.27\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.906\trank_2:0.897\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.65\trank_2:30.41\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.894\trank_2:0.89\tpolarity:pos\n",
      "metricName:PSNR\trank_1:24.27\trank_2:22.07\tpolarity:pos\n",
      "metricName:PSNR\trank_1:35.77\trank_2:30.23\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.9846\trank_2:0.98\tpolarity:pos\n",
      "metricName:BLEU\trank_1:39.2\trank_2:34.9\tpolarity:pos\n",
      "metricName:BLEU\trank_1:40.6\trank_2:35.2\tpolarity:pos\n",
      "metricName:BLEU\trank_1:37.5\trank_2:36.2\tpolarity:pos\n",
      "metricName:BLEU\trank_1:39.5\trank_2:33.1\tpolarity:pos\n",
      "metricName:BLEU\trank_1:27.0\trank_2:20.4\tpolarity:pos\n",
      "metricName:BLEU\trank_1:22.5\trank_2:17.0\tpolarity:pos\n",
      "metricName:BLEU\trank_1:29.7\trank_2:28.3\tpolarity:pos\n",
      "metricName:BLEU\trank_1:21\trank_2:33.3\tpolarity:neg\n",
      "metricName:BLEU (EN-DE)\trank_1:39.7\trank_2:39.4\tpolarity:pos\n",
      "metricName:Meteor (EN-DE)\trank_1:56.8\trank_2:58.7\tpolarity:neg\n",
      "metricName:Dice Score\trank_1:0.8962\trank_2:0.895\tpolarity:pos\n",
      "metricName:Dice\trank_1:0.5349\trank_2:0.4867\tpolarity:pos\n",
      "metricName:Precision\trank_1:0.6331\trank_2:0.6000\tpolarity:pos\n",
      "metricName:Recall\trank_1:0.5243\trank_2:0.4752\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:0.804\trank_2:0.7341\tpolarity:pos\n",
      "metricName:Avg.\trank_1:0.817\trank_2:0.813\tpolarity:pos\n",
      "metricName:TC\trank_1:0.817\trank_2:0.807\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:87\trank_2:85\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:0.84\trank_2:0.84\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:0.9071\trank_2:0.9050\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:0.87049\trank_2:0.8037\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:92.76\trank_2:0.9258\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:0.8690000000000001\trank_2:0.82\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.9887\trank_2:0.9886\tpolarity:pos\n",
      "metricName:F1 score\trank_1:0.8690\trank_2:0.8316\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.9920\trank_2:0.9914\tpolarity:pos\n",
      "metricName:F1 score\trank_1:0.8271\trank_2:0.8957\tpolarity:neg\n",
      "metricName:AUC\trank_1:0.9914\trank_2:0.9898\tpolarity:pos\n",
      "metricName:F1 score\trank_1:0.8475\trank_2:0.8373\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:81.3\trank_2:76.8\tpolarity:pos\n",
      "metricName:Dice\trank_1:94.23\trank_2:92.27\tpolarity:pos\n",
      "metricName:IoU\trank_1:89.46\trank_2:85.6\tpolarity:pos\n",
      "metricName:IoU\trank_1:77.62\trank_2:77.24\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.9946\trank_2:0.9849\tpolarity:pos\n",
      "metricName:F1 score\trank_1:0.9904\trank_2:0.9690\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.8953\trank_2:0.8676\tpolarity:pos\n",
      "metricName:Dice\trank_1:0.7088\trank_2:0.707\tpolarity:pos\n",
      "metricName:F1-score\trank_1:0.8216\trank_2:0.8004\tpolarity:pos\n",
      "metricName:Hausdorff\trank_1:11.3141\trank_2:12.6723\tpolarity:neg\n",
      "metricName:AUC\trank_1:0.9419\trank_2:0.9396\tpolarity:pos\n",
      "metricName:F1 score\trank_1:0.8920\trank_2:0.8799\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:0.869\trank_2:0.780\tpolarity:pos\n",
      "metricName:Per-class Accuracy\trank_1:25\trank_2:17\tpolarity:pos\n",
      "metricName:Per-pixel Accuracy\trank_1:71\trank_2:52\tpolarity:pos\n",
      "metricName:FID\trank_1:31.9\trank_2:28.3\tpolarity:pos\n",
      "metricName:mIoU\trank_1:49\trank_2:48.8\tpolarity:pos\n",
      "metricName:Class IOU\trank_1:0.32\trank_2:0.16\tpolarity:pos\n",
      "metricName:Per-class Accuracy\trank_1:40\trank_2:22\tpolarity:pos\n",
      "metricName:Per-pixel Accuracy\trank_1:85\trank_2:58\tpolarity:pos\n",
      "metricName:mAP\trank_1:38.8\trank_2:36.9\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:Kernel Inception Distance\trank_1:10.23\trank_2:11.52\tpolarity:neg\n",
      "metricName:mIoU\trank_1:63.3\trank_2:59.6\tpolarity:pos\n",
      "metricName:Class IOU\trank_1:0.26\trank_2:0.09\tpolarity:pos\n",
      "metricName:Per-class Accuracy\trank_1:46\trank_2:22\tpolarity:pos\n",
      "metricName:Per-pixel Accuracy\trank_1:70\trank_2:42\tpolarity:pos\n",
      "metricName:mIoU\trank_1:57.5\trank_2:49.9\tpolarity:pos\n",
      "metricName:classification score\trank_1:78.1\trank_2:59.4\tpolarity:pos\n",
      "metricName:FID\trank_1:34.4\trank_2:38.1\tpolarity:neg\n",
      "metricName:DFID\trank_1:35.6\trank_2:56.2\tpolarity:neg\n",
      "metricName:LPIPS\trank_1:0.505\trank_2:0.43\tpolarity:pos\n",
      "metricName:FID\trank_1:13.73\trank_2:14.3\tpolarity:neg\n",
      "metricName:FID\trank_1:48.6\trank_2:63.3\tpolarity:neg\n",
      "metricName:mIoU\trank_1:40.4\trank_2:30.8\tpolarity:pos\n",
      "metricName:Kernel Inception Distance\trank_1:11.40\trank_2:11.61\tpolarity:neg\n",
      "metricName:Classification Error\trank_1:2.12\trank_2:4.10\tpolarity:neg\n",
      "metricName:FID\trank_1:17.0\trank_2:19.1\tpolarity:neg\n",
      "metricName:mIoU\trank_1:44.1\trank_2:42.1\tpolarity:pos\n",
      "metricName:FID\trank_1:26.9\trank_2:44.2\tpolarity:neg\n",
      "metricName:DFID\trank_1:26.1\trank_2:53.6\tpolarity:neg\n",
      "metricName:mIoU (13 classes)\trank_1:62.0\trank_2:57.0\tpolarity:pos\n",
      "metricName:LPIPS\trank_1:0.19\trank_2:0.24\tpolarity:neg\n",
      "metricName:PSNR\trank_1:21.33\trank_2:19.67\tpolarity:pos\n",
      "metricName:RMSE\trank_1:24.28\trank_2:30.75\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.84\trank_2:0.82\tpolarity:pos\n",
      "metricName:free-form mask l2 err\trank_1:1.6\trank_2:4.7\tpolarity:neg\n",
      "metricName:rect mask l1 error\trank_1:8.6\trank_2:8.6\tpolarity:pos\n",
      "metricName:rect mask l2 err\trank_1:2.0\trank_2:2.1\tpolarity:neg\n",
      "metricName:10-20% Mask PSNR\trank_1:32.67\trank_2:28.73\tpolarity:pos\n",
      "metricName:20-30% Mask PSNR\trank_1:30.32\trank_2:26.16\tpolarity:pos\n",
      "metricName:30-40% Mask PSNR\trank_1:24.85\trank_2:24.26\tpolarity:pos\n",
      "metricName:40-50% Mask PSNR\trank_1:23.10\trank_2:22.62\tpolarity:pos\n",
      "metricName:FID\trank_1:13.73\trank_2:16.37\tpolarity:neg\n",
      "metricName:IS\trank_1:12.29\trank_2:11.77\tpolarity:pos\n",
      "metricName:FID\trank_1:3.36\trank_2:5.7\tpolarity:neg\n",
      "metricName:Inception score\trank_1:148.2\trank_2:124.5\tpolarity:pos\n",
      "metricName:FID\trank_1:7.15\trank_2:7.22\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:9.74\trank_2:9.34\tpolarity:pos\n",
      "metricName:FID\trank_1:9.07\trank_2:9.67\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:37.10\trank_2:25.96\tpolarity:pos\n",
      "metricName:FID\trank_1:3.6\trank_2:14.73\tpolarity:neg\n",
      "metricName:Inception score\trank_1:10.21\trank_2:10.14\tpolarity:pos\n",
      "metricName:FID\trank_1:48.68\trank_2:55.28\tpolarity:neg\n",
      "metricName:Inception score\trank_1:3.26\trank_2:3.20\tpolarity:pos\n",
      "metricName:Inception score\trank_1:4.86\trank_2:4.77\tpolarity:pos\n",
      "metricName:FID\trank_1:101.42\trank_2:106.37\tpolarity:neg\n",
      "metricName:Acc\trank_1:20.4\trank_2:18.4\tpolarity:pos\n",
      "metricName:LPIPS\trank_1:0.461\trank_2:0.456\tpolarity:pos\n",
      "metricName:Real\trank_1:21.0\trank_2:22.6\tpolarity:neg\n",
      "metricName:FID\trank_1:20.79\trank_2:23.93\tpolarity:neg\n",
      "metricName:Inception score\trank_1:33.34\trank_2:25.70\tpolarity:pos\n",
      "metricName:IS\trank_1:3.323\trank_2:3.506\tpolarity:neg\n",
      "metricName:PCKh\trank_1:0.94\trank_2:0.93\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.311\trank_2:0.313\tpolarity:neg\n",
      "metricName:mask-IS\trank_1:3.773\trank_2:3.872\tpolarity:neg\n",
      "metricName:mask-SSIM\trank_1:0.811\trank_2:0.816\tpolarity:neg\n",
      "metricName:IS\trank_1:3.476\trank_2:3.430\tpolarity:pos\n",
      "metricName:PCKh\trank_1:0.95\trank_2:0.97\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.778\trank_2:0.778\tpolarity:pos\n",
      "metricName:FID\trank_1:36.4\trank_2:40.85\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:11\trank_2:14.7\tpolarity:neg\n",
      "metricName:FID\trank_1:41.65\trank_2:54.7\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:17.8\trank_2:15.6\tpolarity:pos\n",
      "metricName:FID\trank_1:20.27\trank_2:31.25\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:9.3\trank_2:8.1\tpolarity:pos\n",
      "metricName:FID\trank_1:29.57\trank_2:34.31\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:10.8\trank_2:9.8\tpolarity:pos\n",
      "metricName:FID\trank_1:28.26\trank_2:29.00\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:12.3\trank_2:10.71\tpolarity:pos\n",
      "metricName:FID\trank_1:24.76\trank_2:29.65\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:14.21\trank_2:13.8\tpolarity:pos\n",
      "metricName:Acc\trank_1:100\trank_2:100\tpolarity:pos\n",
      "metricName:Acc\trank_1:99.8\trank_2:99.7\tpolarity:pos\n",
      "metricName:Sensitivity\trank_1:99.8\trank_2:99.7\tpolarity:pos\n",
      "metricName:AUC (ABPA)\trank_1:0.687\trank_2:0.685\tpolarity:pos\n",
      "metricName:AUC (Aspergillus)\trank_1:0.640\trank_2:0.641\tpolarity:neg\n",
      "metricName:AUC (Diabetes)\trank_1:0.771\trank_2:0.764\tpolarity:pos\n",
      "metricName:AUC (E. Coli)\trank_1:0.701\trank_2:0.697\tpolarity:pos\n",
      "metricName:AUC (I. Obstruction)\trank_1:0.577\trank_2:0.578\tpolarity:neg\n",
      "metricName:AUC (K. Pneumonia)\trank_1:0.718\trank_2:0.715\tpolarity:pos\n",
      "metricName:I. Obstruction\trank_1:0.577\trank_2:0.578\tpolarity:neg\n",
      "metricName:MAP\trank_1:55.9\trank_2:40.4\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:72.4\trank_2:63.3\tpolarity:pos\n",
      "metricName:Rank-10\trank_1:87.7\trank_2:80.4\tpolarity:pos\n",
      "metricName:Rank-5\trank_1:84\trank_2:75.8\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:78.0\trank_2:76.2\tpolarity:pos\n",
      "metricName:mAP\trank_1:65.1\trank_2:58.3\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:88.3\trank_2:85.3\tpolarity:pos\n",
      "metricName:mAP\trank_1:65.1\trank_2:65.2\tpolarity:neg\n",
      "metricName:mAP\trank_1:22.9\trank_2:20.7\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:70.1\trank_2:46.4\tpolarity:pos\n",
      "metricName:Rank-10\trank_1:88.6\trank_2:68.0\tpolarity:pos\n",
      "metricName:Rank-5\trank_1:84.1\trank_2:62.3\tpolarity:pos\n",
      "metricName:mAP\trank_1:43.3\trank_2:26.2\tpolarity:pos\n",
      "metricName:MAP\trank_1:71.5\trank_2:43\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:87.5\trank_2:75.1\tpolarity:pos\n",
      "metricName:Rank-10\trank_1:96.8\trank_2:91.6\tpolarity:pos\n",
      "metricName:Rank-5\trank_1:95.2\trank_2:87.6\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:56.5\trank_2:43.6\tpolarity:pos\n",
      "metricName:mAP\trank_1:24.4\trank_2:23.6\tpolarity:pos\n",
      "metricName:mAP (All-search & Single-shot)\trank_1:57.51\trank_2:55.13\tpolarity:pos\n",
      "metricName:rank1\trank_1:61.68\trank_2:57.34\tpolarity:pos\n",
      "metricName:mAP(V2T)\trank_1:83.28\trank_2:73.78\tpolarity:pos\n",
      "metricName:rank1(V2T)\trank_1:91.05\trank_2:83.92\tpolarity:pos\n",
      "metricName:3DIoU\trank_1:77.20\trank_2:62.77\tpolarity:pos\n",
      "metricName:3DIoU\trank_1:79.79\trank_2:79.36\tpolarity:pos\n",
      "metricName:3DIoU\trank_1:82.17\trank_2:78.79\tpolarity:pos\n",
      "metricName:Animals\trank_1:0.64\trank_2:0.61\tpolarity:pos\n",
      "metricName:Humans\trank_1:0.14\trank_2:0.05\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:53.8\trank_2:50.9\tpolarity:pos\n",
      "metricName:MAP\trank_1:66.3\trank_2:63.2\tpolarity:pos\n",
      "metricName:mAP\trank_1:93.2\trank_2:92.4\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:41.1\trank_2:40.2\tpolarity:pos\n",
      "metricName:Vid acc@1\trank_1:84.9\trank_2:84.8\tpolarity:pos\n",
      "metricName:Vid acc@5\trank_1:96.7\trank_2:95.8\tpolarity:pos\n",
      "metricName:CS\trank_1:71.0\trank_2:63.6\tpolarity:pos\n",
      "metricName:Top-1 Accuracy\trank_1:86.1\trank_2:85.8\tpolarity:pos\n",
      "metricName:Top-5 Accuracy\trank_1:97.3\trank_2:96.5\tpolarity:pos\n",
      "metricName:Top-1 Accuracy\trank_1:72.3\trank_2:71.7\tpolarity:pos\n",
      "metricName:mAP\trank_1:86.9\trank_2:85.6\tpolarity:pos\n",
      "metricName:F-Score\trank_1:84.7\trank_2:81.8\tpolarity:pos\n",
      "metricName:Jaccard (Mean)\trank_1:84.3\trank_2:83.4\tpolarity:pos\n",
      "metricName:MSE\trank_1:24.4\trank_2:48.4\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.947\trank_2:0.891\tpolarity:pos\n",
      "metricName:Test Error\trank_1:15.99\trank_2:93.07\tpolarity:neg\n",
      "metricName:Test Error\trank_1:4.03\trank_2:5.98\tpolarity:neg\n",
      "metricName:MAE\trank_1:1620\trank_2:1782.8\tpolarity:neg\n",
      "metricName:MSE\trank_1:369\trank_2:429.9\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.901\trank_2:0.790\tpolarity:pos\n",
      "metricName:Hit@1\trank_1:87.7\trank_2:86.8\tpolarity:pos\n",
      "metricName:FVD score\trank_1:94\trank_2:103.3\tpolarity:neg\n",
      "metricName:Inception Score\trank_1:22.91\trank_2:21.45\tpolarity:pos\n",
      "metricName:Inception Score\trank_1:15.20\trank_2:13.62\tpolarity:pos\n",
      "metricName:Inception Score\trank_1:28.87\trank_2:27.38\tpolarity:pos\n",
      "metricName:text-to-video Mean Rank\trank_1:14.6\trank_2:15.3\tpolarity:neg\n",
      "metricName:text-to-video Median Rank\trank_1:2\trank_2:2\tpolarity:pos\n",
      "metricName:text-to-video R@1\trank_1:45.6\trank_2:44.5\tpolarity:pos\n",
      "metricName:text-to-video R@10\trank_1:81.7\trank_2:81.6\tpolarity:pos\n",
      "metricName:text-to-video R@5\trank_1:72.6\trank_2:71.4\tpolarity:pos\n",
      "metricName:video-to-text Median Rank\trank_1:2\trank_2:2\tpolarity:pos\n",
      "metricName:video-to-text R@1\trank_1:43.5\trank_2:42.7\tpolarity:pos\n",
      "metricName:video-to-text R@10\trank_1:82.1\trank_2:80.6\tpolarity:pos\n",
      "metricName:video-to-text R@5\trank_1:72.3\trank_2:70.9\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:R@1\trank_1:4.34\trank_2:2.70\tpolarity:pos\n",
      "metricName:R@10\trank_1:13.97\trank_2:8.93\tpolarity:pos\n",
      "metricName:R@100\trank_1:21.78\trank_2:15.34\tpolarity:pos\n",
      "metricName:text-to-video Median Rank\trank_1:4\trank_2:9\tpolarity:neg\n",
      "metricName:text-to-video R@1\trank_1:28.9\trank_2:16.7\tpolarity:pos\n",
      "metricName:text-to-video R@10\trank_1:70.0\trank_2:52.3\tpolarity:pos\n",
      "metricName:text-to-video Mean Rank\trank_1:58.0\trank_2:58.0\tpolarity:pos\n",
      "metricName:text-to-video Median Rank\trank_1:11.0\trank_2:12.3\tpolarity:neg\n",
      "metricName:text-to-video R@1\trank_1:21.6\trank_2:18.8\tpolarity:pos\n",
      "metricName:text-to-video R@10\trank_1:49.8\trank_2:47.9\tpolarity:pos\n",
      "metricName:text-to-video R@5\trank_1:41.8\trank_2:38.5\tpolarity:pos\n",
      "metricName:text-to-video Median Rank\trank_1:2.0\trank_2:3\tpolarity:neg\n",
      "metricName:text-to-video R@1\trank_1:43.4\trank_2:31.0\tpolarity:pos\n",
      "metricName:text-to-video R@10\trank_1:80.6\trank_2:72.4\tpolarity:pos\n",
      "metricName:text-to-video R@5\trank_1:70.2\trank_2:59.8\tpolarity:pos\n",
      "metricName:text-to-video Mean Rank\trank_1:9.6\trank_2:10.0\tpolarity:neg\n",
      "metricName:text-to-video Median Rank\trank_1:2\trank_2:2\tpolarity:pos\n",
      "metricName:text-to-video R@1\trank_1:47\trank_2:46.2\tpolarity:pos\n",
      "metricName:text-to-video R@10\trank_1:85.9\trank_2:84.6\tpolarity:pos\n",
      "metricName:text-to-video R@5\trank_1:76.8\trank_2:76.1\tpolarity:pos\n",
      "metricName:video-to-text Median Rank\trank_1:1\trank_2:1\tpolarity:pos\n",
      "metricName:video-to-text R@1\trank_1:58.7\trank_2:62.0\tpolarity:neg\n",
      "metricName:video-to-text R@10\trank_1:91.6\trank_2:92.6\tpolarity:neg\n",
      "metricName:video-to-text R@5\trank_1:85.6\trank_2:87.3\tpolarity:neg\n",
      "metricName:text-to-video Mean Rank\trank_1:7.5\trank_2:16\tpolarity:neg\n",
      "metricName:text-to-video Median Rank\trank_1:2.0\trank_2:3.3\tpolarity:neg\n",
      "metricName:text-to-video R@1\trank_1:40.5\trank_2:28.7\tpolarity:pos\n",
      "metricName:text-to-video R@5\trank_1:72.4\trank_2:61.4\tpolarity:pos\n",
      "metricName:text-to-video R@50\trank_1:98.2\trank_2:94.5\tpolarity:pos\n",
      "metricName:text-to-video Mean Rank\trank_1:45.5\trank_2:52.8\tpolarity:neg\n",
      "metricName:text-to-video Median Rank\trank_1:4\trank_2:6\tpolarity:neg\n",
      "metricName:text-to-video R@1\trank_1:29.8\trank_2:23.1\tpolarity:pos\n",
      "metricName:text-to-video R@10\trank_1:66.2\trank_2:61.8\tpolarity:pos\n",
      "metricName:text-to-video R@5\trank_1:55.5\trank_2:49.8\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.12\trank_2:28.86\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.870\trank_2:0.858\tpolarity:pos\n",
      "metricName:tOF\trank_1:2.15\trank_2:2.67\tpolarity:neg\n",
      "metricName:PSNR\trank_1:36.10\trank_2:36.10\tpolarity:pos\n",
      "metricName:PSNR\trank_1:35.39\trank_2:35.21\tpolarity:pos\n",
      "metricName:Interpolation Error\trank_1:4.22\trank_2:4.48\tpolarity:neg\n",
      "metricName:F1-score (Augmented)\trank_1:50.7\trank_2:51.09\tpolarity:neg\n",
      "metricName:F1-score (Canonical)\trank_1:50.2\trank_2:49.71\tpolarity:pos\n",
      "metricName:F1-score (Augmented)\trank_1:63.9\trank_2:62.37\tpolarity:pos\n",
      "metricName:F1-score (Canonical)\trank_1:62.1\trank_2:61.42\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.85\trank_2:31.83\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.86\trank_2:32.8\tpolarity:pos\n",
      "metricName:PSNR\trank_1:36.43\trank_2:36.08\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.49\trank_2:33.37\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.79\trank_2:31.6\tpolarity:pos\n",
      "metricName:PSNR\trank_1:38.97\trank_2:38.13\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.55\trank_2:30.37\tpolarity:pos\n",
      "metricName:PSNR\trank_1:35.86\trank_2:35.7\tpolarity:pos\n",
      "metricName:PSNR\trank_1:29.56\trank_2:29.42\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.08\trank_2:34.06\tpolarity:pos\n",
      "metricName:AUC-ROC\trank_1:97.21\trank_2:94.83\tpolarity:pos\n",
      "metricName:ROC AUC\trank_1:84.03\trank_2:82.30\tpolarity:pos\n",
      "metricName:AP\trank_1:77.81\trank_2:75.41\tpolarity:pos\n",
      "metricName:Average-mAP\trank_1:40.7\trank_2:39.9\tpolarity:pos\n",
      "metricName:Average-mAP\trank_1:75.1\trank_2:63.3\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:81.4\trank_2:76.6\tpolarity:pos\n",
      "metricName:FPS on CPU\trank_1:1.3\trank_2:50.3\tpolarity:neg\n",
      "metricName:PSNR\trank_1:40.708\trank_2:39.916\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.983\trank_2:0.977\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:42.53\trank_2:36.25\tpolarity:pos\n",
      "metricName:Action@1\trank_1:47.7\trank_2:44.5\tpolarity:pos\n",
      "metricName:Noun@1\trank_1:57.3\trank_2:55.1\tpolarity:pos\n",
      "metricName:Verb@1\trank_1:72.2\trank_2:69.1\tpolarity:pos\n",
      "metricName:mAP\trank_1:31.0\trank_2:28.7\tpolarity:pos\n",
      "metricName:Top-1 Accuracy\trank_1:69.6\trank_2:69.02\tpolarity:pos\n",
      "metricName:Top-5 Accuracy\trank_1:92.7\trank_2:92.70\tpolarity:pos\n",
      "metricName:Average accuracy of 3 splits\trank_1:84.36\trank_2:83.8\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:80.11\trank_2:74.03\tpolarity:pos\n",
      "metricName:3-fold Accuracy\trank_1:98.64\trank_2:98.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.3\trank_2:82.6\tpolarity:pos\n",
      "metricName:Accuracy (CS)\trank_1:97.0\trank_2:95.66\tpolarity:pos\n",
      "metricName:mAP@0.3\trank_1:56.0\trank_2:53.9\tpolarity:pos\n",
      "metricName:mAP@0.4\trank_1:47.4\trank_2:46.8\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:38.8\trank_2:37.4\tpolarity:pos\n",
      "metricName:mAP\trank_1:84.4\trank_2:53.8\tpolarity:pos\n",
      "metricName:Val\trank_1:97.4\trank_2:96.70\tpolarity:pos\n",
      "metricName:Video hit@1 \trank_1:75.5\trank_2:74.9\tpolarity:pos\n",
      "metricName:Video hit@5\trank_1:92.8\trank_2:92.6\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:57.0\trank_2:56.8\tpolarity:pos\n",
      "metricName:Top 5 Accuracy\trank_1:83.7\trank_2:84.1\tpolarity:neg\n",
      "metricName:Top 1 Accuracy\trank_1:84.33\trank_2:83.77\tpolarity:pos\n",
      "metricName:Top 5 Accuracy\trank_1:96.85\trank_2:96.56\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:85.5\trank_2:81\tpolarity:pos\n",
      "metricName:mAP (Val)\trank_1:28.3\trank_2:27.7\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:74.9\trank_2:69.9\tpolarity:pos\n",
      "metricName:Accuracy (Cross-Setup)\trank_1:95.3\trank_2:90.7\tpolarity:pos\n",
      "metricName:Top-1 (%)\trank_1:64.4\trank_2:50.53\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.86\trank_2:80.23\tpolarity:pos\n",
      "metricName:mAP@0.1:0.5\trank_1:53.2\trank_2:51.6\tpolarity:pos\n",
      "metricName:mAP@0.1:0.7\trank_1:42.6\trank_2:41.9\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:34.6\trank_2:33.7\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:35.9\trank_2:34.6\tpolarity:pos\n",
      "metricName:avg-mAP (0.1-0.5)\trank_1:53.2\trank_2:51.6\tpolarity:pos\n",
      "metricName:avg-mAP (0.1-0.9)\trank_1:33.6\trank_2:33.0\tpolarity:pos\n",
      "metricName:avg-mAP (0.3-0.7)\trank_1:33.4\trank_2:32.9\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:42.7\trank_2:42.3\tpolarity:pos\n",
      "metricName:Mean mAP\trank_1:26.1\trank_2:26\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:40.1\trank_2:37\tpolarity:pos\n",
      "metricName:mAP IOU@0.1\trank_1:68.9\trank_2:61.2\tpolarity:pos\n",
      "metricName:mAP IOU@0.2\trank_1:62.7\trank_2:56.1\tpolarity:pos\n",
      "metricName:mAP IOU@0.3\trank_1:55.0\trank_2:48.1\tpolarity:pos\n",
      "metricName:mAP IOU@0.4\trank_1:44.6\trank_2:39.0\tpolarity:pos\n",
      "metricName:mAP IOU@0.5\trank_1:34.6\trank_2:30.1\tpolarity:pos\n",
      "metricName:mAP IOU@0.6\trank_1:21.8\trank_2:19.2\tpolarity:pos\n",
      "metricName:mAP IOU@0.7\trank_1:10.8\trank_2:10.6\tpolarity:pos\n",
      "metricName:mAP IOU@0.8\trank_1:3.6\trank_2:4.8\tpolarity:neg\n",
      "metricName:mAP IOU@0.9\trank_1:0.6\trank_2:1.0\tpolarity:neg\n",
      "metricName:mAP@AVG(0.1:0.9)\trank_1:33.6\trank_2:30.0\tpolarity:pos\n",
      "metricName:AR@100\trank_1:76.63\trank_2:76.52\tpolarity:pos\n",
      "metricName:AUC (val)\trank_1:69.04\trank_2:68.26\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.906\trank_2:0.892\tpolarity:pos\n",
      "metricName:Decidability\trank_1:1.386\trank_2:0.804\tpolarity:pos\n",
      "metricName:EER\trank_1:0.160\trank_2:0.186\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:56.0\trank_2:55.6\tpolarity:pos\n",
      "metricName:Accuracy (easy)\trank_1:75.7\trank_2:75.5\tpolarity:pos\n",
      "metricName:Accuracy (hard)\trank_1:40.5\trank_2:40.0\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:54.6\trank_2:50.9\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:74.7\trank_2:73.2\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:68.66\trank_2:63.38\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.92\trank_2:99.9\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:82.78\trank_2:78.21\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:85.9\trank_2:83.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:68.5\trank_2:63.5\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:90.03\trank_2:81.57\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:75.59\trank_2:70.29\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:62.7\trank_2:60.9\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:89.6\trank_2:66.7\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:51.35\trank_2:50.57\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:87.79\trank_2:87.73\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:82.99\trank_2:82.92\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.63\trank_2:99.11\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.28\trank_2:94.09\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.5\trank_2:90.98\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.65\trank_2:99.63\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:Accuracy\trank_1:59.05\trank_2:55.63\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.97\trank_2:99.92\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:72.15\trank_2:70.72\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:59.5\trank_2:47.31\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:80.6\trank_2:78.5\tpolarity:pos\n",
      "metricName:Top-5 Accuracy\trank_1:58.2\trank_2:59.2\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:62.49\trank_2:49.44\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:90.44\trank_2:89.8\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:94.73\trank_2:91.68\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:73.15\trank_2:72.43\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.89\trank_2:91.05\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:65.1\trank_2:57.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:76.51\trank_2:68.33\tpolarity:pos\n",
      "metricName:Mean Rank\trank_1:2.85\trank_2:3.05\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:67.66\trank_2:67.17\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:82.92\trank_2:78.55\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:85.41\trank_2:84.01\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:71\trank_2:69.30\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:1.5\trank_2:1.4\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.09\trank_2:90.73\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:39.3\trank_2:32.07\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:84.29\trank_2:80.33\tpolarity:pos\n",
      "metricName:Top-1 Accuracy\trank_1:56.8\trank_2:54.7\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:70.4\trank_2:66.6\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:41.2\trank_2:40.3\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:70.6\trank_2:68.1\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:49.5\trank_2:45.6\tpolarity:pos\n",
      "metricName:meanIOU\trank_1:62.2\trank_2:60.6\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:86.5\trank_2:83.36\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:63.1\trank_2:61.1\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:48.7\trank_2:44.1\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:66.2\trank_2:64.3\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:67.7\trank_2:63.4\tpolarity:pos\n",
      "metricName:Recall\trank_1:33.6\trank_2:31.6\tpolarity:pos\n",
      "metricName:Frame-mAP\trank_1:87.2\trank_2:76.3\tpolarity:pos\n",
      "metricName:Video-mAP 0.5\trank_1:48.8\trank_2:59.9\tpolarity:neg\n",
      "metricName:Frame-mAP\trank_1:74.4\trank_2:73.3\tpolarity:pos\n",
      "metricName:Video-mAP 0.5\trank_1:85.7\trank_2:78.6\tpolarity:pos\n",
      "metricName:mAP IOU@0.5\trank_1:57.1\trank_2:56.9\tpolarity:pos\n",
      "metricName:mAP IOU@0.3\trank_1:70.1\trank_2:68.9\tpolarity:pos\n",
      "metricName:mAP IOU@0.7\trank_1:28.8\trank_2:31.0\tpolarity:neg\n",
      "metricName:mAP IOU@0.4\trank_1:64.9\trank_2:64.0\tpolarity:pos\n",
      "metricName:mAP IOU@0.6\trank_1:45.4\trank_2:46.3\tpolarity:neg\n",
      "metricName:mAP\trank_1:36.82\trank_2:37.56\tpolarity:neg\n",
      "metricName:mAP IOU@0.5\trank_1:54.34\trank_2:54.33\tpolarity:pos\n",
      "metricName:mAP IOU@0.75\trank_1:37.76\trank_2:39.13\tpolarity:neg\n",
      "metricName:mAP IOU@0.95\trank_1:8.93\trank_2:8.41\tpolarity:pos\n",
      "metricName:Harmonic mean\trank_1:43\trank_2:41.3\tpolarity:pos\n",
      "metricName:mAP\trank_1:25.9\trank_2:25.7\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:93.52\trank_2:92.19\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:48.9\trank_2:43.9\tpolarity:pos\n",
      "metricName:EM\trank_1:68.0\trank_2:66.0\tpolarity:pos\n",
      "metricName:EM\trank_1:59.3\trank_2:50.2\tpolarity:pos\n",
      "metricName:Exact Match\trank_1:55.9\trank_2:41.6\tpolarity:pos\n",
      "metricName:EM\trank_1:70.0\trank_2:66.2\tpolarity:pos\n",
      "metricName:EM\trank_1:64.2\trank_2:61.3\tpolarity:pos\n",
      "metricName:EM (Quasar-T)\trank_1:42.3\trank_2:42.2\tpolarity:pos\n",
      "metricName:F1 (Quasar-T)\trank_1:49.6\trank_2:49.3\tpolarity:pos\n",
      "metricName:EM\trank_1:44.39\trank_2:38.64\tpolarity:pos\n",
      "metricName:F1\trank_1:52.35\trank_2:47.09\tpolarity:pos\n",
      "metricName:KILT-EM\trank_1:32.69\trank_2:31.99\tpolarity:pos\n",
      "metricName:KILT-F1\trank_1:37.91\trank_2:37.58\tpolarity:pos\n",
      "metricName:R-Prec\trank_1:59.49\trank_2:60.66\tpolarity:neg\n",
      "metricName:Recall@5\trank_1:67.06\trank_2:46.79\tpolarity:pos\n",
      "metricName:EM\trank_1:31.77\trank_2:26.97\tpolarity:pos\n",
      "metricName:F1\trank_1:41.56\trank_2:36.03\tpolarity:pos\n",
      "metricName:KILT-EM\trank_1:9.53\trank_2:3.21\tpolarity:pos\n",
      "metricName:KILT-F1\trank_1:11.27\trank_2:4.1\tpolarity:pos\n",
      "metricName:R-Prec\trank_1:42.92\trank_2:30.59\tpolarity:pos\n",
      "metricName:Recall@5\trank_1:28.39\trank_2:12.59\tpolarity:pos\n",
      "metricName:EM\trank_1:59.6\trank_2:71.27\tpolarity:neg\n",
      "metricName:F1\trank_1:66.53\trank_2:75.88\tpolarity:neg\n",
      "metricName:KILT-EM\trank_1:42.36\trank_2:38.13\tpolarity:pos\n",
      "metricName:KILT-F1\trank_1:46.19\trank_2:40.15\tpolarity:pos\n",
      "metricName:R-Prec\trank_1:61.49\trank_2:48.68\tpolarity:pos\n",
      "metricName:Recall@5\trank_1:68.33\trank_2:57.13\tpolarity:pos\n",
      "metricName:F1\trank_1:27.13\trank_2:22.88\tpolarity:pos\n",
      "metricName:KILT-F1\trank_1:3.0\trank_2:2.34\tpolarity:pos\n",
      "metricName:R-Prec\trank_1:10.83\trank_2:10.67\tpolarity:pos\n",
      "metricName:Recall@5\trank_1:27.25\trank_2:24.56\tpolarity:pos\n",
      "metricName:KILT-RL\trank_1:2.62\trank_2:2.36\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:24.53\trank_2:23.19\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:70.4\trank_2:53.9\tpolarity:pos\n",
      "metricName:F1-Score\trank_1:84.5\trank_2:82.5\tpolarity:pos\n",
      "metricName:EM\trank_1:54.9\trank_2:37.3\tpolarity:pos\n",
      "metricName:F1\trank_1:71.6\trank_2:53.1\tpolarity:pos\n",
      "metricName:EM\trank_1:63.6\trank_2:46.9\tpolarity:pos\n",
      "metricName:F1\trank_1:79.7\trank_2:63.8\tpolarity:pos\n",
      "metricName:EM\trank_1:60.0\trank_2:42.8\tpolarity:pos\n",
      "metricName:F1\trank_1:75.3\trank_2:58.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:56.0\trank_2:55.6\tpolarity:pos\n",
      "metricName:Accuracy (easy)\trank_1:75.7\trank_2:75.5\tpolarity:pos\n",
      "metricName:Accuracy (hard)\trank_1:40.5\trank_2:40.0\tpolarity:pos\n",
      "metricName:L1\trank_1:0.0791\trank_2:0.08\tpolarity:neg\n",
      "metricName:MSE\trank_1:0.023\trank_2:0.7814\tpolarity:neg\n",
      "metricName:MOTA\trank_1:84.24\trank_2:83.34\tpolarity:pos\n",
      "metricName:MOTP\trank_1:85.73\trank_2:85.23\tpolarity:pos\n",
      "metricName:amota\trank_1:0.7\trank_2:0.68\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:86.9\trank_2:86.7\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.50\trank_2:98.5\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:93.99\trank_2:93.5\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.02\trank_2:98.60\tpolarity:pos\n",
      "metricName:PCK@0.1\trank_1:58.4\trank_2:45.2\tpolarity:pos\n",
      "metricName:PCK@0.2\trank_1:78.1\trank_2:69.6\tpolarity:pos\n",
      "metricName:PCK@0.3\trank_1:85.9\trank_2:80.8\tpolarity:pos\n",
      "metricName:PCK@0.4\trank_1:89.8\trank_2:87.5\tpolarity:pos\n",
      "metricName:PCK@0.5\trank_1:92.4\trank_2:91.4\tpolarity:pos\n",
      "metricName:Accuracy (Cross-Setup)\trank_1:89.1\trank_2:89.2\tpolarity:neg\n",
      "metricName:Accuracy (Cross-Subject)\trank_1:88.3\trank_2:88.2\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.0\trank_2:92.91\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:81.4\trank_2:73.8\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:47.7\trank_2:38.6\tpolarity:pos\n",
      "metricName:10%\trank_1:60.6\trank_2:58.1\tpolarity:pos\n",
      "metricName:Accuracy (AV I)\trank_1:57\trank_2:53\tpolarity:pos\n",
      "metricName:Accuracy (AV II)\trank_1:75\trank_2:43\tpolarity:pos\n",
      "metricName:Accuracy (CS)\trank_1:76\trank_2:71\tpolarity:pos\n",
      "metricName:Accuracy (CV I)\trank_1:29\trank_2:25\tpolarity:pos\n",
      "metricName:Accuracy (CV II)\trank_1:71\trank_2:56\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.1\trank_2:98.4\tpolarity:pos\n",
      "metricName:Accuracy (Body + Fingers + Face joints)\trank_1:89.64\trank_2:91.12\tpolarity:neg\n",
      "metricName:Accuracy (Body + Fingers joints)\trank_1:91.78\trank_2:91.76\tpolarity:pos\n",
      "metricName:Accuracy (Body joints)\trank_1:89.56\trank_2:91.26\tpolarity:neg\n",
      "metricName:14 gestures accuracy\trank_1:94.6\trank_2:93.57\tpolarity:pos\n",
      "metricName:28 gestures accuracy\trank_1:91.9\trank_2:91.43\tpolarity:pos\n",
      "metricName:Accuracy (CS)\trank_1:94.1\trank_2:91.0\tpolarity:pos\n",
      "metricName:Accuracy (CV)\trank_1:97.1\trank_2:96.5\tpolarity:pos\n",
      "metricName:Average accuracy of 3 splits\trank_1:77.2\trank_2:67.9\tpolarity:pos\n",
      "metricName:mAP@0.50 (CS)\trank_1:92.9\trank_2:92.6\tpolarity:pos\n",
      "metricName:mAP@0.50 (CV)\trank_1:94.4\trank_2:94.2\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:88.51\trank_2:86.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.1\trank_2:89.3\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:37.98\trank_2:36.97\tpolarity:pos\n",
      "metricName:Accuracy (RGB+pose)\trank_1:90.4\trank_2:86.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.5\trank_2:93.4\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:49.6\trank_2:46.5\tpolarity:pos\n",
      "metricName:MSE (10^-2, 50% missing)\trank_1:1.258\trank_2:1.377\tpolarity:neg\n",
      "metricName:MSE\trank_1:0.48\trank_2:0.62\tpolarity:neg\n",
      "metricName:NegLL\trank_1:0.83\trank_2:1.02\tpolarity:neg\n",
      "metricName:MSE\trank_1:0.43\trank_2:0.53\tpolarity:neg\n",
      "metricName:MSE stdev\trank_1:0.050\trank_2:0.029\tpolarity:pos\n",
      "metricName:mse (10^-3)\trank_1:2.208\trank_2:2.231\tpolarity:neg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:RRSE\trank_1:0.0745\trank_2:0.0823\tpolarity:neg\n",
      "metricName:OMQ\trank_1:0.44\trank_2:0.37\tpolarity:pos\n",
      "metricName:avg_fp_quality\trank_1:0.28\trank_2:0.09\tpolarity:pos\n",
      "metricName:avg_label\trank_1:1.0\trank_2:0.97\tpolarity:pos\n",
      "metricName:avg_pairwise\trank_1:0.68\trank_2:0.74\tpolarity:neg\n",
      "metricName:avg_spatial\trank_1:0.51\trank_2:0.59\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:95.5\trank_2:94.7\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.7\trank_2:95.0\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.8\trank_2:91.5\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:82.2\trank_2:81.6\tpolarity:pos\n",
      "metricName:Top-5 Accuracy\trank_1:0.87\trank_2:0.66\tpolarity:pos\n",
      "metricName:Recall@50\trank_1:18.32\trank_2:18.16\tpolarity:pos\n",
      "metricName:Recall@50\trank_1:31.93\trank_2:31.74\tpolarity:pos\n",
      "metricName:mean Recall @20\trank_1:6.9\trank_2:7.1\tpolarity:neg\n",
      "metricName:BLEU\trank_1:0.6607\trank_2:0.647\tpolarity:pos\n",
      "metricName:BLEU\trank_1:35.1\trank_2:34.96\tpolarity:pos\n",
      "metricName:Precision\trank_1:40.9\trank_2:40.8\tpolarity:pos\n",
      "metricName:Recall\trank_1:49.5\trank_2:54.9\tpolarity:neg\n",
      "metricName:Precision\trank_1:97.6\trank_2:89.46\tpolarity:pos\n",
      "metricName:count\trank_1:42.1\trank_2:21.17\tpolarity:pos\n",
      "metricName:BLEU\trank_1:53.6\trank_2:52.1\tpolarity:pos\n",
      "metricName:BLEU\trank_1:49.5\trank_2:44\tpolarity:pos\n",
      "metricName:PARENT\trank_1:58.4\trank_2:52.6\tpolarity:pos\n",
      "metricName:BLEU\trank_1:12.62\trank_2:11.50\tpolarity:pos\n",
      "metricName:BLEU\trank_1:17.50\trank_2:16.50\tpolarity:pos\n",
      "metricName:DLD\trank_1:18.90\trank_2:18.58\tpolarity:pos\n",
      "metricName:BLEU\trank_1:60.56\trank_2:57.1\tpolarity:pos\n",
      "metricName:DLD\trank_1:21.8\trank_2:20.7\tpolarity:pos\n",
      "metricName:BLEU score\trank_1:26.35\trank_2:21.96\tpolarity:pos\n",
      "metricName:CIDER\trank_1:2.60\trank_2:2.18\tpolarity:pos\n",
      "metricName:METEOR\trank_1:25.81\trank_2:23.32\tpolarity:pos\n",
      "metricName:NIST\trank_1:5.24\trank_2:4.77\tpolarity:pos\n",
      "metricName:Precision\trank_1:94.4\trank_2:81.1\tpolarity:pos\n",
      "metricName:count\trank_1:30.8\trank_2:23.8\tpolarity:pos\n",
      "metricName:BLEU\trank_1:17.50\trank_2:16.50\tpolarity:pos\n",
      "metricName:BLEU\trank_1:80.49\trank_2:0.666\tpolarity:pos\n",
      "metricName:METEOR\trank_1:0.613\trank_2:0.180\tpolarity:pos\n",
      "metricName:Precision\trank_1:39.47\trank_2:34.18\tpolarity:pos\n",
      "metricName:Recall\trank_1:51.64\trank_2:51.22\tpolarity:pos\n",
      "metricName:BLEU\trank_1:68.60\trank_2:67.05\tpolarity:pos\n",
      "metricName:METEOR\trank_1:45.25\trank_2:44.49\tpolarity:pos\n",
      "metricName:NIST\trank_1:8.73\trank_2:8.5150\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:2.37\trank_2:2.2355\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:70.82\trank_2:68.94\tpolarity:pos\n",
      "metricName:METEOR\trank_1:0.462\trank_2:0.287\tpolarity:pos\n",
      "metricName:METEOR\trank_1:0.394\trank_2:0.391\tpolarity:pos\n",
      "metricName:ROUGE-1\trank_1:43.57\trank_2:43.47\tpolarity:pos\n",
      "metricName:ROUGE-2\trank_1:14.03\trank_2:14.89\tpolarity:neg\n",
      "metricName:ROUGE-SU4\trank_1:17.37\trank_2:17.41\tpolarity:neg\n",
      "metricName:G-Score (BLEU, Accuracy)\trank_1:74.56\trank_2:66.25\tpolarity:pos\n",
      "metricName:BLEU-1\trank_1:45.7\trank_2:22.9\tpolarity:pos\n",
      "metricName:BLEU\trank_1:44.89\trank_2:44.71\tpolarity:pos\n",
      "metricName:ROUGE\trank_1:41.21\trank_2:41.65\tpolarity:neg\n",
      "metricName:ADD\trank_1:47.69\trank_2:43.82\tpolarity:pos\n",
      "metricName:BLEU\trank_1:75.68\trank_2:71.03\tpolarity:pos\n",
      "metricName:DELETE\trank_1:0.7707\trank_2:0.7548\tpolarity:pos\n",
      "metricName:Exact Match\trank_1:24.8\trank_2:20.96\tpolarity:pos\n",
      "metricName:F1\trank_1:93.17\trank_2:92.51\tpolarity:pos\n",
      "metricName:KEEP\trank_1:0.9184\trank_2:0.8949\tpolarity:pos\n",
      "metricName:Precision\trank_1:96.88\trank_2:98.06\tpolarity:neg\n",
      "metricName:Recall\trank_1:89.74\trank_2:87.56\tpolarity:pos\n",
      "metricName:SARI\trank_1:72.2\trank_2:69.59\tpolarity:pos\n",
      "metricName:Smoothed BLEU-4\trank_1:26.23\trank_2:21.32\tpolarity:pos\n",
      "metricName:Smoothed BLEU-4\trank_1:26.79\trank_2:26.66\tpolarity:pos\n",
      "metricName:Smoothed BLEU-4\trank_1:15.26\trank_2:8.46\tpolarity:pos\n",
      "metricName:Smoothed BLEU-4\trank_1:15.99\trank_2:15.55\tpolarity:pos\n",
      "metricName:Smoothed BLEU-4\trank_1:21.87\trank_2:14.56\tpolarity:pos\n",
      "metricName:Smoothed BLEU-4\trank_1:25.61\trank_2:18.98\tpolarity:pos\n",
      "metricName:Smoothed BLEU-4\trank_1:20.39\trank_2:15.48\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:69.58\trank_2:62.7\tpolarity:pos\n",
      "metricName:Actions Top-1 (S1)\trank_1:34.8\trank_2:33.06\tpolarity:pos\n",
      "metricName:Actions Top-1 (S2)\trank_1:19.06\trank_2:19.49\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:80.2\trank_2:64.6\tpolarity:pos\n",
      "metricName:Accuracy (CS)\trank_1:95.12\trank_2:94.5\tpolarity:pos\n",
      "metricName:Top-1 (%)\trank_1:34.27\trank_2:31.16\tpolarity:pos\n",
      "metricName:Top-5 (%)\trank_1:62.71\trank_2:57.67\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.90\trank_2:95.00\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.5\trank_2:95.0\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:80.1\trank_2:79.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91\trank_2:83.75\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:92.6\trank_2:91.0\tpolarity:pos\n",
      "metricName:MMD\trank_1:0.285\trank_2:0.338\tpolarity:neg\n",
      "metricName:MMD\trank_1:0.134\trank_2:0.195\tpolarity:neg\n",
      "metricName:Hits@10\trank_1:62.6\trank_2:54.2\tpolarity:pos\n",
      "metricName:FID\trank_1:67.7\trank_2:77.67\tpolarity:neg\n",
      "metricName:FSD\trank_1:71.51\trank_2:111.09\tpolarity:neg\n",
      "metricName:MAE\trank_1:6.3\trank_2:6.32\tpolarity:neg\n",
      "metricName:MAE\trank_1:20.5\trank_2:23.5\tpolarity:neg\n",
      "metricName:Average MAE\trank_1:7.2\trank_2:7.32\tpolarity:neg\n",
      "metricName:MAE\trank_1:162.33\trank_2:212.2\tpolarity:neg\n",
      "metricName:MAE\trank_1:57.55\trank_2:57.6\tpolarity:neg\n",
      "metricName:MAE\trank_1:85.6\trank_2:85.6\tpolarity:pos\n",
      "metricName:Accuracy (median)\trank_1:0.778\trank_2:0.764\tpolarity:pos\n",
      "metricName:Accuracy (median)\trank_1:0.738\trank_2:0.723\tpolarity:pos\n",
      "metricName:F1\trank_1:98.35\trank_2:98.3\tpolarity:pos\n",
      "metricName:AVG\trank_1:53.6\trank_2:24.59\tpolarity:pos\n",
      "metricName:F-Score\trank_1:71.3\trank_2:61.7\tpolarity:pos\n",
      "metricName:V-Measure\trank_1:40.4\trank_2:9.8\tpolarity:pos\n",
      "metricName:AVG\trank_1:37.0\trank_2:22.16\tpolarity:pos\n",
      "metricName:F-BC\trank_1:64.0\trank_2:61.7\tpolarity:pos\n",
      "metricName:F_NMI\trank_1:21.4\trank_2:7.96\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:89.1\trank_2:88.8\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:89.9\trank_2:89.6\tpolarity:pos\n",
      "metricName:Kappa\trank_1:84.6\trank_2:84.3\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:84.5\trank_2:78.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:84.0\trank_2:82\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:88.7\trank_2:87.5\tpolarity:pos\n",
      "metricName:Kappa\trank_1:82.3\trank_2:80.4\tpolarity:pos\n",
      "metricName:F1-score (@IoU = 0.3)\trank_1:0.812\trank_2:0.809\tpolarity:pos\n",
      "metricName:F1-score (@IoU = 0.2)\trank_1:0.812\trank_2:0.809\tpolarity:pos\n",
      "metricName:F1-score (@IoU = 0.2)\trank_1:0.828\trank_2:0.826\tpolarity:pos\n",
      "metricName:F1-score (@IoU = 0.3)\trank_1:0.827\trank_2:0.825\tpolarity:pos\n",
      "metricName:F1-score (@IoU = 0.3)\trank_1:0.71\trank_2:0.61\tpolarity:pos\n",
      "metricName:Accuracy \trank_1:0.93\trank_2:0.82\tpolarity:pos\n",
      "metricName:F1 Score\trank_1:0.93\trank_2:0.83\tpolarity:pos\n",
      "metricName:0..5sec\trank_1:12.3\trank_2:13.4\tpolarity:neg\n",
      "metricName:5..20sec\trank_1:6.1\trank_2:6.6\tpolarity:neg\n",
      "metricName:Average\trank_1:7.1\trank_2:7.6\tpolarity:neg\n",
      "metricName:Accuracy (%)\trank_1:95.4\trank_2:95.0\tpolarity:pos\n",
      "metricName:Accuracy \trank_1:0.91\trank_2:0.63\tpolarity:pos\n",
      "metricName:F1 Score\trank_1:0.91\trank_2:0.63\tpolarity:pos\n",
      "metricName:Accuracy \trank_1:0.96\trank_2:0.91\tpolarity:pos\n",
      "metricName:F1 Score\trank_1:0.96\trank_2:0.91\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:96.3\trank_2:96.0\tpolarity:pos\n",
      "metricName:Average\trank_1:4.00\trank_2:4.06\tpolarity:neg\n",
      "metricName:10 sec\trank_1:2.61\trank_2:2.49\tpolarity:pos\n",
      "metricName:3 sec\trank_1:8.25\trank_2:8.59\tpolarity:neg\n",
      "metricName:30 sec\trank_1:1.16\trank_2:1.09\tpolarity:pos\n",
      "metricName:EC\trank_1:0.022\trank_2:0.033\tpolarity:neg\n",
      "metricName:EO\trank_1:0.058\trank_2:0.059\tpolarity:neg\n",
      "metricName:PC\trank_1:0.041\trank_2:0.055\tpolarity:neg\n",
      "metricName:PO\trank_1:0.056\trank_2:0.083\tpolarity:neg\n",
      "metricName:ACC\trank_1:45.2\trank_2:40.4\tpolarity:pos\n",
      "metricName:PRC\trank_1:44.8\trank_2:40.2\tpolarity:pos\n",
      "metricName:RCL\trank_1:45.4\trank_2:41.3\tpolarity:pos\n",
      "metricName:Accuracy \trank_1:0.89\trank_2:0.70\tpolarity:pos\n",
      "metricName:F1 Score\trank_1:0.89\trank_2:0.70\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:85.2\trank_2:74.3\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:84.7\trank_2:67.7\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:83.7\trank_2:71.3\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:84.2\trank_2:70.5\tpolarity:pos\n",
      "metricName: RMSE (Subject-exposed)\trank_1:186\trank_2:212\tpolarity:neg\n",
      "metricName:RMSE (Subject-na√Øve)\trank_1:216\trank_2:247\tpolarity:neg\n",
      "metricName:Perplexity\trank_1:1.76\trank_2:1.93\tpolarity:neg\n",
      "metricName:Perplexity\trank_1:1.69\trank_2:1.88\tpolarity:neg\n",
      "metricName:F1\trank_1:0.8843\trank_2:0.8342\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:PR-AUC\trank_1:0.9584\trank_2:0.9436\tpolarity:pos\n",
      "metricName:ROC-AUC\trank_1:0.9550\trank_2:0.9488\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.40\trank_2:99.16\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.03\trank_2:98.16\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.071\trank_2:0.077\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:0.844\trank_2:0.836\tpolarity:pos\n",
      "metricName:max E-Measure\trank_1:0.887\trank_2:0.882\tpolarity:pos\n",
      "metricName:max F-Measure\trank_1:0.844\trank_2:0.832\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:0.658\trank_2:0.627\tpolarity:pos\n",
      "metricName:max F-Measure\trank_1:0.513\trank_2:0.499\tpolarity:pos\n",
      "metricName:Mean F-measure\trank_1:0.504\trank_2:0.390\tpolarity:pos\n",
      "metricName:mean E-Measure\trank_1:0.701\trank_2:0.606\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.092\trank_2:0.119\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:0.762\trank_2:0.7619\tpolarity:pos\n",
      "metricName:max E-Measure\trank_1:0.825\trank_2:0.793\tpolarity:pos\n",
      "metricName:max F-Measure\trank_1:0.736\trank_2:0.702\tpolarity:pos\n",
      "metricName:NSS\trank_1:2.87\trank_2:2.667\tpolarity:pos\n",
      "metricName:Macro F1\trank_1:0.805\trank_2:0.803\tpolarity:pos\n",
      "metricName:Macro F1\trank_1:0.494\trank_2:0.48\tpolarity:pos\n",
      "metricName:AAA\trank_1:50.94\trank_2:46.51\tpolarity:pos\n",
      "metricName:F1 (micro)\trank_1:84.42\trank_2:82.18\tpolarity:pos\n",
      "metricName:Macro F1\trank_1:0.687\trank_2:0.687\tpolarity:pos\n",
      "metricName:AUROC\trank_1:0.851\trank_2:0.851\tpolarity:pos\n",
      "metricName:F1-score\trank_1:0.75\trank_2:0.74\tpolarity:pos\n",
      "metricName:Hamming Loss\trank_1:0.2948\trank_2:0.1606\tpolarity:pos\n",
      "metricName:F1-score\trank_1:0.768\trank_2:0.7883\tpolarity:neg\n",
      "metricName:Classification Accuracy\trank_1:0.7734\trank_2:0.7664\tpolarity:pos\n",
      "metricName:Precision\trank_1:77.76\trank_2:79.17\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:0.832\trank_2:0.704\tpolarity:pos\n",
      "metricName:Macro F1\trank_1:0.742\trank_2:0.724\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.6\trank_2:99.5\tpolarity:pos\n",
      "metricName:Precision\trank_1:99.5\trank_2:99.5\tpolarity:pos\n",
      "metricName:Recall\trank_1:99.75\trank_2:99.55\tpolarity:pos\n",
      "metricName:F1\trank_1:92.36\trank_2:91.59\tpolarity:pos\n",
      "metricName:F1\trank_1:95.74\trank_2:95.68\tpolarity:pos\n",
      "metricName:F1\trank_1:91.25\trank_2:90.95\tpolarity:pos\n",
      "metricName:F1\trank_1:94.4\trank_2:91.24\tpolarity:pos\n",
      "metricName:Error rate\trank_1:8.72\trank_2:17.27\tpolarity:neg\n",
      "metricName:F1\trank_1:63.96\trank_2:60.06\tpolarity:pos\n",
      "metricName:F1 score\trank_1:0.511\trank_2:0.507\tpolarity:pos\n",
      "metricName:Temporal awareness\trank_1:67.2\trank_2:30.98\tpolarity:pos\n",
      "metricName:F1\trank_1:0.8408\trank_2:0.8367\tpolarity:pos\n",
      "metricName:Micro F1\trank_1:84.08\trank_2:83.67\tpolarity:pos\n",
      "metricName:mIOU\trank_1:58.2\trank_2:53.8\tpolarity:pos\n",
      "metricName:mIoU\trank_1:70.8\trank_2:68.9\tpolarity:pos\n",
      "metricName:Mean IoU (class)\trank_1:40.2\trank_2:18.64\tpolarity:pos\n",
      "metricName:PQ\trank_1:69.6\trank_2:68.5\tpolarity:pos\n",
      "metricName:mIoU\trank_1:85.3\trank_2:84.6\tpolarity:pos\n",
      "metricName:PQ\trank_1:43.7\trank_2:42.2\tpolarity:pos\n",
      "metricName:PQ\trank_1:51.1\trank_2:48.5\tpolarity:pos\n",
      "metricName:PQ\trank_1:68.5\trank_2:67.1\tpolarity:pos\n",
      "metricName:PQ\trank_1:44.8\trank_2:41.1\tpolarity:pos\n",
      "metricName:mIoU\trank_1:60.0\trank_2:58.4\tpolarity:pos\n",
      "metricName:PQst\trank_1:51.9\trank_2:51.3\tpolarity:pos\n",
      "metricName:PQth\trank_1:39.3\trank_2:33.4\tpolarity:pos\n",
      "metricName:PQ\trank_1:52.7\trank_2:51.1\tpolarity:pos\n",
      "metricName:PQst\trank_1:44.0\trank_2:42.2\tpolarity:pos\n",
      "metricName:PQth\trank_1:58.5\trank_2:57.0\tpolarity:pos\n",
      "metricName:PQ\trank_1:52.1\trank_2:51.5\tpolarity:pos\n",
      "metricName:PQst\trank_1:42.8\trank_2:39.2\tpolarity:pos\n",
      "metricName:PQth\trank_1:58.2\trank_2:59.6\tpolarity:neg\n",
      "metricName:Time (ms)\trank_1:10.6\trank_2:60.2\tpolarity:neg\n",
      "metricName:mIoU\trank_1:79.9\trank_2:79.1\tpolarity:pos\n",
      "metricName:mIoU\trank_1:77.4\trank_2:76.8\tpolarity:pos\n",
      "metricName:mIoU\trank_1:68.48\trank_2:67.8\tpolarity:pos\n",
      "metricName:mIoU\trank_1:44.4\trank_2:43.6\tpolarity:pos\n",
      "metricName:Speed(ms/f)\trank_1:36\trank_2:27\tpolarity:pos\n",
      "metricName:mIoU\trank_1:28.7\trank_2:25.2\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:75.0\trank_2:60.2\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:33.48\trank_2:32.08\tpolarity:pos\n",
      "metricName:mIoU\trank_1:35.7\trank_2:33.6\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:72.2\trank_2:71.8\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:71.9\trank_2:71.0\tpolarity:pos\n",
      "metricName:Class Average IoU\trank_1:87.7\trank_2:85.1\tpolarity:pos\n",
      "metricName:Instance Average IoU\trank_1:86.6\trank_2:86.4\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:63.60\trank_2:63.16\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:78.2\trank_2:76.5\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:74.85\trank_2:73.7\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:80.21\trank_2:78.7\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:72.14\trank_2:67.9\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:77.62\trank_2:74.1\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:41.7\trank_2:37.8\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:53.51\trank_2:52.14\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:79.21\trank_2:77.8\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:77.8\trank_2:77.68\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:64.9\trank_2:60.28\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:40.3\trank_2:35.3\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:73.66\trank_2:70.0\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:76.44\trank_2:76.4\tpolarity:pos\n",
      "metricName:Validation mIoU\trank_1:59.98\trank_2:58.77\tpolarity:pos\n",
      "metricName:IoU [256 distractors]\trank_1:43.7\trank_2:38.4\tpolarity:pos\n",
      "metricName:IoU [32 distractors]\trank_1:65.6\trank_2:62.4\tpolarity:pos\n",
      "metricName:IoU [4 distractors]\trank_1:95.8\trank_2:97.1\tpolarity:neg\n",
      "metricName:mIoU\trank_1:63.9\trank_2:58.4\tpolarity:pos\n",
      "metricName:mIoU (KMeans)\trank_1:44.2\trank_2:35.0\tpolarity:pos\n",
      "metricName:DSC\trank_1:0.8133\trank_2:0.7877\tpolarity:pos\n",
      "metricName:Top-1 Localization Accuracy\trank_1:43.34\trank_2:40.55\tpolarity:pos\n",
      "metricName:Top-1 Error Rate\trank_1:34.8\trank_2:37.71\tpolarity:neg\n",
      "metricName:Top-1 Error Rate\trank_1:51.40\trank_2:67.19\tpolarity:neg\n",
      "metricName:Top-5 Error\trank_1:40.00\trank_2:42.58\tpolarity:neg\n",
      "metricName:UA\trank_1:0.805\trank_2:0.701\tpolarity:pos\n",
      "metricName:Weighted Macro-F1\trank_1:38.11\trank_2:36.75\tpolarity:pos\n",
      "metricName:Micro-F1\trank_1:63.12\trank_2:60.14\tpolarity:pos\n",
      "metricName:Weighted-F1\trank_1:68.23\trank_2:65.21\tpolarity:pos\n",
      "metricName:Weighted-F1\trank_1:68.03\trank_2:67.1\tpolarity:pos\n",
      "metricName:MAE (Arousal)\trank_1:0.161\trank_2:0.16\tpolarity:pos\n",
      "metricName:MAE (Expectancy)\trank_1:0.168\trank_2:0.16\tpolarity:pos\n",
      "metricName:MAE (Power)\trank_1:7.68\trank_2:7.70\tpolarity:neg\n",
      "metricName:MAE (Valence)\trank_1:0.157\trank_2:0.16\tpolarity:neg\n",
      "metricName:Micro-F1\trank_1:0.7765\trank_2:0.7731\tpolarity:pos\n",
      "metricName:F1\trank_1:0.768\trank_2:0.760\tpolarity:pos\n",
      "metricName:UA\trank_1:0.765\trank_2:0.761\tpolarity:pos\n",
      "metricName:F1\trank_1:73.6\trank_2:68.89\tpolarity:pos\n",
      "metricName:F1\trank_1:69.15\trank_2:67.99\tpolarity:pos\n",
      "metricName:F1\trank_1:76.77\trank_2:72.42\tpolarity:pos\n",
      "metricName:mAP\trank_1:35.48\trank_2:32.03\tpolarity:pos\n",
      "metricName:Acc\trank_1:84.5\trank_2:84.4\tpolarity:pos\n",
      "metricName:Edit\trank_1:79.3\trank_2:74.3\tpolarity:pos\n",
      "metricName:F1@10%\trank_1:84.9\trank_2:82.3\tpolarity:pos\n",
      "metricName:F1@25%\trank_1:83.5\trank_2:81.3\tpolarity:pos\n",
      "metricName:F1@50%\trank_1:77.3\trank_2:74\tpolarity:pos\n",
      "metricName:Acc\trank_1:76.0\trank_2:71.0\tpolarity:pos\n",
      "metricName:Edit\trank_1:69.6\trank_2:73.6\tpolarity:neg\n",
      "metricName:F1@10%\trank_1:72.2\trank_2:74.2\tpolarity:neg\n",
      "metricName:F1@25%\trank_1:68.7\trank_2:68.6\tpolarity:pos\n",
      "metricName:F1@50%\trank_1:57.6\trank_2:56.5\tpolarity:pos\n",
      "metricName:Acc\trank_1:77.3\trank_2:79.8\tpolarity:neg\n",
      "metricName:Edit\trank_1:83.7\trank_2:86.2\tpolarity:neg\n",
      "metricName:F1@10%\trank_1:89.4\trank_2:90.0\tpolarity:neg\n",
      "metricName:F1@25%\trank_1:87.8\trank_2:89.1\tpolarity:neg\n",
      "metricName:F1@50%\trank_1:79.8\trank_2:78.0\tpolarity:pos\n",
      "metricName:Average 3D Error\trank_1:13.66\trank_2:13.76\tpolarity:neg\n",
      "metricName:Average 3D Error\trank_1:7.15\trank_2:7.2\tpolarity:neg\n",
      "metricName:Average 3D Error\trank_1:7.48\trank_2:8.57\tpolarity:neg\n",
      "metricName:Average 3D Error\trank_1:5.98\trank_2:6.152\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:94.03\trank_2:93.87\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:98.23\trank_2:93\tpolarity:pos\n",
      "metricName:14 gestures accuracy\trank_1:95.9\trank_2:94.4\tpolarity:pos\n",
      "metricName:28 gestures accuracy\trank_1:94.7\trank_2:90.7\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:96.6\trank_2:94.78\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:87.9\trank_2:86.93\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:57.4\trank_2:39.23\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:14 gestures accuracy\trank_1:95.9\trank_2:94.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:86.08\trank_2:83.1\tpolarity:pos\n",
      "metricName:AMT\trank_1:22.6\trank_2:6.9\tpolarity:pos\n",
      "metricName:IS\trank_1:3.4107\trank_2:3.3874\tpolarity:pos\n",
      "metricName:PSNR\trank_1:27.9749\trank_2:26.9451\tpolarity:pos\n",
      "metricName:AMT\trank_1:26.1\trank_2:2.6\tpolarity:pos\n",
      "metricName:IS\trank_1:2.5532\trank_2:2.4919\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.6091\trank_2:28.0185\tpolarity:pos\n",
      "metricName:F1\trank_1:98.4\trank_2:96.4\tpolarity:pos\n",
      "metricName:F1\trank_1:83.38\trank_2:82.9\tpolarity:pos\n",
      "metricName:F1\trank_1:86.90\trank_2:86.0\tpolarity:pos\n",
      "metricName:F1\trank_1:89.2\trank_2:88.5\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:80.41\trank_2:72.75\tpolarity:pos\n",
      "metricName:F1\trank_1:85.96\trank_2:81.48\tpolarity:pos\n",
      "metricName:F1 Full\trank_1:0.70\trank_2:0.68\tpolarity:pos\n",
      "metricName:F1 Newswire\trank_1:0.75\trank_2:0.73\tpolarity:pos\n",
      "metricName:Smatch\trank_1:84.3\trank_2:81.4\tpolarity:pos\n",
      "metricName:F1 Full\trank_1:68.4\trank_2:66\tpolarity:pos\n",
      "metricName:F1 Newswire\trank_1:73.3\trank_2:71\tpolarity:pos\n",
      "metricName:Smatch\trank_1:73.7\trank_2:70.7\tpolarity:pos\n",
      "metricName:Smatch\trank_1:83.0\trank_2:83.0\tpolarity:pos\n",
      "metricName:In-domain\trank_1:95.6\trank_2:94.4\tpolarity:pos\n",
      "metricName:Out-of-domain\trank_1:92.6\trank_2:91.0\tpolarity:pos\n",
      "metricName:In-domain\trank_1:95.8\trank_2:95.1\tpolarity:pos\n",
      "metricName:Out-of-domain\trank_1:94.6\trank_2:93.4\tpolarity:pos\n",
      "metricName:In-domain\trank_1:83.8\trank_2:82.6\tpolarity:pos\n",
      "metricName:Out-of-domain\trank_1:83.4\trank_2:82.0\tpolarity:pos\n",
      "metricName:English-Wiki (open) F1\trank_1:80.5\trank_2:76.6\tpolarity:pos\n",
      "metricName:Full MRP F1\trank_1:81.7\trank_2:77.7\tpolarity:pos\n",
      "metricName:Full UCCA F1\trank_1:66.7\trank_2:57.4\tpolarity:pos\n",
      "metricName:LPP MRP F1\trank_1:82.6\trank_2:82.2\tpolarity:pos\n",
      "metricName:LPP UCCA F1\trank_1:64.4\trank_2:65.9\tpolarity:neg\n",
      "metricName:F1\trank_1:88.3\trank_2:87.1\tpolarity:pos\n",
      "metricName:F1\trank_1:89.3\trank_2:87.7\tpolarity:pos\n",
      "metricName:F0.5\trank_1:63.1\trank_2:45.1\tpolarity:pos\n",
      "metricName:F0.5\trank_1:72.2\trank_2:52.07\tpolarity:pos\n",
      "metricName:F0.5\trank_1:54.3\trank_2:36.1\tpolarity:pos\n",
      "metricName:UAS\trank_1:79.9\trank_2:75.6\tpolarity:pos\n",
      "metricName:LAS\trank_1:77.3\trank_2:70.5\tpolarity:pos\n",
      "metricName:UAS\trank_1:66.2\trank_2:64.4\tpolarity:pos\n",
      "metricName:Accuracy (10-fold)\trank_1:79\trank_2:73\tpolarity:pos\n",
      "metricName:3DPCK\trank_1:95.1\trank_2:94.8\tpolarity:pos\n",
      "metricName:AUC\trank_1:62.2\trank_2:61.4\tpolarity:pos\n",
      "metricName:MJPE\trank_1:60.1\trank_2:61.6\tpolarity:neg\n",
      "metricName:MPJPE\trank_1:37.1\trank_2:49.1\tpolarity:neg\n",
      "metricName:MPJPE\trank_1:79.1\trank_2:76.7\tpolarity:pos\n",
      "metricName:MPVPE\trank_1:94.2\trank_2:93.4\tpolarity:pos\n",
      "metricName:PA-MPJPE\trank_1:46.4\trank_2:47.3\tpolarity:neg\n",
      "metricName:Average MPJPE (mm)\trank_1:24.6\trank_2:28.9\tpolarity:neg\n",
      "metricName:Average MPJPE (mm)\trank_1:18.7\trank_2:19.0\tpolarity:neg\n",
      "metricName:Mean Reconstruction Error (mm)\trank_1:12.2\trank_2:13.5\tpolarity:neg\n",
      "metricName:MPJPE\trank_1:83.15\trank_2:81.76\tpolarity:pos\n",
      "metricName:MPJAE\trank_1:19.69\trank_2:20.80\tpolarity:neg\n",
      "metricName:MPJPE (CA)\trank_1:57.8\trank_2:81.1\tpolarity:neg\n",
      "metricName:MPJPE (CS)\trank_1:67.3\trank_2:91.0\tpolarity:neg\n",
      "metricName:PCK3D (CA)\trank_1:95.5\trank_2:88.1\tpolarity:pos\n",
      "metricName:PCK3D (CS)\trank_1:93.9\trank_2:85.7\tpolarity:pos\n",
      "metricName:Mean PCK\trank_1:82.5\trank_2:78.6\tpolarity:pos\n",
      "metricName:AP50\trank_1:92.7\trank_2:92.4\tpolarity:pos\n",
      "metricName:AP75\trank_1:84.5\trank_2:84.0\tpolarity:pos\n",
      "metricName:APL\trank_1:83.1\trank_2:82.7\tpolarity:pos\n",
      "metricName:APM\trank_1:73.4\trank_2:73.0\tpolarity:pos\n",
      "metricName:AR\trank_1:82.0\trank_2:81.5\tpolarity:pos\n",
      "metricName:Test AP\trank_1:79.5\trank_2:78.9\tpolarity:pos\n",
      "metricName:AP50\trank_1:92.9\trank_2:90.9\tpolarity:pos\n",
      "metricName:AP75\trank_1:82.6\trank_2:80.8\tpolarity:pos\n",
      "metricName:APL\trank_1:88.6\trank_2:87.5\tpolarity:pos\n",
      "metricName:AR\trank_1:82.2\trank_2:80.5\tpolarity:pos\n",
      "metricName:AR50\trank_1:96\trank_2:95.1\tpolarity:pos\n",
      "metricName:AR75\trank_1:87.7\trank_2:86.3\tpolarity:pos\n",
      "metricName:ARL\trank_1:83.2\trank_2:82.9\tpolarity:pos\n",
      "metricName:ARM\trank_1:77.5\trank_2:75.3\tpolarity:pos\n",
      "metricName:AP\trank_1:76.4\trank_2:74.5\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:82.1\trank_2:80.4\tpolarity:pos\n",
      "metricName:Average MPJPE (mm)\trank_1:18.7\trank_2:44.8\tpolarity:neg\n",
      "metricName:mAP @0.5:0.95\trank_1:71.3\trank_2:67.6\tpolarity:pos\n",
      "metricName:AP\trank_1:0.774\trank_2:0.764\tpolarity:pos\n",
      "metricName:Mean mAP\trank_1:77.94\trank_2:77.9\tpolarity:pos\n",
      "metricName:AP\trank_1:79.1\trank_2:77.8\tpolarity:pos\n",
      "metricName:AOP\trank_1:88.10\trank_2:86.5\tpolarity:pos\n",
      "metricName:AP\trank_1:78.7\trank_2:76.2\tpolarity:pos\n",
      "metricName:AP\trank_1:82.1\trank_2:80.4\tpolarity:pos\n",
      "metricName:ADDS AUC\trank_1:96.1\trank_2:93.3\tpolarity:pos\n",
      "metricName:Accuracy (ADD)\trank_1:99.4\trank_2:97.8\tpolarity:pos\n",
      "metricName:Accuracy (ADD)\trank_1:97.35\trank_2:96.1\tpolarity:pos\n",
      "metricName:Mean ADD\trank_1:97.35\trank_2:96.1\tpolarity:pos\n",
      "metricName:Mean ADD\trank_1:70.1\trank_2:53.7\tpolarity:pos\n",
      "metricName:Mean ADD\trank_1:55.5\trank_2:51.6\tpolarity:pos\n",
      "metricName:MAP\trank_1:0.48\trank_2:0.38\tpolarity:pos\n",
      "metricName:MAE\trank_1:4.06\trank_2:5.09\tpolarity:neg\n",
      "metricName:MAE (trained with other data)\trank_1:3.48\trank_2:3.66\tpolarity:neg\n",
      "metricName:MAE\trank_1:3.83\trank_2:3.913\tpolarity:neg\n",
      "metricName:Average Orientation Similarity\trank_1:80.96\trank_2:80.39\tpolarity:pos\n",
      "metricName:Mean ADD\trank_1:99.4\trank_2:97.8\tpolarity:pos\n",
      "metricName:Mean ADD\trank_1:93.3\trank_2:80.6\tpolarity:pos\n",
      "metricName:PCK@0.3\trank_1:88.4\trank_2:84.3\tpolarity:pos\n",
      "metricName:MAR, walking, 1,000ms\trank_1:0.67\trank_2:0.69\tpolarity:neg\n",
      "metricName:MAR, walking, 400ms\trank_1:0.56\trank_2:0.59\tpolarity:neg\n",
      "metricName:mAP (@0.1, Through-wall)\trank_1:86.5\trank_2:78.5\tpolarity:pos\n",
      "metricName:mAP (@0.1, Visible)\trank_1:90.1\trank_2:825\tpolarity:neg\n",
      "metricName:F1\trank_1:70.71\trank_2:68.06\tpolarity:pos\n",
      "metricName:Acc\trank_1:84.52\trank_2:84.52\tpolarity:pos\n",
      "metricName:Aspect\trank_1:87.9\trank_2:86.4\tpolarity:pos\n",
      "metricName:Sentiment\trank_1:93.3\trank_2:93.6\tpolarity:neg\n",
      "metricName:Laptop (F1)\trank_1:84.26\trank_2:81.59\tpolarity:pos\n",
      "metricName:Restaurant (F1)\trank_1:77.97\trank_2:74.37\tpolarity:pos\n",
      "metricName:F1\trank_1:68.06\trank_2:63.4\tpolarity:pos\n",
      "metricName:Restaurant (Acc)\trank_1:88.70\trank_2:88.0\tpolarity:pos\n",
      "metricName:Restaurant (Acc)\trank_1:90.18\trank_2:87.37\tpolarity:pos\n",
      "metricName:Laptop (Acc)\trank_1:82.29\trank_2:83.78\tpolarity:neg\n",
      "metricName:Mean Acc (Restaurant + Laptop)\trank_1:86.24\trank_2:85.58\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:83.91\trank_2:83\tpolarity:pos\n",
      "metricName:F1 score\trank_1:81.17\trank_2:82.8\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:82.40\trank_2:82.10\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:95.19\trank_2:95.16\tpolarity:pos\n",
      "metricName:F1\trank_1:72.46\trank_2:70.32\tpolarity:pos\n",
      "metricName:Avg F1\trank_1:69.18\trank_2:68.99\tpolarity:pos\n",
      "metricName:Laptop 2014 (F1)\trank_1:67.37\trank_2:65.94\tpolarity:pos\n",
      "metricName:Restaurant 2014 (F1)\trank_1:73.56\trank_2:75.95\tpolarity:neg\n",
      "metricName:Restaurant 2015 (F1)\trank_1:66.61\trank_2:65.08\tpolarity:pos\n",
      "metricName:Holder Binary F1\trank_1:84.91\trank_2:83.80\tpolarity:pos\n",
      "metricName:Target Binary F1\trank_1:73.29\trank_2:72.06\tpolarity:pos\n",
      "metricName:Average F1\trank_1:0.5807\trank_2:0.5035\tpolarity:pos\n",
      "metricName:F1\trank_1:96.79\trank_2:96.62\tpolarity:pos\n",
      "metricName:F1\trank_1:96.72\trank_2:96.09\tpolarity:pos\n",
      "metricName:F1\trank_1:71.25\trank_2:69.8\tpolarity:pos\n",
      "metricName:F1\trank_1:96.3\trank_2:95.2\tpolarity:pos\n",
      "metricName:F1\trank_1:84.47\trank_2:82.11\tpolarity:pos\n",
      "metricName:F1\trank_1:87.41\trank_2:86.84\tpolarity:pos\n",
      "metricName:F1\trank_1:86.67\trank_2:85.4\tpolarity:pos\n",
      "metricName:F1\trank_1:83.75\trank_2:80.54\tpolarity:pos\n",
      "metricName:F1\trank_1:0.8371\trank_2:0.792\tpolarity:pos\n",
      "metricName:Exact Span F1\trank_1:65.5\trank_2:66.4\tpolarity:neg\n",
      "metricName:F1 score\trank_1:0.97\trank_2:0.96\tpolarity:pos\n",
      "metricName:Precision\trank_1:0.97\trank_2:0.95\tpolarity:pos\n",
      "metricName:Recall\trank_1:0.97\trank_2:0.96\tpolarity:pos\n",
      "metricName:AUROC\trank_1:84.04\trank_2:82.55\tpolarity:pos\n",
      "metricName:Accuracy (Inter-Patient)\trank_1:99.53\trank_2:99.47\tpolarity:pos\n",
      "metricName:Accuracy (TEST-DB)\trank_1:79\trank_2:79\tpolarity:pos\n",
      "metricName:Accuracy (TRAIN-DB)\trank_1:72.0\trank_2:62.4\tpolarity:pos\n",
      "metricName:F1 (Sequence)\trank_1:0.807\trank_2:0.753\tpolarity:pos\n",
      "metricName:F1 (Set)\trank_1:0.837\trank_2:0.780\tpolarity:pos\n",
      "metricName:F1 (1dAVb)\trank_1:0.893\trank_2:0.776\tpolarity:pos\n",
      "metricName:F1 (AF)\trank_1:0.857\trank_2:0.769\tpolarity:pos\n",
      "metricName:F1 (LBBB)\trank_1:0.984\trank_2:0.947\tpolarity:pos\n",
      "metricName:F1 (RBBB)\trank_1:0.932\trank_2:0.917\tpolarity:pos\n",
      "metricName:F1 (SB)\trank_1:0.882\trank_2:0.882\tpolarity:pos\n",
      "metricName:F1 (ST)\trank_1:0.933\trank_2:0.896\tpolarity:pos\n",
      "metricName:PPV (VEB)\trank_1:95.7\trank_2:90.9\tpolarity:pos\n",
      "metricName:Sensitivity (VEB)\trank_1:92.7\trank_2:93.9\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:99.43\trank_2:95.9\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.42\trank_2:96.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.60\trank_2:99.22\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.85\trank_2:91.65\tpolarity:pos\n",
      "metricName:F1-Score\trank_1:0.962\trank_2:0.953\tpolarity:pos\n",
      "metricName:F1\trank_1:91.79\trank_2:87.3\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:validation mean average precision\trank_1:93.6\trank_2:93.5\tpolarity:pos\n",
      "metricName:Weighted Average F1-score\trank_1:0.93\trank_2:0.90\tpolarity:pos\n",
      "metricName:F-Measure\trank_1:86.3\trank_2:75.6\tpolarity:pos\n",
      "metricName:Image-Level Recall\trank_1:45.96\trank_2:34.27\tpolarity:pos\n",
      "metricName:METEOR\trank_1:9.71\trank_2:8.75\tpolarity:pos\n",
      "metricName:spl\trank_1:0.917\trank_2:0.79\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.679\trank_2:0.672\tpolarity:pos\n",
      "metricName:Expected Average Overlap (EAO)\trank_1:0.397\trank_2:0.30\tpolarity:pos\n",
      "metricName:F-Measure (Seen)\trank_1:60.5\trank_2:62.7\tpolarity:neg\n",
      "metricName:F-Measure (Unseen)\trank_1:60.7\trank_2:51.4\tpolarity:pos\n",
      "metricName:O (Average of Measures)\trank_1:58.8\trank_2:55.2\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.68\trank_2:0.677\tpolarity:pos\n",
      "metricName:Precision\trank_1:79.1\trank_2:80.0\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:82.0\trank_2:81.2\tpolarity:pos\n",
      "metricName:Normalized Precision\trank_1:86.9\trank_2:85.4\tpolarity:pos\n",
      "metricName:Average Overlap\trank_1:68.8\trank_2:64.9\tpolarity:pos\n",
      "metricName:Success Rate 0.5\trank_1:78.1\trank_2:72.8\tpolarity:pos\n",
      "metricName:Expected Average Overlap (EAO)\trank_1:0.327\trank_2:0.309\tpolarity:pos\n",
      "metricName:AUC\trank_1:67.1\trank_2:64.9\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.61\trank_2:0.610\tpolarity:pos\n",
      "metricName:Expected Average Overlap (EAO)\trank_1:0.466\trank_2:0.3903\tpolarity:pos\n",
      "metricName:Expected Average Overlap (EAO)\trank_1:0.446\trank_2:0.4160\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.701\trank_2:0.70\tpolarity:pos\n",
      "metricName:Precision\trank_1:0.931\trank_2:0.91\tpolarity:pos\n",
      "metricName:sMOTSA\trank_1:70.4\trank_2:54.9\tpolarity:pos\n",
      "metricName:MOTA\trank_1:76.7\trank_2:74.9\tpolarity:pos\n",
      "metricName:MOTA\trank_1:76.7\trank_2:73.7\tpolarity:pos\n",
      "metricName:IDF1\trank_1:75.1\trank_2:72.3\tpolarity:pos\n",
      "metricName:MOTA\trank_1:77.5\trank_2:67.1\tpolarity:pos\n",
      "metricName:MOTA\trank_1:60.7\trank_2:52.5\tpolarity:pos\n",
      "metricName:MOTA\trank_1:60.6\trank_2:57\tpolarity:pos\n",
      "metricName:MOTA\trank_1:90.03\trank_2:89.44\tpolarity:pos\n",
      "metricName:MOTA\trank_1:53.5\trank_2:49.9\tpolarity:pos\n",
      "metricName:MOTA\trank_1:67.7\trank_2:54.4\tpolarity:pos\n",
      "metricName:R@1\trank_1:51.2\trank_2:49.4\tpolarity:pos\n",
      "metricName:R@10\trank_1:85.7\trank_2:82.7\tpolarity:pos\n",
      "metricName:R@1\trank_1:85.6\trank_2:81.4\tpolarity:pos\n",
      "metricName:R@10\trank_1:97.9\trank_2:95.9\tpolarity:pos\n",
      "metricName:Recall@10\trank_1:98.3\trank_2:97.2\tpolarity:pos\n",
      "metricName:R@1\trank_1:0.346\trank_2:0.048\tpolarity:pos\n",
      "metricName:R@5\trank_1:0.642\trank_2:0.122\tpolarity:pos\n",
      "metricName:Acc\trank_1:85.2\trank_2:77.1\tpolarity:pos\n",
      "metricName:Acc\trank_1:46.2\trank_2:43.62\tpolarity:pos\n",
      "metricName:Acc\trank_1:75.5\trank_2:51.14\tpolarity:pos\n",
      "metricName:Chamfer Distance\trank_1:8.828\trank_2:9.636\tpolarity:neg\n",
      "metricName:F-Score@1%\trank_1:0.708\trank_2:0.695\tpolarity:pos\n",
      "metricName:Chamfer Distance\trank_1:10.64\trank_2:14.25\tpolarity:neg\n",
      "metricName:Average Success Rate\trank_1:71.7\trank_2:68.2\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:90.5\trank_2:62.0\tpolarity:pos\n",
      "metricName:Image-to-text R@1\trank_1:95.3\trank_2:83.5\tpolarity:pos\n",
      "metricName:Image-to-text R@10\trank_1:100\trank_2:98.6\tpolarity:pos\n",
      "metricName:Image-to-text R@5\trank_1:99.8\trank_2:96.7\tpolarity:pos\n",
      "metricName:Text-to-image R@1\trank_1:84.9\trank_2:64.4\tpolarity:pos\n",
      "metricName:Text-to-image R@10\trank_1:98.6\trank_2:93.8\tpolarity:pos\n",
      "metricName:Text-to-image R@5\trank_1:97.4\trank_2:88.7\tpolarity:pos\n",
      "metricName:Image-to-text R@1\trank_1:77\trank_2:73.5\tpolarity:pos\n",
      "metricName:Image-to-text R@10\trank_1:96.9\trank_2:96.0\tpolarity:pos\n",
      "metricName:Image-to-text R@5\trank_1:93.5\trank_2:92.2\tpolarity:pos\n",
      "metricName:Text-to-image R@1\trank_1:59.9\trank_2:57.5\tpolarity:pos\n",
      "metricName:Text-to-image R@10\trank_1:89.8\trank_2:89.8\tpolarity:pos\n",
      "metricName:Text-to-image R@5\trank_1:83.3\trank_2:82.8\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:76.05\trank_2:60.94\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:93.32\trank_2:82.48\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:84.4\trank_2:79.2\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:76.02\trank_2:69.43\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:69.57\trank_2:67.63\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.95\trank_2:91.62\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.05\trank_2:89.42\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:89.7\trank_2:67.83\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:92.7\trank_2:88.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.8\trank_2:79.1\tpolarity:pos\n",
      "metricName:PSNR\trank_1:29.71\trank_2:29.7\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.838\trank_2:0.8252\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.54\trank_2:34.45\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.54\trank_2:33.45\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.11\trank_2:31.71\tpolarity:pos\n",
      "metricName:PSNR-B\trank_1:32.47\trank_2:32.02\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.815\trank_2:0.809\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.04\trank_2:29.92\tpolarity:pos\n",
      "metricName:PSNR-B\trank_1:30.01\trank_2:29.51\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.882\trank_2:0.882\tpolarity:pos\n",
      "metricName:PSNR\trank_1:37.12\trank_2:36.56\tpolarity:pos\n",
      "metricName:PSNR-B\trank_1:36.88\trank_2:36.44\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.924\trank_2:0.902\tpolarity:pos\n",
      "metricName:PSNR\trank_1:35.20\trank_2:34.11\tpolarity:pos\n",
      "metricName:PSNR-B\trank_1:35.67\trank_2:34.69\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.860\trank_2:0.845\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.23\trank_2:33.99\tpolarity:pos\n",
      "metricName:PSNR-B\trank_1:34.67\trank_2:34.37\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.845\trank_2:0.838\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.19\trank_2:32.16\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.29\trank_2:34.27\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.73\trank_2:34.18\tpolarity:pos\n",
      "metricName:PSNR-B\trank_1:34.58\trank_2:34.15\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.896\trank_2:0.874\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.46\trank_2:33.43\tpolarity:pos\n",
      "metricName:PSNR\trank_1:27.65\trank_2:27.63\tpolarity:pos\n",
      "metricName:PSNR-B\trank_1:27.40\trank_2:27.63\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.819\trank_2:0.816\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.03\trank_2:30.01\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.1\trank_2:32.09\tpolarity:pos\n",
      "metricName:SSIM\trank_1:0.8886\trank_2:0.9006\tpolarity:neg\n",
      "metricName:PSNR\trank_1:37\trank_2:37.2\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.890\trank_2:0.8893\tpolarity:pos\n",
      "metricName:PSNR\trank_1:40\trank_2:40.2\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.930\trank_2:0.9287\tpolarity:pos\n",
      "metricName:PSNR\trank_1:41\trank_2:41.3\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.959\trank_2:0.9581\tpolarity:pos\n",
      "metricName:PSNR\trank_1:38\trank_2:38.1\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.943\trank_2:0.9408\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:0.870\trank_2:0.833\tpolarity:pos\n",
      "metricName:Size (MB)\trank_1:1.9\trank_2:2.9\tpolarity:neg\n",
      "metricName:3DIoU\trank_1:0.67\trank_2:0.661\tpolarity:pos\n",
      "metricName:Accuracy (Cross-View)\trank_1:88.7\trank_2:87.1\tpolarity:pos\n",
      "metricName:Accuracy (Cross-View, Avg)\trank_1:88.8\trank_2:84.2\tpolarity:pos\n",
      "metricName:BG#1-2\trank_1:91.5\trank_2:87.2\tpolarity:pos\n",
      "metricName:CL#1-2\trank_1:78.7\trank_2:70.4\tpolarity:pos\n",
      "metricName:NM#5-6 \trank_1:96.2\trank_2:95.0\tpolarity:pos\n",
      "metricName:PSNR (sRGB)\trank_1:39.99\trank_2:39.77\tpolarity:pos\n",
      "metricName:SSIM (sRGB)\trank_1:0.958\trank_2:0.970\tpolarity:neg\n",
      "metricName:PSNR (sRGB)\trank_1:39.96\trank_2:39.88\tpolarity:pos\n",
      "metricName:SSIM (sRGB)\trank_1:0.956\trank_2:0.956\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.1\trank_2:33.87\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.37\trank_2:31.18\tpolarity:pos\n",
      "metricName:PSNR (Raw)\trank_1:48.88\trank_2:48.5\tpolarity:pos\n",
      "metricName:PSNR (sRGB)\trank_1:40.35\trank_2:39.4\tpolarity:pos\n",
      "metricName:SSIM (Raw)\trank_1:0.9821\trank_2:0.9806\tpolarity:pos\n",
      "metricName:SSIM (sRGB)\trank_1:0.9641\trank_2:0.9528\tpolarity:pos\n",
      "metricName:PSNR\trank_1:26.35\trank_2:26.24\tpolarity:pos\n",
      "metricName:PSNR\trank_1:35.98\trank_2:35.92\tpolarity:pos\n",
      "metricName:PSNR\trank_1:29.58\trank_2:29.34\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.98\trank_2:31.95\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.01\trank_2:33.86\tpolarity:pos\n",
      "metricName:PSNR\trank_1:40.05\trank_2:39.73\tpolarity:pos\n",
      "metricName:PSNR\trank_1:28.34\trank_2:28.14\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.21\trank_2:30.99\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.4\trank_2:31.42\tpolarity:pos\n",
      "metricName:PSNR\trank_1:40.90\trank_2:39.83\tpolarity:pos\n",
      "metricName:PSNR\trank_1:36.39\trank_2:34.95\tpolarity:pos\n",
      "metricName:PSNR\trank_1:37.28\trank_2:35.92\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.56\trank_2:30.49\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.35\trank_2:32.27\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.88\trank_2:31.86\tpolarity:pos\n",
      "metricName:PSNR\trank_1:34.01\trank_2:33.47\tpolarity:pos\n",
      "metricName:PSNR\trank_1:24.79\trank_2:22.67\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.79\trank_2:30.78\tpolarity:pos\n",
      "metricName:PSNR\trank_1:25.71\trank_2:25.15\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.48\trank_2:25.97\tpolarity:pos\n",
      "metricName:PSNR\trank_1:36.36\trank_2:33.63\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.47\trank_2:33.45\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.57\trank_2:28.2\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:PSNR\trank_1:26.53\trank_2:26.47\tpolarity:pos\n",
      "metricName:PSNR\trank_1:27.49\trank_2:27.47\tpolarity:pos\n",
      "metricName:PSNR\trank_1:27.74\trank_2:27.64\tpolarity:pos\n",
      "metricName:PSNR\trank_1:28.58\trank_2:27.82\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.8\trank_2:30.43\tpolarity:pos\n",
      "metricName:PSNR\trank_1:33.16\trank_2:33.15\tpolarity:pos\n",
      "metricName:PSNR\trank_1:30.95\trank_2:30.94\tpolarity:pos\n",
      "metricName:PSNR\trank_1:25.14\trank_2:25.12\tpolarity:pos\n",
      "metricName:PSNR\trank_1:29.41\trank_2:29.41\tpolarity:pos\n",
      "metricName:PSNR\trank_1:31.17\trank_2:24.62\tpolarity:pos\n",
      "metricName:PSNR\trank_1:27.73\trank_2:27.03\tpolarity:pos\n",
      "metricName:Overall Accuracy\trank_1:95.36\trank_2:86.61\tpolarity:pos\n",
      "metricName:Overall Accuracy\trank_1:100\trank_2:100\tpolarity:pos\n",
      "metricName:Overall Accuracy\trank_1:99.86\trank_2:99.81\tpolarity:pos\n",
      "metricName:Overall Accuracy\trank_1:99.99\trank_2:99.97\tpolarity:pos\n",
      "metricName:CR\trank_1:0.407\trank_2:0.39\tpolarity:pos\n",
      "metricName:F1\trank_1:0.855\trank_2:0.851\tpolarity:pos\n",
      "metricName:Pearson Correlation\trank_1:0.767\trank_2:0.759\tpolarity:pos\n",
      "metricName:Pearson Correlation\trank_1:0.817\trank_2:0.795\tpolarity:pos\n",
      "metricName:Mean F1 (WSJ)\trank_1:60.4\trank_2:57.7\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.8051\trank_2:0.7983\tpolarity:pos\n",
      "metricName:CC\trank_1:0.6121\trank_2:0.5817\tpolarity:pos\n",
      "metricName:NSS\trank_1:1.5077\trank_2:1.4272\tpolarity:pos\n",
      "metricName:F1-score\trank_1:60.6\trank_2:58.8\tpolarity:pos\n",
      "metricName:F1-score\trank_1:51.3\trank_2:50.8\tpolarity:pos\n",
      "metricName:F1-score (Canonical)\trank_1:67.5\trank_2:63.9\tpolarity:pos\n",
      "metricName:F1-score (Canonical)\trank_1:50.2\trank_2:48.6\tpolarity:pos\n",
      "metricName:F1-score (Augmented)\trank_1:50.7\trank_2:48.7\tpolarity:pos\n",
      "metricName:ROUGE-1\trank_1:44.51\trank_2:44.45\tpolarity:pos\n",
      "metricName:ROUGE-2\trank_1:21.58\trank_2:21.25\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:41.24\trank_2:41.4\tpolarity:neg\n",
      "metricName:ROUGE-1\trank_1:37.68\trank_2:23.67\tpolarity:pos\n",
      "metricName:ROUGE-2\trank_1:21.25\trank_2:10.29\tpolarity:pos\n",
      "metricName:ROUGE-1\trank_1:44.48\trank_2:44.41\tpolarity:pos\n",
      "metricName:ROUGE-2\trank_1:21.31\trank_2:20.86\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:41.52\trank_2:40.55\tpolarity:pos\n",
      "metricName:ROUGE-1\trank_1:26.55\trank_2:26.55\tpolarity:pos\n",
      "metricName:ROUGE-2\trank_1:7.06\trank_2:7.06\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:57.21\trank_2:53.23\tpolarity:pos\n",
      "metricName:ROUGE-1\trank_1:44.68\trank_2:44.41\tpolarity:pos\n",
      "metricName:ROUGE-2\trank_1:21.30\trank_2:20.86\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:40.75\trank_2:40.55\tpolarity:pos\n",
      "metricName:ROUGE-1\trank_1:53.09\trank_2:41.26\tpolarity:pos\n",
      "metricName:Average Return (NoOp)\trank_1:1668\trank_2:900\tpolarity:pos\n",
      "metricName:Score\trank_1:3084781.7\trank_2:2193605.67\tpolarity:pos\n",
      "metricName:Score\trank_1:999153.3\trank_2:998425.00\tpolarity:pos\n",
      "metricName:Score\trank_1:74335.30\trank_2:54681\tpolarity:pos\n",
      "metricName:Score\trank_1:23.9\trank_2:23.84\tpolarity:pos\n",
      "metricName:Score\trank_1:998532.37\trank_2:995048.4\tpolarity:pos\n",
      "metricName:Score\trank_1:715545.61\trank_2:653662\tpolarity:pos\n",
      "metricName:Score\trank_1:725853.90\trank_2:249808.9\tpolarity:pos\n",
      "metricName:Score\trank_1:993010\trank_2:839642.95\tpolarity:pos\n",
      "metricName:Score\trank_1:2354.91\trank_2:491.48\tpolarity:pos\n",
      "metricName:Score\trank_1:839573.53\trank_2:717344.0\tpolarity:pos\n",
      "metricName:Score\trank_1:135784.96\trank_2:87291.7\tpolarity:pos\n",
      "metricName:Score\trank_1:34.0\trank_2:34\tpolarity:pos\n",
      "metricName:Score\trank_1:79.3\trank_2:67.04\tpolarity:pos\n",
      "metricName:Score\trank_1:323417.18\trank_2:171673.78\tpolarity:pos\n",
      "metricName:Score\trank_1:233413.3\trank_2:206845.82\tpolarity:pos\n",
      "metricName:Score\trank_1:999997.63\trank_2:999996.7\tpolarity:pos\n",
      "metricName:Score\trank_1:999900\trank_2:991039.70\tpolarity:pos\n",
      "metricName:Score\trank_1:269358.27\trank_2:251997.31\tpolarity:pos\n",
      "metricName:Score\trank_1:955137.84\trank_2:908264.15\tpolarity:pos\n",
      "metricName:Score\trank_1:741812.63\trank_2:297638.17\tpolarity:pos\n",
      "metricName:Score\trank_1:157177.85\trank_2:101197.71\tpolarity:pos\n",
      "metricName:Score\trank_1:197376\trank_2:85932.60\tpolarity:pos\n",
      "metricName:Score\trank_1:24\trank_2:23.94\tpolarity:pos\n",
      "metricName:Score\trank_1:999383.2\trank_2:992340.74\tpolarity:pos\n",
      "metricName:Score\trank_1:197126.00\trank_2:157306.41\tpolarity:pos\n",
      "metricName:Score\trank_1:230324\trank_2:143964.26\tpolarity:pos\n",
      "metricName:Score\trank_1:107363\trank_2:18756.01\tpolarity:pos\n",
      "metricName:Score\trank_1:131.13\trank_2:127.32\tpolarity:pos\n",
      "metricName:Score\trank_1:864.00\trank_2:855\tpolarity:pos\n",
      "metricName:Score\trank_1:143972.03\trank_2:108197.0\tpolarity:pos\n",
      "metricName:Score\trank_1:106.1\trank_2:21.0\tpolarity:pos\n",
      "metricName:Score\trank_1:613411.80\trank_2:599246.7\tpolarity:pos\n",
      "metricName:Score\trank_1:29100\trank_2:27088.89\tpolarity:pos\n",
      "metricName:Score\trank_1:934134.88\trank_2:848623.00\tpolarity:pos\n",
      "metricName:Score\trank_1:17763.4\trank_2:4801.27\tpolarity:pos\n",
      "metricName:Score\trank_1:100.00\trank_2:100\tpolarity:pos\n",
      "metricName:Score\trank_1:114736.26\trank_2:49244.11\tpolarity:pos\n",
      "metricName:Score\trank_1:0\trank_2:0\tpolarity:pos\n",
      "metricName:Medium Human-Normalized Score\trank_1:2041.1\trank_2:1920.6\tpolarity:pos\n",
      "metricName:Score\trank_1:454993.53\trank_2:333077.44\tpolarity:pos\n",
      "metricName:Score\trank_1:3454.0\trank_2:2382.44\tpolarity:pos\n",
      "metricName:Score\trank_1:130345.58\trank_2:124776.3\tpolarity:pos\n",
      "metricName:Score\trank_1:19213.96\trank_2:15680.7\tpolarity:pos\n",
      "metricName:Score\trank_1:29660.08\trank_2:29321.4\tpolarity:pos\n",
      "metricName:Score\trank_1:678558.64\trank_2:476412\tpolarity:pos\n",
      "metricName:Score\trank_1:44199.93\trank_2:19671\tpolarity:pos\n",
      "metricName:Score\trank_1:243401.10\trank_2:70659.76\tpolarity:pos\n",
      "metricName:Score\trank_1:565909.85\trank_2:458315.40\tpolarity:pos\n",
      "metricName:Score\trank_1:631378.53\trank_2:541280.88\tpolarity:pos\n",
      "metricName:Score\trank_1:1422628\trank_2:1159049.27\tpolarity:pos\n",
      "metricName:Score\trank_1:27219.8\trank_2:24235.9\tpolarity:pos\n",
      "metricName:Score\trank_1:580328.14\trank_2:572510\tpolarity:pos\n",
      "metricName:Score\trank_1:24034.16\trank_2:16763.60\tpolarity:pos\n",
      "metricName:Score\trank_1:260.13\trank_2:260\tpolarity:pos\n",
      "metricName:Score\trank_1:43791\trank_2:43763\tpolarity:pos\n",
      "metricName:Score\trank_1:2623.71\trank_2:2281\tpolarity:pos\n",
      "metricName:Score\trank_1:95756\trank_2:79716.46\tpolarity:pos\n",
      "metricName:Score\trank_1:91.16\trank_2:86.97\tpolarity:pos\n",
      "metricName:Score\trank_1:476763.90\trank_2:445377.3\tpolarity:pos\n",
      "metricName:Score\trank_1:10\trank_2:9.99\tpolarity:pos\n",
      "metricName:Score\trank_1:169300\trank_2:83733\tpolarity:pos\n",
      "metricName:Score\trank_1:16929\trank_2:12343\tpolarity:pos\n",
      "metricName:Score\trank_1:20030\trank_2:16946\tpolarity:pos\n",
      "metricName:Score\trank_1:100\trank_2:83\tpolarity:pos\n",
      "metricName:Score\trank_1:5161\trank_2:3636\tpolarity:pos\n",
      "metricName:FID\trank_1:2.475\trank_2:3.189\tpolarity:neg\n",
      "metricName:LSE-C\trank_1:7.263\trank_2:7.49\tpolarity:neg\n",
      "metricName:LSE-D\trank_1:6.774\trank_2:6.512\tpolarity:pos\n",
      "metricName:FID\trank_1:4.446\trank_2:4.887\tpolarity:neg\n",
      "metricName:LSE-D\trank_1:6.469\trank_2:6.386\tpolarity:pos\n",
      "metricName:FID\trank_1:4.35\trank_2:4.844\tpolarity:neg\n",
      "metricName:LSE-C\trank_1:7.574\trank_2:7.887\tpolarity:neg\n",
      "metricName:LSE-D\trank_1:6.986\trank_2:6.652\tpolarity:pos\n",
      "metricName:Hit@20\trank_1:56.25\trank_2:56.58\tpolarity:neg\n",
      "metricName:MRR@20\trank_1:19.86\trank_2:19.48\tpolarity:pos\n",
      "metricName:MRR@20\trank_1:11.33\trank_2:9.02\tpolarity:pos\n",
      "metricName:HR@20\trank_1:28.82\trank_2:24.76\tpolarity:pos\n",
      "metricName:MRR@20\trank_1:31.16\trank_2:31.04\tpolarity:pos\n",
      "metricName:Precision@20\trank_1:74.3\trank_2:73.0\tpolarity:pos\n",
      "metricName:MRR@20\trank_1:32.87\trank_2:31.78\tpolarity:pos\n",
      "metricName:HR@20\trank_1:70.74\trank_2:72.08\tpolarity:neg\n",
      "metricName:MRR@20\trank_1:32.04\trank_2:31.89\tpolarity:pos\n",
      "metricName:HR@20\trank_1:72.90\trank_2:71.36\tpolarity:pos\n",
      "metricName:Hit@20\trank_1:0.5073\trank_2:0.4704\tpolarity:pos\n",
      "metricName:MRR@20\trank_1:0.3664\trank_2:0.3524\tpolarity:pos\n",
      "metricName:MRR@20\trank_1:26.67\trank_2:24.25\tpolarity:pos\n",
      "metricName:HR@20\trank_1:55.33\trank_2:50.32\tpolarity:pos\n",
      "metricName:PA-MPJPE\trank_1:6.8\trank_2:7.4\tpolarity:neg\n",
      "metricName:PA-MPVPE\trank_1:6.7\trank_2:7.6\tpolarity:neg\n",
      "metricName:FID\trank_1:24.9\trank_2:42.2\tpolarity:neg\n",
      "metricName:FID\trank_1:43.0\trank_2:45.8\tpolarity:neg\n",
      "metricName:FID\trank_1:29.5\trank_2:56.5\tpolarity:neg\n",
      "metricName:FID\trank_1:38.0\trank_2:51.5\tpolarity:neg\n",
      "metricName:CSIM\trank_1:0.653\trank_2:0.638\tpolarity:pos\n",
      "metricName:LPIPS\trank_1:0.358\trank_2:0.311\tpolarity:pos\n",
      "metricName:Normalized Pose Error\trank_1:43.3\trank_2:47.8\tpolarity:neg\n",
      "metricName:SSIM\trank_1:0.508\trank_2:0.553\tpolarity:neg\n",
      "metricName:inference time (ms)\trank_1:4\trank_2:13\tpolarity:neg\n",
      "metricName:mAP@0.25\trank_1:63.4\trank_2:63.0\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:AP\trank_1:64.71\trank_2:61.32\tpolarity:pos\n",
      "metricName:AP\trank_1:56.92\trank_2:55.21\tpolarity:pos\n",
      "metricName:AP\trank_1:95.45\trank_2:92.57\tpolarity:pos\n",
      "metricName:AP\trank_1:81.4\trank_2:77.15\tpolarity:pos\n",
      "metricName:AP\trank_1:66.13\trank_2:64.68\tpolarity:pos\n",
      "metricName:AP\trank_1:59.97\trank_2:56.49\tpolarity:pos\n",
      "metricName:mAP@0.25\trank_1:69.1\trank_2:66.1\tpolarity:pos\n",
      "metricName:mAP@0.5\trank_1:52.8\trank_2:50.9\tpolarity:pos\n",
      "metricName:AP\trank_1:86.83\trank_2:84.83\tpolarity:pos\n",
      "metricName:AP\trank_1:57.65\trank_2:57.64\tpolarity:pos\n",
      "metricName:AP\trank_1:79.58\trank_2:79.22\tpolarity:pos\n",
      "metricName:APH/L2\trank_1:71.52\trank_2:70.16\tpolarity:pos\n",
      "metricName:AP\trank_1:82.54\trank_2:81.88\tpolarity:pos\n",
      "metricName:mAP@0.25\trank_1:63.0\trank_2:61.6\tpolarity:pos\n",
      "metricName:AP\trank_1:91.67\trank_2:91.49\tpolarity:pos\n",
      "metricName:AP\trank_1:44.56\trank_2:42.39\tpolarity:pos\n",
      "metricName:APH/L2\trank_1:71.93\trank_2:71.52\tpolarity:pos\n",
      "metricName:AP\trank_1:82.69\trank_2:82.23\tpolarity:pos\n",
      "metricName:AP\trank_1:73.2\trank_2:70.00\tpolarity:pos\n",
      "metricName:AP\trank_1:56.24\trank_2:53.37\tpolarity:pos\n",
      "metricName:AP\trank_1:77.15\trank_2:77.15\tpolarity:pos\n",
      "metricName:AP\trank_1:56.78\trank_2:53.59\tpolarity:pos\n",
      "metricName:AP\trank_1:47.71\trank_2:44.81\tpolarity:pos\n",
      "metricName:APH/L2\trank_1:71.28\trank_2:71.16\tpolarity:pos\n",
      "metricName:AVG-CDS\trank_1:0.41\trank_2:0.35\tpolarity:pos\n",
      "metricName:NDS\trank_1:0.75\trank_2:0.72\tpolarity:pos\n",
      "metricName:mAAE\trank_1:0.13\trank_2:0.13\tpolarity:pos\n",
      "metricName:mAOE\trank_1:0.32\trank_2:0.35\tpolarity:neg\n",
      "metricName:mAP\trank_1:0.72\trank_2:0.68\tpolarity:pos\n",
      "metricName:mASE\trank_1:0.23\trank_2:0.23\tpolarity:pos\n",
      "metricName:mATE\trank_1:0.24\trank_2:0.25\tpolarity:neg\n",
      "metricName:mAVE\trank_1:0.21\trank_2:0.26\tpolarity:neg\n",
      "metricName:Balanced Error Rate\trank_1:7.21\trank_2:7.69\tpolarity:neg\n",
      "metricName:Average MAE\trank_1:0.089\trank_2:0.091\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:0.849\trank_2:0.842\tpolarity:pos\n",
      "metricName:mean E-Measure\trank_1:0.872\trank_2:0.868\tpolarity:pos\n",
      "metricName:F-measure\trank_1:0.912\trank_2:0.895\tpolarity:pos\n",
      "metricName:MAE\trank_1:0.029\trank_2:0.033\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:0.921\trank_2:0.917\tpolarity:pos\n",
      "metricName:MAE\trank_1:0.026\trank_2:0.027\tpolarity:neg\n",
      "metricName:MAE\trank_1:0.043\trank_2:0.034\tpolarity:pos\n",
      "metricName:MAE\trank_1:0.102\trank_2:0.108\tpolarity:neg\n",
      "metricName:Balanced Error Rate\trank_1:6.76\trank_2:7.10\tpolarity:neg\n",
      "metricName:MAE\trank_1:0.035\trank_2:0.037\tpolarity:neg\n",
      "metricName:F-measure\trank_1:0.88\trank_2:0.824\tpolarity:pos\n",
      "metricName:MAE\trank_1:0.065\trank_2:0.072\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:0.843\trank_2:0.839\tpolarity:pos\n",
      "metricName:MAE\trank_1:0.050\trank_2:0.051\tpolarity:neg\n",
      "metricName:Balanced Error Rate\trank_1:4.19\trank_2:5.59\tpolarity:neg\n",
      "metricName:FPS\trank_1:46.0\trank_2:24\tpolarity:pos\n",
      "metricName:MAP\trank_1:63.4\trank_2:79.1\tpolarity:neg\n",
      "metricName:FPS\trank_1:30\trank_2:16\tpolarity:pos\n",
      "metricName:MAP\trank_1:55.4\trank_2:55.4\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.044\trank_2:0.046\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:89.6\trank_2:89.2\tpolarity:pos\n",
      "metricName:max E-Measure\trank_1:93.3\trank_2:94.9\tpolarity:neg\n",
      "metricName:max F-Measure\trank_1:90.1\trank_2:90.0\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.035\trank_2:0.036\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:92.1\trank_2:92.1\tpolarity:pos\n",
      "metricName:max E-Measure\trank_1:94.9\trank_2:95.4\tpolarity:neg\n",
      "metricName:max F-Measure\trank_1:92.0\trank_2:92.4\tpolarity:neg\n",
      "metricName:Average MAE\trank_1:0.018\trank_2:0.016\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:94.3\trank_2:94.0\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.042\trank_2:0.044\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:88.5\trank_2:88.2\tpolarity:pos\n",
      "metricName:max F-Measure\trank_1:88.1\trank_2:85.9\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.038\trank_2:0.039\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:91.5\trank_2:91.1\tpolarity:pos\n",
      "metricName:max E-Measure\trank_1:94.9\trank_2:94.9\tpolarity:pos\n",
      "metricName:max F-Measure\trank_1:91.1\trank_2:90.7\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.065\trank_2:0.07\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:86.8\trank_2:86.7\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.023\trank_2:0.023\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:93.4\trank_2:93.0\tpolarity:pos\n",
      "metricName:max E-Measure\trank_1:96.5\trank_2:96.1\tpolarity:pos\n",
      "metricName:max F-Measure\trank_1:92.3\trank_2:91.8\tpolarity:pos\n",
      "metricName:MAP \trank_1:85.5\trank_2:85.4\tpolarity:pos\n",
      "metricName:MAP\trank_1:38.4\trank_2:41.1\tpolarity:neg\n",
      "metricName:MAP\trank_1:55.5\trank_2:54.3\tpolarity:pos\n",
      "metricName:mAP\trank_1:40.5\trank_2:39.8\tpolarity:pos\n",
      "metricName:MAP\trank_1:58.1\trank_2:56.8\tpolarity:pos\n",
      "metricName:MAP\trank_1:53.6\trank_2:52.1\tpolarity:pos\n",
      "metricName:MAP\trank_1:58.3\trank_2:55.4\tpolarity:pos\n",
      "metricName:MAP\trank_1:56.6\trank_2:55.3\tpolarity:pos\n",
      "metricName:MAP\trank_1:27\trank_2:37.2\tpolarity:neg\n",
      "metricName:MAP\trank_1:19.6\trank_2:16.3\tpolarity:pos\n",
      "metricName:AP50\trank_1:24.8\trank_2:13.6\tpolarity:pos\n",
      "metricName:MAP\trank_1:15.1\trank_2:13.2\tpolarity:pos\n",
      "metricName:MAP\trank_1:5.39\trank_2:3.62\tpolarity:pos\n",
      "metricName:MAP\trank_1:10.03\trank_2:2.83\tpolarity:pos\n",
      "metricName:AP\trank_1:17.8\trank_2:16.7\tpolarity:pos\n",
      "metricName:AP\trank_1:22.9\trank_2:21.3\tpolarity:pos\n",
      "metricName:AP\trank_1:37.72\trank_2:35.84\tpolarity:pos\n",
      "metricName:AP50\trank_1:52.58\trank_2:49.3\tpolarity:pos\n",
      "metricName:AP75\trank_1:40.64\trank_2:38.4\tpolarity:pos\n",
      "metricName:APc\trank_1:38.23\trank_2:34.49\tpolarity:pos\n",
      "metricName:APf\trank_1:42.8\trank_2:42.69\tpolarity:pos\n",
      "metricName:APr\trank_1:25.18\trank_2:24.05\tpolarity:pos\n",
      "metricName:AP\trank_1:27.26\trank_2:25.8\tpolarity:pos\n",
      "metricName:AP50\trank_1:41.58\trank_2:39.76\tpolarity:pos\n",
      "metricName:AP75\trank_1:28.99\trank_2:27.53\tpolarity:pos\n",
      "metricName:APc\trank_1:26.13\trank_2:25.51\tpolarity:pos\n",
      "metricName:APf\trank_1:31.95\trank_2:31.39\tpolarity:pos\n",
      "metricName:APr\trank_1:19.47\trank_2:13.82\tpolarity:pos\n",
      "metricName:Average Recall\trank_1:0.814\trank_2:0.667\tpolarity:pos\n",
      "metricName:AP\trank_1:0.587\trank_2:0.492\tpolarity:pos\n",
      "metricName:AP75\trank_1:0.673\trank_2:0.556\tpolarity:pos\n",
      "metricName:mPC [AP]\trank_1:17.2\trank_2:12.2\tpolarity:pos\n",
      "metricName:rPC [%]\trank_1:47.4\trank_2:33.4\tpolarity:pos\n",
      "metricName:mPC [AP]\trank_1:20.4\trank_2:18.2\tpolarity:pos\n",
      "metricName:rPC [%]\trank_1:58.9\trank_2:50.2\tpolarity:pos\n",
      "metricName:rPC [%]\trank_1:69.9\trank_2:60.4\tpolarity:pos\n",
      "metricName:mPC [AP50]\trank_1:56.2\trank_2:48.6\tpolarity:pos\n",
      "metricName:E-Measure\trank_1:88.2\trank_2:84.9\tpolarity:pos\n",
      "metricName:MAE\trank_1:0.070\trank_2:0.077\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:82.0\trank_2:78.5\tpolarity:pos\n",
      "metricName:Weighted F-Measure\trank_1:74.3\trank_2:71.9\tpolarity:pos\n",
      "metricName:E-Measure\trank_1:88.7\trank_2:80.6\tpolarity:pos\n",
      "metricName:MAE\trank_1:0.037\trank_2:0.051\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:81.5\trank_2:77.1\tpolarity:pos\n",
      "metricName:Weighted F-Measure\trank_1:68.0\trank_2:55.1\tpolarity:pos\n",
      "metricName:AP 0.5\trank_1:22.0\trank_2:16.3\tpolarity:pos\n",
      "metricName:mAP\trank_1:24.3\trank_2:16.4\tpolarity:pos\n",
      "metricName:Mean Accuracy\trank_1:81\trank_2:74\tpolarity:pos\n",
      "metricName:mAP\trank_1:92.9\trank_2:89.1\tpolarity:pos\n",
      "metricName:F-Measure\trank_1:0.732\trank_2:0.724\tpolarity:pos\n",
      "metricName:Test perplexity\trank_1:515\trank_2:836\tpolarity:neg\n",
      "metricName:F1\trank_1:87.3\trank_2:86.08\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:59.06\trank_2:56.80\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:54.8\trank_2:52.8\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.44\trank_2:97.17\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:72.6\trank_2:71.05\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:94.31\trank_2:93.42\tpolarity:pos\n",
      "metricName:F1\trank_1:72.9\trank_2:69.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.85\trank_2:96.24\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:83.5\trank_2:83.3\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.59\trank_2:97.73\tpolarity:pos\n",
      "metricName:F1\trank_1:78.1\trank_2:70.98\tpolarity:pos\n",
      "metricName:F1\trank_1:84.99\trank_2:84.43\tpolarity:pos\n",
      "metricName:F1\trank_1:65.71\trank_2:64.02\tpolarity:pos\n",
      "metricName:F1\trank_1:84.9\trank_2:84.4\tpolarity:pos\n",
      "metricName:F1\trank_1:92.60\trank_2:86.81\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:0.601\trank_2:0.591\tpolarity:pos\n",
      "metricName:Macro-F1\trank_1:0.578\trank_2:0.549\tpolarity:pos\n",
      "metricName:Micro-F1\trank_1:0.713\trank_2:0.713\tpolarity:pos\n",
      "metricName:F1\trank_1:65.88\trank_2:30.29\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:82.4\trank_2:78.24\tpolarity:pos\n",
      "metricName:F1-score\trank_1:46\trank_2:45.3\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:Micro-F1\trank_1:68.555\trank_2:64.1\tpolarity:pos\n",
      "metricName:Average F1\trank_1:0.8887\trank_2:0.8246\tpolarity:pos\n",
      "metricName:Weighted F1\trank_1:0.8634\trank_2:0.8231\tpolarity:pos\n",
      "metricName:Average F1\trank_1:0.8882\trank_2:0.6642\tpolarity:pos\n",
      "metricName:Weighted F1\trank_1:0.9072\trank_2:0.8137\tpolarity:pos\n",
      "metricName:P@5\trank_1:68.7\trank_2:52.83\tpolarity:pos\n",
      "metricName:nDCG@5\trank_1:82.3\trank_2:68.8\tpolarity:pos\n",
      "metricName:Average F1\trank_1:0.8843\trank_2:0.7761\tpolarity:pos\n",
      "metricName:Weighted F1\trank_1:0.8957\trank_2:0.8235\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:57.9\trank_2:56.3\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:82.1\trank_2:80.2\tpolarity:pos\n",
      "metricName:F1\trank_1:67.9\trank_2:54.6\tpolarity:pos\n",
      "metricName:F1\trank_1:84.99\trank_2:84.0\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:55.39\trank_2:55.09\tpolarity:pos\n",
      "metricName:Average F1\trank_1:46.98\trank_2:46.65\tpolarity:pos\n",
      "metricName:NME\trank_1:6.31\trank_2:6.54\tpolarity:neg\n",
      "metricName:NME\trank_1:7.53\trank_2:10.53\tpolarity:neg\n",
      "metricName:NME\trank_1:2.54\trank_2:2.86\tpolarity:neg\n",
      "metricName:NME\trank_1:4.65\trank_2:5.71\tpolarity:neg\n",
      "metricName:MAE [bpm, session-wise]\trank_1:7.47\trank_2:8.42\tpolarity:neg\n",
      "metricName:MAE [bpm, session-wise]\trank_1:20.45\trank_2:15.56\tpolarity:pos\n",
      "metricName:MAE for DBP [mmHg]\trank_1:6.7\trank_2:6.88\tpolarity:neg\n",
      "metricName:MAE for SBP [mmHg]\trank_1:8.54\trank_2:9.43\tpolarity:neg\n",
      "metricName:Log-Spectral Distance\trank_1:2.5\trank_2:3.2\tpolarity:neg\n",
      "metricName:Log-Spectral Distance\trank_1:2\trank_2:3.4\tpolarity:neg\n",
      "metricName:Log-Spectral Distance\trank_1:1.8\trank_2:3.1\tpolarity:neg\n",
      "metricName:F1 - macro\trank_1:66.4\trank_2:63.5\tpolarity:pos\n",
      "metricName:Top-1\trank_1:75.96\trank_2:72.39\tpolarity:pos\n",
      "metricName:Top-3\trank_1:89.37\trank_2:86.57\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.7\trank_2:91.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.74\trank_2:99.68\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.7\trank_2:91.6\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:94.7\trank_2:94.1\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:90.8\trank_2:89.2\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:92.3\trank_2:90.3\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.6\trank_2:95.9\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:90.07\trank_2:84.3\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.55\trank_2:99.02\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.56\trank_2:97.32\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.32\trank_2:96.2\tpolarity:pos\n",
      "metricName:Top-1 Error Rate\trank_1:2.68\trank_2:2.89\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:96.18\trank_2:95.88\tpolarity:pos\n",
      "metricName:F1 - macro\trank_1:74.7\trank_2:72.4\tpolarity:pos\n",
      "metricName:Top-1\trank_1:81.25\trank_2:79.4\tpolarity:pos\n",
      "metricName:Top-3\trank_1:91.93\trank_2:90.93\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:80.9\trank_2:80.2\tpolarity:pos\n",
      "metricName:Top 5 Accuracy\trank_1:95.5\trank_2:95.0\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:92.97\trank_2:90.74\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:77.89\trank_2:77.08\tpolarity:pos\n",
      "metricName:Percentage correct\trank_1:93.73\trank_2:92.4\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:92.25\trank_2:91.82\tpolarity:pos\n",
      "metricName:Percentage error\trank_1:6.81\trank_2:7.45\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:98.01\trank_2:97.64\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:44.65\trank_2:40.65\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.79\trank_2:96.52\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:94.83\trank_2:93.82\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:76.6\trank_2:75.9\tpolarity:pos\n",
      "metricName:Top 5 Accuracy\trank_1:93.4\trank_2:93.0\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:97.13\trank_2:96.2\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:95.13\trank_2:94.93\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:66.55\trank_2:60.95\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:96.39\trank_2:96.36\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:65.21\trank_2:58.75\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:95.48\trank_2:88.8\tpolarity:pos\n",
      "metricName:% Test Accuracy\trank_1:45.96\trank_2:45.96\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:84.2\trank_2:83.8\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:81.3\trank_2:81.0\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:84.1\trank_2:83.4\tpolarity:pos\n",
      "metricName:Permuted Accuracy\trank_1:98.54\trank_2:98.49\tpolarity:pos\n",
      "metricName:Unpermuted Accuracy\trank_1:73.42\trank_2:64.2\tpolarity:pos\n",
      "metricName:Top 1 Accuracy\trank_1:24.7\trank_2:13.5\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.3\trank_2:98.32\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:86.7\trank_2:80.90\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:54.3\trank_2:50.7\tpolarity:pos\n",
      "metricName:# of clusters (k)\trank_1:10\trank_2:10\tpolarity:pos\n",
      "metricName:Acc\trank_1:76.80\trank_2:57.4\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:90.3\trank_2:88.3\tpolarity:pos\n",
      "metricName:ARI\trank_1:27.5\trank_2:23.94\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:39.9\trank_2:46.03\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:96.68\trank_2:95.46\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:95.64\trank_2:95.52\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:98.43\trank_2:97.62\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:89.54\trank_2:77.22\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:91.95\trank_2:91\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.90\trank_2:98.74\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.84\trank_2:93.92\tpolarity:pos\n",
      "metricName:Classification Error\trank_1:0.95\trank_2:1.24\tpolarity:neg\n",
      "metricName:City level (25 km)\trank_1:43.0\trank_2:37.1\tpolarity:pos\n",
      "metricName:Continent level (2500 km)\trank_1:80.2\trank_2:78.5\tpolarity:pos\n",
      "metricName:Country level (750 km)\trank_1:66.7\trank_2:62.0\tpolarity:pos\n",
      "metricName:Reference images\trank_1:0\trank_2:0\tpolarity:pos\n",
      "metricName:Region level (200 km)\trank_1:51.9\trank_2:46.6\tpolarity:pos\n",
      "metricName:Street level (1 km)\trank_1:16.9\trank_2:16.5\tpolarity:pos\n",
      "metricName:City level (25 km)\trank_1:28.0\trank_2:26.5\tpolarity:pos\n",
      "metricName:Continent level (2500 km)\trank_1:66.0\trank_2:64.4\tpolarity:pos\n",
      "metricName:Country level (750 km)\trank_1:49.7\trank_2:48.6\tpolarity:pos\n",
      "metricName:Region level (200 km)\trank_1:36.6\trank_2:34.6\tpolarity:pos\n",
      "metricName:Street level (1 km)\trank_1:10.5\trank_2:10.2\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:99.32\trank_2:99.32\tpolarity:pos\n",
      "metricName:MRR\trank_1:0.351\trank_2:0.340\tpolarity:pos\n",
      "metricName:MRR\trank_1:0.126\trank_2:0.117\tpolarity:pos\n",
      "metricName:MRR\trank_1:0.323\trank_2:0.304\tpolarity:pos\n",
      "metricName:MAP\trank_1:88.3\trank_2:77.3\tpolarity:pos\n",
      "metricName:GAR @0.1% FAR\trank_1:23.25\trank_2:17.73\tpolarity:pos\n",
      "metricName:GAR @1% FAR\trank_1:60.89\trank_2:33.76\tpolarity:pos\n",
      "metricName:Average Class Accuracy \trank_1:98.86\trank_2:93.46\tpolarity:pos\n",
      "metricName:Dice\trank_1:0.812\trank_2:0.801\tpolarity:pos\n",
      "metricName:Grad Det-Jac\trank_1:1.4\trank_2:3.4\tpolarity:neg\n",
      "metricName:Hausdorff Distance (mm)\trank_1:7.3\trank_2:8.1\tpolarity:neg\n",
      "metricName:RMSE\trank_1:0.30\trank_2:0.32\tpolarity:neg\n",
      "metricName:Mean target overlap ratio\trank_1:0.520\trank_2:0.514\tpolarity:pos\n",
      "metricName:CPU (sec)\trank_1:2347\trank_2:84.2\tpolarity:pos\n",
      "metricName:Dice (Average)\trank_1:0.755\trank_2:0.754\tpolarity:pos\n",
      "metricName:Dice (SE)\trank_1:0.143\trank_2:0.139\tpolarity:pos\n",
      "metricName:Neg Jacob Det\trank_1:33838\trank_2:0.2\tpolarity:pos\n",
      "metricName:AMrTRE\trank_1:2.3\trank_2:2.5\tpolarity:neg\n",
      "metricName:MMrTRE\trank_1:1.67\trank_2:0.51\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:99.76\trank_2:99.55\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:94.78\trank_2:93.2\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:84.81\trank_2:73.56\tpolarity:pos\n",
      "metricName:Rank-1 Recognition Rate\trank_1:99.65\trank_2:99.13\tpolarity:pos\n",
      "metricName:PCK\trank_1:79.4\trank_2:79.2\tpolarity:pos\n",
      "metricName:IoU\trank_1:63\trank_2:62\tpolarity:pos\n",
      "metricName:LT-ACC\trank_1:87\trank_2:87\tpolarity:pos\n",
      "metricName:PCK\trank_1:92.6\trank_2:91.6\tpolarity:pos\n",
      "metricName:PCK\trank_1:49.9\trank_2:46.3\tpolarity:pos\n",
      "metricName:Patch Matching\trank_1:53.95\trank_2:45.3\tpolarity:pos\n",
      "metricName:Patch Retrieval\trank_1:71.66\trank_2:55.6\tpolarity:pos\n",
      "metricName:Patch Verification\trank_1:89.06\trank_2:95.6\tpolarity:neg\n",
      "metricName:FPR95\trank_1:0.9\trank_2:1.27\tpolarity:neg\n",
      "metricName:ACER\trank_1:0.097\trank_2:0.3\tpolarity:neg\n",
      "metricName:3DIoU\trank_1:0.642\trank_2:0.640\tpolarity:pos\n",
      "metricName:Average Accuracy\trank_1:31.68\trank_2:10.29\tpolarity:pos\n",
      "metricName:CD\trank_1:0.119\trank_2:0.125\tpolarity:neg\n",
      "metricName:EMD\trank_1:0.118\trank_2:0.128\tpolarity:neg\n",
      "metricName:Top-10 Accuracy\trank_1:0.8\trank_2:0.78\tpolarity:pos\n",
      "metricName:Top-5 Accuracy\trank_1:0.7\trank_2:0.68\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:93.8\trank_2:90\tpolarity:pos\n",
      "metricName:Classification Accuracy\trank_1:89.9\trank_2:89.3\tpolarity:pos\n",
      "metricName:R@1\trank_1:0.53\trank_2:0.42\tpolarity:pos\n",
      "metricName:R@16\trank_1:0.85\trank_2:0.71\tpolarity:pos\n",
      "metricName:R@2\trank_1:0.62\trank_2:0.51\tpolarity:pos\n",
      "metricName:R@32\trank_1:0.90\trank_2:0.78\tpolarity:pos\n",
      "metricName:R@4\trank_1:0.71\trank_2:0.57\tpolarity:pos\n",
      "metricName:R@8\trank_1:0.78\trank_2:0.64\tpolarity:pos\n",
      "metricName:AED\trank_1:0.152\trank_2:0.172\tpolarity:neg\n",
      "metricName:AKD\trank_1:5.58\trank_2:6.53\tpolarity:neg\n",
      "metricName:L1\trank_1:0.047\trank_2:0.056\tpolarity:neg\n",
      "metricName:MKR\trank_1:0.027\trank_2:0.033\tpolarity:neg\n",
      "metricName:AED\trank_1:0.163\trank_2:0.114\tpolarity:pos\n",
      "metricName:AKD\trank_1:7.07\trank_2:3.75\tpolarity:pos\n",
      "metricName:L1\trank_1:0.033\trank_2:0.026\tpolarity:pos\n",
      "metricName:MKR\trank_1:0.014\trank_2:0.007\tpolarity:pos\n",
      "metricName:L1\trank_1:0.0206\trank_2:0.0223\tpolarity:neg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:AED\trank_1:0.133\trank_2:0.134\tpolarity:neg\n",
      "metricName:AKD\trank_1:1.28\trank_2:1.27\tpolarity:pos\n",
      "metricName:L1\trank_1:0.040\trank_2:0.041\tpolarity:neg\n",
      "metricName:AED\trank_1:0.172\trank_2:0.203\tpolarity:neg\n",
      "metricName:AKD\trank_1:13.86\trank_2:17.12\tpolarity:neg\n",
      "metricName:L1\trank_1:0.064\trank_2:0.075\tpolarity:neg\n",
      "metricName:MKR\trank_1:0.043\trank_2:0.066\tpolarity:neg\n",
      "metricName:Rank-1 Recognition Rate\trank_1:98.2\trank_2:85.45\tpolarity:pos\n",
      "metricName:Average Recall\trank_1:99.18\trank_2:98.36\tpolarity:pos\n",
      "metricName:GFLOPs\trank_1:0.34\trank_2:0.94\tpolarity:neg\n",
      "metricName:Rank-1 Recognition Rate\trank_1:90\trank_2:64.89\tpolarity:pos\n",
      "metricName:mIoU\trank_1:47.3\trank_2:35.8\tpolarity:pos\n",
      "metricName:Accuracy (Test)\trank_1:71.56\trank_2:70.61\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:89.4\trank_2:89.0\tpolarity:pos\n",
      "metricName:Percentage error\trank_1:1.6\trank_2:1.8\tpolarity:neg\n",
      "metricName:Percentage Error\trank_1:11.7\trank_2:12.3\tpolarity:neg\n",
      "metricName:Top-1 Error Rate\trank_1:1.98\trank_2:2.0\tpolarity:neg\n",
      "metricName:Accuracy (Test)\trank_1:93.51\trank_2:92.63\tpolarity:pos\n",
      "metricName:Search time (s)\trank_1:28926\trank_2:2.3\tpolarity:pos\n",
      "metricName:Accuracy (val)\trank_1:90.00\trank_2:89.90\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:92.9\trank_2:92.6\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:94.3\trank_2:94.1\tpolarity:pos\n",
      "metricName:Top-1 Error Rate\trank_1:19.4\trank_2:19.5\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:80.6\trank_2:80.5\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:98.3\trank_2:98.1\tpolarity:pos\n",
      "metricName:Accuracy (Test)\trank_1:46.38\trank_2:46.37\tpolarity:pos\n",
      "metricName:Search time (s)\trank_1:151200\trank_2:75600\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:79.1\trank_2:78.4\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:90.8\trank_2:90.1\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:97.9\trank_2:97.8\tpolarity:pos\n",
      "metricName:Accuracy (%)\trank_1:94.8\trank_2:94.3\tpolarity:pos\n",
      "metricName:CMC1\trank_1:0.8\trank_2:0.78\tpolarity:pos\n",
      "metricName:CMC10\trank_1:0.95\trank_2:0.95\tpolarity:pos\n",
      "metricName:CMC5\trank_1:0.89\trank_2:0.89\tpolarity:pos\n",
      "metricName:MAP\trank_1:0.78\trank_2:0.78\tpolarity:pos\n",
      "metricName:CMC1\trank_1:0.81\trank_2:0.68\tpolarity:pos\n",
      "metricName:CMC10\trank_1:0.94\trank_2:0.89\tpolarity:pos\n",
      "metricName:CMC5\trank_1:0.9\trank_2:0.81\tpolarity:pos\n",
      "metricName:MAP\trank_1:0.79\trank_2:0.69\tpolarity:pos\n",
      "metricName:mAP\trank_1:83.41\trank_2:82.2\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:91.9\trank_2:80.5\tpolarity:pos\n",
      "metricName:Rank-5\trank_1:96.2\trank_2:94.6\tpolarity:pos\n",
      "metricName:mAP\trank_1:87.1\trank_2:83.41\tpolarity:pos\n",
      "metricName:Rank-1\trank_1:94.9\trank_2:87.9\tpolarity:pos\n",
      "metricName:Rank-5\trank_1:97.6\trank_2:97.8\tpolarity:neg\n",
      "metricName:Rank-1\trank_1:93.3\trank_2:82.8\tpolarity:pos\n",
      "metricName:Rank-5\trank_1:96.4\trank_2:96.2\tpolarity:pos\n",
      "metricName:F1\trank_1:81.9\trank_2:59.3\tpolarity:pos\n",
      "metricName:F1\trank_1:81.3\trank_2:77.7\tpolarity:pos\n",
      "metricName:Laptop (F1)\trank_1:87.4\trank_2:86.09\tpolarity:pos\n",
      "metricName:Restaurant (F1)\trank_1:92.0\trank_2:82.34\tpolarity:pos\n",
      "metricName:Restaurant (F1)\trank_1:80.3\trank_2:72.7\tpolarity:pos\n",
      "metricName:F1\trank_1:79.9\trank_2:74.3\tpolarity:pos\n",
      "metricName:F1\trank_1:80.5\trank_2:72.9\tpolarity:pos\n",
      "metricName:F1\trank_1:79.4\trank_2:71.8\tpolarity:pos\n",
      "metricName:F1 score\trank_1:0.70\trank_2:0.63\tpolarity:pos\n",
      "metricName:Laptop 2014 (F1)\trank_1:80.55\trank_2:79.90\tpolarity:pos\n",
      "metricName:Restaurant 2014 (F1)\trank_1:85.38\trank_2:83.73\tpolarity:pos\n",
      "metricName:Restaurant 2015 (F1)\trank_1:80.52\trank_2:74.50\tpolarity:pos\n",
      "metricName:Restaurant 2016 (F1)\trank_1:87.92\trank_2:83.33\tpolarity:pos\n",
      "metricName:DAC (K=6)\trank_1:0.9883\trank_2:0.9878\tpolarity:pos\n",
      "metricName:MR (K=1)\trank_1:0.5395\trank_2:0.5378\tpolarity:pos\n",
      "metricName:MR (K=6)\trank_1:0.1143\trank_2:0.1155\tpolarity:neg\n",
      "metricName:brier-minFDE (K=6)\trank_1:1.7568\trank_2:1.7654\tpolarity:neg\n",
      "metricName:minADE (K=1)\trank_1:1.5599\trank_2:1.5349\tpolarity:pos\n",
      "metricName:minADE (K=6)\trank_1:0.8014\trank_2:0.8053\tpolarity:neg\n",
      "metricName:minFDE (K=1)\trank_1:3.3814\trank_2:3.3216\tpolarity:pos\n",
      "metricName:minFDE (K=6)\trank_1:1.2139\trank_2:1.1385\tpolarity:pos\n",
      "metricName:Mean AP @ 0.5\trank_1:63.8\trank_2:63.6\tpolarity:pos\n",
      "metricName:mPrec\trank_1:72.8\trank_2:69.6\tpolarity:pos\n",
      "metricName:mRec\trank_1:60.3\trank_2:69.2\tpolarity:neg\n",
      "metricName:mAP@0.5\trank_1:47.1\trank_2:12.1\tpolarity:pos\n",
      "metricName:Overall IoU\trank_1:60.06\trank_2:53.44\tpolarity:pos\n",
      "metricName:Overall IoU\trank_1:69.33\trank_2:64.53\tpolarity:pos\n",
      "metricName:Overall IoU\trank_1:45.49\trank_2:43.23\tpolarity:pos\n",
      "metricName:Overall IoU\trank_1:65.76\trank_2:61.36\tpolarity:pos\n",
      "metricName:Precision@0.5\trank_1:0.704\trank_2:0.655\tpolarity:pos\n",
      "metricName:Precision@0.6\trank_1:0.677\trank_2:0.592\tpolarity:pos\n",
      "metricName:Precision@0.7\trank_1:0.617\trank_2:0.506\tpolarity:pos\n",
      "metricName:Precision@0.8\trank_1:0.489\trank_2:0.342\tpolarity:pos\n",
      "metricName:Precision@0.9\trank_1:0.171\trank_2:0.098\tpolarity:pos\n",
      "metricName:AP\trank_1:0.494\trank_2:0.404\tpolarity:pos\n",
      "metricName:IoU mean\trank_1:0.655\trank_2:0.573\tpolarity:pos\n",
      "metricName:IoU overall\trank_1:0.644\trank_2:0.653\tpolarity:neg\n",
      "metricName:Overall IoU\trank_1:60.93\trank_2:59.64\tpolarity:pos\n",
      "metricName:Precision@0.5\trank_1:0.880\trank_2:0.813\tpolarity:pos\n",
      "metricName:Precision@0.6\trank_1:0.796\trank_2:0.657\tpolarity:pos\n",
      "metricName:Precision@0.7\trank_1:0.566\trank_2:0.371\tpolarity:pos\n",
      "metricName:Precision@0.8\trank_1:0.147\trank_2:0.07\tpolarity:pos\n",
      "metricName:Precision@0.9\trank_1:0.002\trank_2:0.000\tpolarity:pos\n",
      "metricName:AP\trank_1:0.433\trank_2:0.342\tpolarity:pos\n",
      "metricName:IoU mean\trank_1:0.655\trank_2:0.617\tpolarity:pos\n",
      "metricName:IoU overall\trank_1:0.644\trank_2:0.616\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:53.7\trank_2:41.3\tpolarity:pos\n",
      "metricName:Pr@0.5\trank_1:57.5\trank_2:42.9\tpolarity:pos\n",
      "metricName:Pr@0.7\trank_1:39.9\trank_2:27.8\tpolarity:pos\n",
      "metricName:Pr@0.9\trank_1:11.9\trank_2:5.9\tpolarity:pos\n",
      "metricName:J&F 1st frame\trank_1:44.5\trank_2:44.1\tpolarity:pos\n",
      "metricName:Overall IoU\trank_1:53.97\trank_2:49.56\tpolarity:pos\n",
      "metricName:Frame (fps)\trank_1:27.0\trank_2:33.3\tpolarity:neg\n",
      "metricName:mask AP\trank_1:35.4\trank_2:35.2\tpolarity:pos\n",
      "metricName:mAP@0.50\trank_1:61.1\trank_2:54.9\tpolarity:pos\n",
      "metricName:MS-SSIM\trank_1:0.47\trank_2:0.45\tpolarity:pos\n",
      "metricName:F-Measure (Seen)\trank_1:87.9\trank_2:87.5\tpolarity:pos\n",
      "metricName:F-Measure (Unseen)\trank_1:87.3\trank_2:86.7\tpolarity:pos\n",
      "metricName:Jaccard (Seen)\trank_1:83.2\trank_2:82.5\tpolarity:pos\n",
      "metricName:Jaccard (Unseen)\trank_1:79.0\trank_2:77.9\tpolarity:pos\n",
      "metricName:Overall\trank_1:84.3\trank_2:83.7\tpolarity:pos\n",
      "metricName:Speed  (FPS)\trank_1:13.4\trank_2:15.2\tpolarity:neg\n",
      "metricName:F-measure (Decay)\trank_1:85.3\trank_2:8.2\tpolarity:pos\n",
      "metricName:F-measure (Mean)\trank_1:88.6\trank_2:87.4\tpolarity:pos\n",
      "metricName:F-measure (Recall)\trank_1:94.6\trank_2:93.1\tpolarity:pos\n",
      "metricName:J&F\trank_1:85.3\trank_2:84.5\tpolarity:pos\n",
      "metricName:Jaccard (Decay)\trank_1:6.2\trank_2:7.0\tpolarity:neg\n",
      "metricName:Jaccard (Mean)\trank_1:82.0\trank_2:81.7\tpolarity:pos\n",
      "metricName:Jaccard (Recall)\trank_1:91.3\trank_2:90.9\tpolarity:pos\n",
      "metricName:Speed (FPS)\trank_1:20.2\trank_2:11.2\tpolarity:pos\n",
      "metricName:mIoU\trank_1:0.821\trank_2:0.796\tpolarity:pos\n",
      "metricName:F-measure (Mean)\trank_1:83.5\trank_2:81.6\tpolarity:pos\n",
      "metricName:J&F\trank_1:79.9\trank_2:78.0\tpolarity:pos\n",
      "metricName:Jaccard (Mean)\trank_1:76.3\trank_2:74.4\tpolarity:pos\n",
      "metricName:F-measure (Decay)\trank_1:4.3\trank_2:5.1\tpolarity:neg\n",
      "metricName:F-measure (Mean)\trank_1:93.0\trank_2:92.4\tpolarity:pos\n",
      "metricName:F-measure (Recall)\trank_1:97.1\trank_2:96.4\tpolarity:pos\n",
      "metricName:J&F\trank_1:91.7\trank_2:91.0\tpolarity:pos\n",
      "metricName:Jaccard (Decay)\trank_1:4.1\trank_2:6.6\tpolarity:neg\n",
      "metricName:Jaccard (Mean)\trank_1:90.4\trank_2:89.7\tpolarity:pos\n",
      "metricName:Jaccard (Recall)\trank_1:98.1\trank_2:97.5\tpolarity:pos\n",
      "metricName:Speed (FPS)\trank_1:26.9\trank_2:16.9\tpolarity:pos\n",
      "metricName:F-measure (Decay)\trank_1:6.6\trank_2:2.6\tpolarity:pos\n",
      "metricName:F-measure (Mean)\trank_1:62.0\trank_2:49.0\tpolarity:pos\n",
      "metricName:F-measure (Recall)\trank_1:66.6\trank_2:51.5\tpolarity:pos\n",
      "metricName:J&F\trank_1:58.0\trank_2:45.6\tpolarity:pos\n",
      "metricName:Jaccard (Decay)\trank_1:3.5\trank_2:2.6\tpolarity:pos\n",
      "metricName:Jaccard (Mean)\trank_1:54.0\trank_2:42.1\tpolarity:pos\n",
      "metricName:Jaccard (Recall)\trank_1:62.9\trank_2:48.5\tpolarity:pos\n",
      "metricName:Jaccard (Mean)\trank_1:77.6\trank_2:75.6\tpolarity:pos\n",
      "metricName:F-measure (Decay)\trank_1:4.3\trank_2:3.0\tpolarity:pos\n",
      "metricName:F-measure (Mean)\trank_1:86.2\trank_2:85.4\tpolarity:pos\n",
      "metricName:F-measure (Recall)\trank_1:94.1\trank_2:93.2\tpolarity:pos\n",
      "metricName:J&F\trank_1:86.1\trank_2:85.6\tpolarity:pos\n",
      "metricName:Jaccard (Decay)\trank_1:5.7\trank_2:3.4\tpolarity:pos\n",
      "metricName:Jaccard (Mean)\trank_1:86.0\trank_2:85.8\tpolarity:pos\n",
      "metricName:Jaccard (Recall)\trank_1:96.1\trank_2:96.2\tpolarity:neg\n",
      "metricName:F-measure (Decay)\trank_1:1.5\trank_2:0.01\tpolarity:pos\n",
      "metricName:F-measure (Mean)\trank_1:72.7\trank_2:69.3\tpolarity:pos\n",
      "metricName:F-measure (Recall)\trank_1:80.3\trank_2:76.9\tpolarity:pos\n",
      "metricName:J&F\trank_1:70.7\trank_2:67.9\tpolarity:pos\n",
      "metricName:Jaccard (Decay)\trank_1:0.9\trank_2:0.2\tpolarity:pos\n",
      "metricName:Jaccard (Mean)\trank_1:68.7\trank_2:66.4\tpolarity:pos\n",
      "metricName:Jaccard (Recall)\trank_1:77.7\trank_2:76.4\tpolarity:pos\n",
      "metricName:Mean IoU\trank_1:72.2\trank_2:70.9\tpolarity:pos\n",
      "metricName:AVERAGE MAE\trank_1:0.021\trank_2:0.026\tpolarity:neg\n",
      "metricName:MAX E-MEASURE\trank_1:0.911\trank_2:0.889\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:S-Measure\trank_1:0.856\trank_2:0.819\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:0.619\trank_2:0.608\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.114\trank_2:0.131\tpolarity:neg\n",
      "metricName:max E-measure\trank_1:0.696\trank_2:0.698\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:0.942\trank_2:0.907\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.021\trank_2:0.032\tpolarity:neg\n",
      "metricName:max E-measure\trank_1:0.980\trank_2:0.846\tpolarity:pos\n",
      "metricName:AVERAGE MAE\trank_1:0.040\trank_2:0.054\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:0.879\trank_2:0.870\tpolarity:pos\n",
      "metricName:MAX F-MEASURE\trank_1:0.865\trank_2:0.861\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:0.755\trank_2:0.706\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.084\trank_2:0.114\tpolarity:neg\n",
      "metricName:max E-Measure\trank_1:0.806\trank_2:0.749\tpolarity:pos\n",
      "metricName:max F-Measure\trank_1:0.659\trank_2:0.591\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:0.901\trank_2:0.860\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.018\trank_2:0.025\tpolarity:neg\n",
      "metricName:max E-measure\trank_1:0.975\trank_2:0.939\tpolarity:pos\n",
      "metricName:AVERAGE MAE\trank_1:0.024\trank_2:0.023\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:0.864\trank_2:0.850\tpolarity:pos\n",
      "metricName:max E-measure\trank_1:0.935\trank_2:0.917\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:0.661\trank_2:0.649\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.117\trank_2:0.132\tpolarity:neg\n",
      "metricName:max E-measure\trank_1:0.723\trank_2:0.698\tpolarity:pos\n",
      "metricName:S-Measure\trank_1:0.872\trank_2:0.819\tpolarity:pos\n",
      "metricName:Average MAE\trank_1:0.049\trank_2:0.074\tpolarity:neg\n",
      "metricName:max E-measure\trank_1:0.856\trank_2:0.839\tpolarity:pos\n",
      "metricName:AVERAGE MAE\trank_1:0.028\trank_2:0.031\tpolarity:neg\n",
      "metricName:MAX E-MEASURE\trank_1:0.948\trank_2:0.966\tpolarity:neg\n",
      "metricName:S-Measure\trank_1:0.893\trank_2:0.887\tpolarity:pos\n",
      "metricName:MAX F-MEASURE\trank_1:0.861\trank_2:0.862\tpolarity:neg\n",
      "metricName:AUC-J\trank_1:0.849\trank_2:0.820\tpolarity:pos\n",
      "metricName:AUC-J&F\trank_1:0.879\trank_2:0.856\tpolarity:pos\n",
      "metricName:J&F@60s\trank_1:0.885\trank_2:0.866\tpolarity:pos\n",
      "metricName:J@60s\trank_1:0.854\trank_2:0.829\tpolarity:pos\n",
      "metricName:Conn\trank_1:20.83\trank_2:36.03\tpolarity:neg\n",
      "metricName:Grad\trank_1:11.57\trank_2:28.70\tpolarity:neg\n",
      "metricName:MSE(10^3)\trank_1:4.7\trank_2:11.0\tpolarity:neg\n",
      "metricName:SAD\trank_1:27.87\trank_2:39.28\tpolarity:neg\n",
      "metricName:Accuracy\trank_1:99.3\trank_2:98.7\tpolarity:pos\n",
      "metricName:ABX-across\trank_1:13.4\trank_2:14\tpolarity:neg\n",
      "metricName:Time (ms)\trank_1:3.5\trank_2:7.18\tpolarity:neg\n",
      "metricName:3DPCK\trank_1:87.5\trank_2:85.3\tpolarity:pos\n",
      "metricName:3DPCK\trank_1:45.7\trank_2:43.8\tpolarity:pos\n",
      "metricName:FID\trank_1:9.5\trank_2:11.4\tpolarity:neg\n",
      "metricName:FID\trank_1:2.97\trank_2:5.7\tpolarity:neg\n",
      "metricName:FID\trank_1:12.15\trank_2:12.84\tpolarity:neg\n",
      "metricName:FID\trank_1:2.99\trank_2:7.22\tpolarity:neg\n",
      "metricName:FID\trank_1:2.84\trank_2:2.99\tpolarity:neg\n",
      "metricName:FID\trank_1:3.31\trank_2:3.35\tpolarity:neg\n",
      "metricName:nats\trank_1:76.93\trank_2:77.58\tpolarity:neg\n",
      "metricName:FID\trank_1:3.94\trank_2:4.59\tpolarity:neg\n",
      "metricName:Inception score\trank_1:215.84\trank_2:186.7\tpolarity:pos\n",
      "metricName:FID\trank_1:5.57\trank_2:6.93\tpolarity:neg\n",
      "metricName:FID\trank_1:2.95\trank_2:19.9\tpolarity:neg\n",
      "metricName:Bits per dim\trank_1:3.35\trank_2:3.43\tpolarity:neg\n",
      "metricName:FID-5k-training-steps\trank_1:9.1679\trank_2:16.0534\tpolarity:neg\n",
      "metricName:bits/dimension\trank_1:0.65\trank_2:0.97\tpolarity:neg\n",
      "metricName:FID\trank_1:2.10\trank_2:2.17\tpolarity:neg\n",
      "metricName:bits/dimension\trank_1:3.43\trank_2:2.95\tpolarity:pos\n",
      "metricName:FID\trank_1:2.57\trank_2:3.43\tpolarity:neg\n",
      "metricName:FID\trank_1:7.16\trank_2:7.22\tpolarity:neg\n",
      "metricName:Intra-FID\trank_1:0.307\trank_2:0.389\tpolarity:neg\n",
      "metricName:FID\trank_1:3.85\trank_2:7.72\tpolarity:neg\n",
      "metricName:Inception score\trank_1:221.72\trank_2:172.71\tpolarity:pos\n",
      "metricName:FID\trank_1:2.03\trank_2:5.74\tpolarity:neg\n",
      "metricName:FID\trank_1:5.06\trank_2:5.5\tpolarity:neg\n",
      "metricName:FID\trank_1:10.3\trank_2:13.7\tpolarity:neg\n",
      "metricName:FID\trank_1:1.90\trank_2:2.65\tpolarity:neg\n",
      "metricName:bpd\trank_1:0.61\trank_2:0.67\tpolarity:neg\n",
      "metricName:FID\trank_1:11.25\trank_2:13.20\tpolarity:neg\n",
      "metricName:Inception score\trank_1:52.53\trank_2:47.32\tpolarity:pos\n",
      "metricName:FID\trank_1:12.96\trank_2:23.965\tpolarity:neg\n",
      "metricName:FID\trank_1:10.16\trank_2:32.11\tpolarity:neg\n",
      "metricName:FID\trank_1:2.92\trank_2:3.86\tpolarity:neg\n",
      "metricName:FID\trank_1:52.6\trank_2:62.8\tpolarity:neg\n",
      "metricName:FID\trank_1:15.17\trank_2:17.68\tpolarity:neg\n",
      "metricName:Inception score\trank_1:11.01\trank_2:9.33\tpolarity:pos\n",
      "metricName:FID-10k-training-steps\trank_1:5.7589\trank_2:8.35\tpolarity:neg\n",
      "metricName:FID\trank_1:4.0\trank_2:5.31\tpolarity:neg\n",
      "metricName:bpd\trank_1:3.63\trank_2:3.76\tpolarity:neg\n",
      "metricName:FID\trank_1:3.66\trank_2:5.25\tpolarity:neg\n",
      "metricName:FID\trank_1:112.7\trank_2:186.8\tpolarity:neg\n",
      "metricName:FID\trank_1:14.5\trank_2:18.7\tpolarity:neg\n",
      "metricName:FID\trank_1:25.66\trank_2:29.34\tpolarity:neg\n",
      "metricName:Inception score\trank_1:46.92\trank_2:43.16\tpolarity:pos\n",
      "metricName:FID\trank_1:16.03\trank_2:17.63\tpolarity:neg\n",
      "metricName:Inception score\trank_1:32.62\trank_2:28.62\tpolarity:pos\n",
      "metricName:FID\trank_1:5.74\trank_2:29.115\tpolarity:neg\n",
      "metricName:FID-50k\trank_1:2.65\trank_2:4.9\tpolarity:neg\n",
      "metricName:Parameters (M)\trank_1:0.44\trank_2:1.0\tpolarity:neg\n",
      "metricName:Speed  (FPS)\trank_1:98\trank_2:47\tpolarity:pos\n",
      "metricName:mIoU\trank_1:46.9\trank_2:55.2\tpolarity:neg\n",
      "metricName:Normalized cPSNR\trank_1:0.9336790819983855\trank_2:0.9411827883122681\tpolarity:neg\n",
      "metricName:3-fold Accuracy\trank_1:95.5\trank_2:95.2\tpolarity:pos\n",
      "metricName:Top-1 Accuracy\trank_1:77.8\trank_2:70.1\tpolarity:pos\n",
      "metricName:Top-1 Accuracy\trank_1:71.4\trank_2:70.4\tpolarity:pos\n",
      "metricName:3-fold Accuracy\trank_1:95.7\trank_2:95.5\tpolarity:pos\n",
      "metricName:Top-1 Accuracy\trank_1:75.0\trank_2:70.5\tpolarity:pos\n",
      "metricName:PESQ\trank_1:2.75\trank_2:1.82\tpolarity:pos\n",
      "metricName:Macro F1\trank_1:19.7\trank_2:8.8\tpolarity:pos\n",
      "metricName:Macro Precision\trank_1:20.2\trank_2:9.5\tpolarity:pos\n",
      "metricName:Macro Recall\trank_1:20.6\trank_2:8.6\tpolarity:pos\n",
      "metricName:Micro F1\trank_1:25.8\trank_2:7.5\tpolarity:pos\n",
      "metricName:Micro Precision\trank_1:27.4\trank_2:6.8\tpolarity:pos\n",
      "metricName:Micro Recall\trank_1:24.4\trank_2:8.4\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:93.8\trank_2:92.3\tpolarity:pos\n",
      "metricName:AP 0.5\trank_1:57.09\trank_2:52.68\tpolarity:pos\n",
      "metricName:AP 0.5\trank_1:59.70\trank_2:40.60\tpolarity:pos\n",
      "metricName:AP 0.5\trank_1:25.14\trank_2:17.99\tpolarity:pos\n",
      "metricName:Macro-F1 (20% training data)\trank_1:92.24\trank_2:92.03\tpolarity:pos\n",
      "metricName:Macro-F1 (60% training data)\trank_1:93.70\trank_2:93.31\tpolarity:pos\n",
      "metricName:Macro-F1 (80% training data)\trank_1:93.08\trank_2:92.53\tpolarity:pos\n",
      "metricName:Micro-F1 (20% training data)\trank_1:93.11\trank_2:92.99\tpolarity:pos\n",
      "metricName:Micro-F1 (80% training data)\trank_1:93.99\trank_2:93.29\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:92.7\trank_2:87.4\tpolarity:pos\n",
      "metricName:F1\trank_1:93.38\trank_2:91.51\tpolarity:pos\n",
      "metricName:Precision\trank_1:92.98\trank_2:91.3\tpolarity:pos\n",
      "metricName:Recall\trank_1:93.85\trank_2:91.79\tpolarity:pos\n",
      "metricName:MSE\trank_1:0.2532\trank_2:0.2687\tpolarity:neg\n",
      "metricName:Pearson Correlation\trank_1:0.8676\trank_2:0.8584\tpolarity:pos\n",
      "metricName:Spearman Correlation\trank_1:0.8083\trank_2:0.7916\tpolarity:pos\n",
      "metricName:F1\trank_1:89.75\trank_2:89.3\tpolarity:pos\n",
      "metricName:Precision\trank_1:88.93\trank_2:87.99\tpolarity:pos\n",
      "metricName:Recall\trank_1:90.76\trank_2:90.78\tpolarity:neg\n",
      "metricName:Chamfer (cm)\trank_1:1.41\trank_2:1.55\tpolarity:neg\n",
      "metricName:Point-to-surface distance (cm)\trank_1:1.44\trank_2:1.66\tpolarity:neg\n",
      "metricName:Surface normal consistency\trank_1:0.111\trank_2:0.117\tpolarity:neg\n",
      "metricName:Chamfer (cm)\trank_1:1.73\trank_2:1.81\tpolarity:neg\n",
      "metricName:Point-to-surface distance (cm)\trank_1:1.63\trank_2:1.88\tpolarity:neg\n",
      "metricName:Surface normal consistency\trank_1:0.133\trank_2:0.147\tpolarity:neg\n",
      "metricName:avg. log MAE\trank_1:3.241\trank_2:3.453\tpolarity:neg\n",
      "metricName:AP Hard\trank_1:6.76\trank_2:4.82\tpolarity:pos\n",
      "metricName:AP Medium\trank_1:13.41\trank_2:13.17\tpolarity:pos\n",
      "metricName:AP at 10' Elevation error\trank_1:0.8584\trank_2:0.6658\tpolarity:pos\n",
      "metricName:AP at 15' Azimuth error\trank_1:0.7844\trank_2:0.5088\tpolarity:pos\n",
      "metricName:Average Precision at 0.5 3D IoU\trank_1:0.6512\trank_2:0.4624\tpolarity:pos\n",
      "metricName:MPE\trank_1:0.0467\trank_2:0.1001\tpolarity:neg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:AP@0.15\trank_1:44.91\trank_2:39.09\tpolarity:pos\n",
      "metricName:AP Hard\trank_1:11.46\trank_2:9.94\tpolarity:pos\n",
      "metricName:MRR\trank_1:0.493\trank_2:0.402\tpolarity:pos\n",
      "metricName:Hits@1\trank_1:0.289\trank_2:0.222\tpolarity:pos\n",
      "metricName:Avg\trank_1:81.7\trank_2:81.6\tpolarity:pos\n",
      "metricName:Question Answering\trank_1:71.9\trank_2:72.5\tpolarity:neg\n",
      "metricName:Sentence Retrieval\trank_1:90.8\trank_2:93.7\tpolarity:neg\n",
      "metricName:Sentence-pair Classification\trank_1:88.3\trank_2:88.4\tpolarity:neg\n",
      "metricName:Structured Prediction\trank_1:80.6\trank_2:76.2\tpolarity:pos\n",
      "metricName:F1\trank_1:75.33\trank_2:74.97\tpolarity:pos\n",
      "metricName:F1\trank_1:77.89\trank_2:76.42\tpolarity:pos\n",
      "metricName:F1\trank_1:69.2\trank_2:68.9\tpolarity:pos\n",
      "metricName:F1\trank_1:55.3\trank_2:50.89\tpolarity:pos\n",
      "metricName:F1\trank_1:79.31\trank_2:78\tpolarity:pos\n",
      "metricName:F1\trank_1:83.35\trank_2:82.9\tpolarity:pos\n",
      "metricName:F-Measure (Seen)\trank_1:67.2\trank_2:60.5\tpolarity:pos\n",
      "metricName:F-Measure (Unseen)\trank_1:51\trank_2:60.7\tpolarity:neg\n",
      "metricName:Jaccard (Seen)\trank_1:63.6\trank_2:59.8\tpolarity:pos\n",
      "metricName:Jaccard (Unseen)\trank_1:45.5\trank_2:54.2\tpolarity:neg\n",
      "metricName:Accuracy (%)\trank_1:92.5\trank_2:84.59\tpolarity:pos\n",
      "metricName:AP\trank_1:99.6\trank_2:99.5\tpolarity:pos\n",
      "metricName:TTA\trank_1:4.87\trank_2:4.74\tpolarity:pos\n",
      "metricName:ACC@1-50Clients\trank_1:81.89\trank_2:72.03\tpolarity:pos\n",
      "metricName:ACC@1-50Clients\trank_1:60.17\trank_2:59.46\tpolarity:pos\n",
      "metricName:ACC@1-100Clients\trank_1:52.40\trank_2:53.24\tpolarity:neg\n",
      "metricName:ACC@1-10Clients\trank_1:68.15\trank_2:65.74\tpolarity:pos\n",
      "metricName:ACC@1-50Clients\trank_1:90.08\trank_2:88.38\tpolarity:pos\n",
      "metricName:ACC@1-100Clients\trank_1:88.09\trank_2:87.97\tpolarity:pos\n",
      "metricName:ACC@1-10Clients\trank_1:92.47\trank_2:90.83\tpolarity:pos\n",
      "metricName:Average Precision\trank_1:63.86\trank_2:63.39\tpolarity:pos\n",
      "metricName:Average Recall\trank_1:66.24\trank_2:63.26\tpolarity:pos\n",
      "metricName:Error Rate\trank_1:50.9\trank_2:52.67\tpolarity:neg\n",
      "metricName:Error Rate\trank_1:11.84\trank_2:11.84\tpolarity:pos\n",
      "metricName:Error Rate\trank_1:36.59\trank_2:38.3\tpolarity:neg\n",
      "metricName:Per-Class Accuracy\trank_1:56.8\trank_2:55.4\tpolarity:pos\n",
      "metricName:Per-Class Accuracy\trank_1:38.8\trank_2:38.1\tpolarity:pos\n",
      "metricName:Error Rate\trank_1:20.37\trank_2:22.97\tpolarity:neg\n",
      "metricName:Long-Tailed Accuracy\trank_1:38.5\trank_2:40.4\tpolarity:neg\n",
      "metricName:Per-Class Accuracy\trank_1:36.1\trank_2:34.8\tpolarity:pos\n",
      "metricName:Long-Tailed Accuracy\trank_1:92.2\trank_2:94.1\tpolarity:neg\n",
      "metricName:Per-Class Accuracy\trank_1:76.2\trank_2:74.1\tpolarity:pos\n",
      "metricName:Long-Tailed Accuracy\trank_1:66.5\trank_2:67.7\tpolarity:neg\n",
      "metricName:Per-Class Accuracy\trank_1:60.1\trank_2:57.8\tpolarity:pos\n",
      "metricName:Per-Class Accuracy\trank_1:53.5\trank_2:51.2\tpolarity:pos\n",
      "metricName:Average-AP\trank_1:41.8\trank_2:24.3\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:103.22\trank_2:100.82\tpolarity:pos\n",
      "metricName:METEOR\trank_1:21.85\trank_2:21.32\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:45.65\trank_2:46.11\tpolarity:neg\n",
      "metricName:SPICE\trank_1:14.64\trank_2:13.81\tpolarity:pos\n",
      "metricName:B4\trank_1:21.86\trank_2:22.93\tpolarity:neg\n",
      "metricName:CIDEr\trank_1:131.2\trank_2:125.2\tpolarity:pos\n",
      "metricName:BLEU-4\trank_1:41.7\trank_2:39.5\tpolarity:pos\n",
      "metricName:METEOR\trank_1:30.6\trank_2:29.3\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:97.99\trank_2:89.03\tpolarity:pos\n",
      "metricName:METEOR\trank_1:29.51\trank_2:27.85\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:58.54\trank_2:56.13\tpolarity:pos\n",
      "metricName:SPICE\trank_1:13.63\trank_2:12.95\tpolarity:pos\n",
      "metricName:B4\trank_1:30.62\trank_2:26.48\tpolarity:pos\n",
      "metricName:B1\trank_1:83.24\trank_2:80.84\tpolarity:pos\n",
      "metricName:B2\trank_1:68.04\trank_2:64.35\tpolarity:pos\n",
      "metricName:B3\trank_1:49.68\trank_2:44.47\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:95.16\trank_2:87.79\tpolarity:pos\n",
      "metricName:METEOR\trank_1:28.24\trank_2:26.82\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:57.95\trank_2:55.27\tpolarity:pos\n",
      "metricName:SPICE\trank_1:13.36\trank_2:12.58\tpolarity:pos\n",
      "metricName:B4\trank_1:27.97\trank_2:23.52\tpolarity:pos\n",
      "metricName:B1\trank_1:82.77\trank_2:80.35\tpolarity:pos\n",
      "metricName:B2\trank_1:66.94\trank_2:62.87\tpolarity:pos\n",
      "metricName:B3\trank_1:47.02\trank_2:42.13\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:91.62\trank_2:87.15\tpolarity:pos\n",
      "metricName:METEOR\trank_1:26.83\trank_2:24.01\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:51.5\trank_2:51.75\tpolarity:neg\n",
      "metricName:SPICE\trank_1:14.21\trank_2:11.43\tpolarity:pos\n",
      "metricName:B4\trank_1:16.6\trank_2:17.96\tpolarity:neg\n",
      "metricName:B1\trank_1:74.84\trank_2:75.71\tpolarity:neg\n",
      "metricName:B2\trank_1:53.9\trank_2:56.39\tpolarity:neg\n",
      "metricName:B3\trank_1:33.51\trank_2:35.94\tpolarity:neg\n",
      "metricName:CIDEr\trank_1:92.46\trank_2:87.34\tpolarity:pos\n",
      "metricName:METEOR\trank_1:27.57\trank_2:26.29\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:56.96\trank_2:55.03\tpolarity:pos\n",
      "metricName:SPICE\trank_1:13.07\trank_2:12.01\tpolarity:pos\n",
      "metricName:B4\trank_1:26.15\trank_2:24.62\tpolarity:pos\n",
      "metricName:B1\trank_1:81.59\trank_2:79.0\tpolarity:pos\n",
      "metricName:B2\trank_1:65.15\trank_2:61.95\tpolarity:pos\n",
      "metricName:B3\trank_1:45.04\trank_2:42.36\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:101.2\trank_2:85.73\tpolarity:pos\n",
      "metricName:METEOR\trank_1:30.0\trank_2:26.37\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:58.76\trank_2:55.13\tpolarity:pos\n",
      "metricName:SPICE\trank_1:14.27\trank_2:11.96\tpolarity:pos\n",
      "metricName:B4\trank_1:30.21\trank_2:24.97\tpolarity:pos\n",
      "metricName:B1\trank_1:82.88\trank_2:79.51\tpolarity:pos\n",
      "metricName:B2\trank_1:67.01\trank_2:62.65\tpolarity:pos\n",
      "metricName:B3\trank_1:48.73\trank_2:43.22\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:95.5\trank_2:91.62\tpolarity:pos\n",
      "metricName:METEOR\trank_1:26.56\trank_2:26.83\tpolarity:neg\n",
      "metricName:ROUGE-L\trank_1:55.49\trank_2:51.5\tpolarity:pos\n",
      "metricName:SPICE\trank_1:12.66\trank_2:14.21\tpolarity:neg\n",
      "metricName:B4\trank_1:21.79\trank_2:16.6\tpolarity:pos\n",
      "metricName:B1\trank_1:79.44\trank_2:74.84\tpolarity:pos\n",
      "metricName:B2\trank_1:61.15\trank_2:53.9\tpolarity:pos\n",
      "metricName:B3\trank_1:41.03\trank_2:33.51\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:100.12\trank_2:85.34\tpolarity:pos\n",
      "metricName:METEOR\trank_1:29.47\trank_2:28.15\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:58.26\trank_2:52.83\tpolarity:pos\n",
      "metricName:SPICE\trank_1:14.04\trank_2:14.67\tpolarity:neg\n",
      "metricName:B4\trank_1:28.95\trank_2:19.48\tpolarity:pos\n",
      "metricName:B1\trank_1:82.27\trank_2:76.64\tpolarity:pos\n",
      "metricName:B2\trank_1:66.04\trank_2:56.46\tpolarity:pos\n",
      "metricName:B3\trank_1:47.48\trank_2:36.37\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:100.62\trank_2:82.86\tpolarity:pos\n",
      "metricName:METEOR\trank_1:30.62\trank_2:26.82\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:59.43\trank_2:55.37\tpolarity:pos\n",
      "metricName:SPICE\trank_1:14.7\trank_2:11.9\tpolarity:pos\n",
      "metricName:B4\trank_1:32.07\trank_2:25.67\tpolarity:pos\n",
      "metricName:B1\trank_1:82.94\trank_2:79.14\tpolarity:pos\n",
      "metricName:B2\trank_1:67.56\trank_2:62.18\tpolarity:pos\n",
      "metricName:B3\trank_1:49.66\trank_2:43.04\tpolarity:pos\n",
      "metricName:CIDEr\trank_1:80.67\trank_2:79.15\tpolarity:pos\n",
      "metricName:METEOR\trank_1:22.21\trank_2:22.12\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:50.11\trank_2:50.27\tpolarity:neg\n",
      "metricName:SPICE\trank_1:16.96\trank_2:17.9\tpolarity:neg\n",
      "metricName:B4\trank_1:27.35\trank_2:28.27\tpolarity:neg\n",
      "metricName:B1\trank_1:72.48\trank_2:72.22\tpolarity:pos\n",
      "metricName:B2\trank_1:53.93\trank_2:53.84\tpolarity:pos\n",
      "metricName:B3\trank_1:38.79\trank_2:39.28\tpolarity:neg\n",
      "metricName:CIDEr\trank_1:81.04\trank_2:77.7\tpolarity:pos\n",
      "metricName:METEOR\trank_1:22.25\trank_2:21.93\tpolarity:pos\n",
      "metricName:ROUGE-L\trank_1:50.2\trank_2:49.9\tpolarity:pos\n",
      "metricName:SPICE\trank_1:17.0\trank_2:17.79\tpolarity:neg\n",
      "metricName:B4\trank_1:27.44\trank_2:27.96\tpolarity:neg\n",
      "metricName:B1\trank_1:72.77\trank_2:71.97\tpolarity:pos\n",
      "metricName:B2\trank_1:54.17\trank_2:53.59\tpolarity:pos\n",
      "metricName:B3\trank_1:38.97\trank_2:38.94\tpolarity:pos\n",
      "metricName:F1\trank_1:77.06\trank_2:76.51\tpolarity:pos\n",
      "metricName:Neg. F1\trank_1:87.89\trank_2:88.74\tpolarity:neg\n",
      "metricName:Pos. F1\trank_1:66.23\trank_2:64.28\tpolarity:pos\n",
      "metricName:Mean PCK@0.2\trank_1:93.6\trank_2:92.1\tpolarity:pos\n",
      "metricName:PCK@0.2\trank_1:99.0\trank_2:97.59\tpolarity:pos\n",
      "metricName:PCKh-0.5\trank_1:94.1\trank_2:93.9\tpolarity:pos\n",
      "metricName:PCK\trank_1:99.5\trank_2:94.8\tpolarity:pos\n",
      "metricName:Mean PCK@0.2\trank_1:99.4\trank_2:99.3\tpolarity:pos\n",
      "metricName:mAP\trank_1:56.9\trank_2:56.5\tpolarity:pos\n",
      "metricName:Mean mAP\trank_1:83.44\trank_2:75.5\tpolarity:pos\n",
      "metricName:AP\trank_1:61.6\trank_2:55.8\tpolarity:pos\n",
      "metricName:PCKh@0.5\trank_1:92.1\trank_2:91.2\tpolarity:pos\n",
      "metricName:Mean mAP\trank_1:88.74\trank_2:84.9\tpolarity:pos\n",
      "metricName:AP\trank_1:79.5\trank_2:78.9\tpolarity:pos\n",
      "metricName:AP50\trank_1:93.6\trank_2:93.6\tpolarity:pos\n",
      "metricName:AP75\trank_1:85.9\trank_2:85.8\tpolarity:pos\n",
      "metricName:APL\trank_1:84.3\trank_2:83.6\tpolarity:pos\n",
      "metricName:APM\trank_1:76.3\trank_2:76.1\tpolarity:pos\n",
      "metricName:AR\trank_1:81.9\trank_2:81.4\tpolarity:pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metricName:Average PSNR\trank_1:34.80\trank_2:24.09\tpolarity:pos\n",
      "metricName:PSNR (sRGB)\trank_1:39.31\trank_2:38.65\tpolarity:pos\n",
      "metricName:SSIM (sRGB)\trank_1:0.972\trank_2:0.965\tpolarity:pos\n",
      "metricName:PSNR (sRGB)\trank_1:35.99\trank_2:35.70\tpolarity:pos\n",
      "metricName:SSIM (sRGB)\trank_1:0.952\trank_2:0.948\tpolarity:pos\n",
      "metricName:PSNR (sRGB)\trank_1:30.96\trank_2:29.98\tpolarity:pos\n",
      "metricName:SSIM (sRGB)\trank_1:0.939\trank_2:0.930\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.71\trank_2:32.66\tpolarity:pos\n",
      "metricName:PSNR\trank_1:32.34\trank_2:32.13\tpolarity:pos\n",
      "metricName:PSNR (sRGB)\trank_1:31.76\trank_2:31.38\tpolarity:pos\n",
      "metricName:SSIM (sRGB)\trank_1:0.922\trank_2:0.909\tpolarity:pos\n",
      "metricName:PSNR (sRGB)\trank_1:28.70\trank_2:28.70\tpolarity:pos\n",
      "metricName:SSIM (sRGB)\trank_1:0.873\trank_2:0.866\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:0.876\trank_2:0.831\tpolarity:pos\n",
      "metricName:Dice Score\trank_1:0.840\trank_2:0.814\tpolarity:pos\n",
      "metricName:Precision\trank_1:0.849\trank_2:0.848\tpolarity:pos\n",
      "metricName:Recall\trank_1:0.841\trank_2:0.806\tpolarity:pos\n",
      "metricName:Execution Accuracy\trank_1:43.8\trank_2:41.0\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:6.9\trank_2:6.4\tpolarity:pos\n",
      "metricName:Accuracy (5-fold)\trank_1:75.9\trank_2:75.5\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:58.9\trank_2:42.3\tpolarity:pos\n",
      "metricName:Accuracy\trank_1:64.1\trank_2:52.5\tpolarity:pos\n",
      "metricName:F1-Score\trank_1:0.89\trank_2:0.876\tpolarity:pos\n",
      "metricName:F1-Score\trank_1:0.764\trank_2:0.74\tpolarity:pos\n",
      "metricName:MAP\trank_1:40.97\trank_2:33.32\tpolarity:pos\n",
      "metricName:MRR\trank_1:60.93\trank_2:51.48\tpolarity:pos\n",
      "metricName:P@5\trank_1:41.31\trank_2:35.76\tpolarity:pos\n",
      "metricName:MAP\trank_1:34.05\trank_2:28.93\tpolarity:pos\n",
      "metricName:MRR\trank_1:54.64\trank_2:35.80\tpolarity:pos\n",
      "metricName:P@5\trank_1:36.77\trank_2:34.20\tpolarity:pos\n",
      "metricName:MAP\trank_1:19.78\trank_2:10.60\tpolarity:pos\n",
      "metricName:MRR\trank_1:36.10\trank_2:23.83\tpolarity:pos\n",
      "metricName:P@5\trank_1:19.03\trank_2:9.91\tpolarity:pos\n",
      "metricName:Exact Span F1\trank_1:97.3\trank_2:97.04\tpolarity:pos\n",
      "metricName:F1\trank_1:92.5\trank_2:92.0\tpolarity:pos\n",
      "metricName:F1\trank_1:95.0\trank_2:94.4\tpolarity:pos\n",
      "metricName:F1 score\trank_1:97.3\trank_2:96.72\tpolarity:pos\n",
      "metricName:Average F1\trank_1:73.2\trank_2:71.9\tpolarity:pos\n",
      "metricName:AUC\trank_1:0.846\trank_2:0.819\tpolarity:pos\n",
      "metricName:Decidability\trank_1:1.108\trank_2:0.986\tpolarity:pos\n",
      "metricName:EER\trank_1:0.216\trank_2:0.284\tpolarity:neg\n",
      "metricName:Weighted Average F1-score\trank_1:0.93\trank_2:0.90\tpolarity:pos\n",
      "metricName:Weighted Average F1-score\trank_1:0.60\trank_2:0.56\tpolarity:pos\n",
      "metricName:Weighted Average F1-score\trank_1:0.87\trank_2:0.73\tpolarity:pos\n",
      "metricName:mAP\trank_1:78.5\trank_2:64.0\tpolarity:pos\n",
      "metricName:FID-0\trank_1:27.1\trank_2:27.5\tpolarity:neg\n",
      "metricName:FID-1\trank_1:19.4\trank_2:28.0\tpolarity:neg\n",
      "metricName:FID-2\trank_1:13.9\trank_2:45.5\tpolarity:neg\n",
      "metricName:FID-4\trank_1:19.4\trank_2:83.5\tpolarity:neg\n",
      "metricName:FID-8\trank_1:23.6\trank_2:85.0\tpolarity:neg\n",
      "metricName:IS\trank_1:18.2\trank_2:17.9\tpolarity:pos\n"
     ]
    }
   ],
   "source": [
    "#calculate polarity (takes 1-2 min to run)\n",
    "#report df\n",
    "polarity_df = pd.DataFrame(columns=['task','datasets','metricName','rank_1', 'rank_2', 'polarity'])\n",
    "\n",
    "#iterate over tasks\n",
    "for task in df_metric_all[\"task\"].unique():\n",
    "    #iterate over datasets\n",
    "    for dataset in df_metric_all[df_metric_all[\"task\"]==task][\"datasets\"].unique():\n",
    "        #iterate over metrics\n",
    "        for metric in df_metric_all[df_metric_all[\"task\"]==task][\"metrics\"].unique():\n",
    "\n",
    "            #get ranks 1 and 2, compare and set polarity\n",
    "            try:\n",
    "                rank_1 = df_metric_all[(df_metric_all[\"task\"] == task) & (df_metric_all[\"datasets\"] == dataset) & (df_metric_all[\"metrics\"] == metric ) & (df_metric_all[\"ranking\"] == 1 )]\n",
    "                rank_2 = df_metric_all[(df_metric_all[\"task\"] == task) & (df_metric_all[\"datasets\"] == dataset) & (df_metric_all[\"metrics\"] == metric ) & (df_metric_all[\"ranking\"] == 2 )]\n",
    "                polarity = float(rank_1.value.iloc[0]) - float(rank_2.value.iloc[0])\n",
    "                if(polarity >=0):\n",
    "                    polarity = \"pos\"\n",
    "                else:\n",
    "                    polarity = \"neg\"\n",
    "\n",
    "                print(\"metricName:\"+metric + \"\\trank_1:\"+rank_1[\"value\"].iloc[0]+\"\\trank_2:\"+rank_2[\"value\"].iloc[0]+\"\\tpolarity:\"+polarity)\n",
    "                \n",
    "                #save to report df\n",
    "                polarity_df = polarity_df.append({'task':task,\n",
    "                                                  'datasets':dataset,\n",
    "                                                  'metricName':metric,\n",
    "                                                  'rank_1':rank_1[\"value\"].iloc[0],\n",
    "                                                  'rank_2':rank_2[\"value\"].iloc[0],\n",
    "                                                  'polarity':polarity},\n",
    "                                                  ignore_index=True)\n",
    "            #skip if metric is not present for such dataset    \n",
    "            except:\n",
    "                next\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ba0c3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>datasets</th>\n",
       "      <th>metricName</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trajectory Forecasting</td>\n",
       "      <td>TrajNet++</td>\n",
       "      <td>COL</td>\n",
       "      <td>5.31</td>\n",
       "      <td>6.560</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trajectory Forecasting</td>\n",
       "      <td>TrajNet++</td>\n",
       "      <td>FDE</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.150</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monocular Depth Estimation</td>\n",
       "      <td>KITTI Eigen split unsupervised</td>\n",
       "      <td>absolute relative error</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.087</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Monocular Depth Estimation</td>\n",
       "      <td>Make3D</td>\n",
       "      <td>Abs Rel</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.322</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monocular Depth Estimation</td>\n",
       "      <td>Make3D</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>8.388</td>\n",
       "      <td>7.417</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         task                        datasets  \\\n",
       "0      Trajectory Forecasting                       TrajNet++   \n",
       "1      Trajectory Forecasting                       TrajNet++   \n",
       "2  Monocular Depth Estimation  KITTI Eigen split unsupervised   \n",
       "3  Monocular Depth Estimation                          Make3D   \n",
       "4  Monocular Depth Estimation                          Make3D   \n",
       "\n",
       "                metricName rank_1 rank_2 polarity  \n",
       "0                      COL   5.31  6.560      neg  \n",
       "1                      FDE   1.14  1.150      neg  \n",
       "2  absolute relative error  0.079  0.087      neg  \n",
       "3                  Abs Rel  0.377  0.322      pos  \n",
       "4                     RMSE  8.388  7.417      pos  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the polarity data frame. (Results from above)\n",
    "polarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f89c7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>datasets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metricName</th>\n",
       "      <th>polarity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE (Subject-exposed)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>three pixel error</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th># of clusters (k)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>% Test Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0..5sec</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-1</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 sec</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10%</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-20% Mask PSNR</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14 gestures accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20-30% Mask PSNR</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28 gestures accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 sec</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-fold Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30 sec</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-40% Mask PSNR</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3DIoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3DPCK</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40-50% Mask PSNR</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5..20sec</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAA</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABX-across</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ACC@1-100Clients</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACC@1-10Clients</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACC@1-50Clients</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACER</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADD</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADDS AUC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">AED</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">AKD</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMT</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMrTRE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AOP</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <th>pos</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP 0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP Hard</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP Medium</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP at 10' Elevation error</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP at 15' Azimuth error</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP50</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP75</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP@0.15</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP@0.7</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APH/L2</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APL</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APM</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APc</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APf</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APr</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR50</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR75</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AR@100</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARI</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARL</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARM</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <th>pos</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC (ABPA)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC (Aspergillus)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC (Diabetes)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC (E. Coli)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC (I. Obstruction)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC (K. Pneumonia)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC (val)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC-J</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC-J&amp;F</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC-ROC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC0.08 private</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC@0.1 (all)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">AVERAGE MAE</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG-CDS</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Abs Rel</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Acc</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Accuracy (%)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (10-fold)</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (5-fold)</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (8 emotion)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (ADD)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (AV I)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (AV II)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (Body + Fingers + Face joints)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (Body + Fingers joints)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (Body joints)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (CS)</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (CV I)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (CV II)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (CV)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Accuracy (Cross-Setup)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (Cross-Subject)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (Cross-View)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (Cross-View, Avg)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (Inter-Patient)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (RGB+pose)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (TEST-DB)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (TRAIN-DB)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (Test)</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (easy)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (hard)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (median)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy (val)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy(on validation set)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Action@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actions Top-1 (S1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actions Top-1 (S2)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animals</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aspect</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average 3D Error</th>\n",
       "      <th>neg</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Accuracy (10 times)</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Class Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Average MAE</th>\n",
       "      <th>neg</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average MPJPE (mm)</th>\n",
       "      <th>neg</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Orientation Similarity</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Overlap</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average PSNR</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Per-Class Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Precision</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Precision at 0.5 3D IoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Recall</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Return (NoOp)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Success Rate</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average accuracy of 3 splits</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average-AP</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average-mAP</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg.</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">B1</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">B2</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">B3</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">B4</th>\n",
       "      <th>neg</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BG#1-2</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">BLEU</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU (EN-DE)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU score</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU-1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU-4</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Backpack</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced Error Rate</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bare MR^-2</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bits per dim</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CIDER</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CIDEr</th>\n",
       "      <th>pos</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CIS</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL#1-2</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMC1</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMC10</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMC5</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COL</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPU (sec)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CS</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CSIM</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chamfer (cm)</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chamfer Distance</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City level (25 km)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class Average IoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class IOU</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classification Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classification Error</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conn</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continent level (2500 km)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country level (750 km)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3R</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAC (K=6)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELETE</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DFID</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLD</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DSC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decidability</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dice</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dice (Average)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dice (SE)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dice Score</th>\n",
       "      <th>pos</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diversity</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Driving Score</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E-Measure</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EC</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EER</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EM</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM (Quasar-T)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMD</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EO</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Edit</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English-Wiki (open) F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entity F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equal Error Rate</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Error Rate</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error rate</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exact Match</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Exact Span F1</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Execution Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expected Average Overlap (EAO)</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-BC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-Measure</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F-Measure (Seen)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F-Measure (Unseen)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-Score</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-Score@1%</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-measure</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F-measure (Decay)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-measure (Mean)</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F-measure (Recall)</th>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F1</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (1dAVb)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (AF)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (LBBB)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (Quasar-T)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (RBBB)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (SB)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (ST)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (Sequence)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (Set)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (micro)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 (v2)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 - macro</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Full</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Newswire</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F1 score</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F1-score</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score (@IoU = 0.2)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score (@IoU = 0.3)</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F1-score (Augmented)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-score (Canonical)</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F1@10%</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">F1@25%</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1@50%</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1c (v2)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FDE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FED</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">FID</th>\n",
       "      <th>neg</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FID-0</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FID-1</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FID-10k-training-steps</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FID-2</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FID-4</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FID-50k</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FID-5k-training-steps</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FID-8</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR95</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPS</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPS on CPU</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR@0.1(%, all)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FSD</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FSIM</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FVD score</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F_NMI</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frame (fps)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frame-mAP</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frames Needed</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Full MRP F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Full UCCA F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fullset (public)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G-Score (BLEU, Accuracy)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @0.01% FAR Impersonation</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @0.01% FAR Obfuscation</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @0.01% FAR Overall</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @0.01% FAR Plastic Surgery</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @0.1% FAR</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @0.1% FAR Impersonation</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GAR @0.1% FAR Obfuscation</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GAR @0.1% FAR Overall</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @0.1% FAR Plastic Surgery</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @1% FAR</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @1% FAR Impersonation</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @1% FAR Obfuscation</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAR @1% FAR Overall</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GFLOPs</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grad Det-Jac</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">HR@20</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harmonic mean</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hat</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hausdorff</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hausdorff Distance (mm)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Heavy MR^-2</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hit@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Hit@20</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hits@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hits@10</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holder Binary F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Humans</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I. Obstruction</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDF1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">IS</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image-Level Recall</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image-to-text R@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image-to-text R@10</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Image-to-text R@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In-domain</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Inception Score</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inception score</th>\n",
       "      <th>pos</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instance Average IoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interpolation Error</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intra-FID</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IoU [256 distractors]</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IoU [32 distractors]</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IoU [4 distractors]</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IoU mean</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">IoU overall</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J&amp;F</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J&amp;F 1st frame</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J&amp;F@60s</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J@60s</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Jaccard (Decay)</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard (Mean)</th>\n",
       "      <th>pos</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Jaccard (Recall)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jaccard (Seen)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Jaccard (Unseen)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joint</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KEEP</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KILT-EM</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KILT-F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KILT-RL</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kappa</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kendall's Tau</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kernel Inception Distance</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">L1</th>\n",
       "      <th>neg</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 Loss (10^-4)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAS</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LCC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LCS</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLE</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LPIPS</th>\n",
       "      <th>neg</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPP MRP F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPP UCCA F1</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSE-C</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSE-D</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LT-ACC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Laptop (Acc)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Laptop (F1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Laptop 2014 (F1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Local</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log-Spectral Distance</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Long-Tailed Accuracy</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MAE</th>\n",
       "      <th>neg</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE (10% missing)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE (10% of data as GT)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE (Arousal)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE (Expectancy)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE (PM2.5)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE (Power)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE (Valence)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE (trained with other data)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MAE [bpm, session-wise]</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE for DBP [mmHg]</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE for SBP [mmHg]</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MAP</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAR, walking, 1,000ms</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAR, walking, 400ms</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MAX E-MEASURE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MAX F-MEASURE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ME (%, all)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">METEOR</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIoU (13 classes)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIoU (16 classes)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MJPE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MKR</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMD</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMrTRE</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOTA</th>\n",
       "      <th>pos</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOTP</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPJAE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MPJPE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPJPE (CA)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPJPE (CS)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPS</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPVPE</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MR (K=1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MR (K=6)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRPE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRR</th>\n",
       "      <th>pos</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRR (x 100)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRR@20</th>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MS-SSIM</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <th>neg</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE (10% missing)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE (10^-2, 50% missing)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE (10^2, 50% missing)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE stdev</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE(10^3)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro-F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro-F1 (20% training data)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro-F1 (60% training data)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro-F1 (80% training data)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean ADD</th>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean AP @ 0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Acc (Restaurant + Laptop)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Error Rate</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean F-measure</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean F1 (WSJ)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean IoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean IoU (class)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean NME</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean NME</th>\n",
       "      <th>neg</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean PCK</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean PCK@0.2</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Rank</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean Reconstruction Error (mm)</th>\n",
       "      <th>neg</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean mAP</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean target overlap ratio</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium Human-Normalized Score</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meteor (EN-DE)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro-F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro-F1 (20% training data)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Micro-F1 (80% training data)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiWOZ (Inform)</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiWOZ (Success)</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG (x 100)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDS</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NIQE</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NIST</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLDA</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NM#5-6</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NME</th>\n",
       "      <th>neg</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NSS</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg Jacob Det</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neg. F1</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NegLL</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized Pose Error</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized Precision</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized cPSNR</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noun@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Frames Per View</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of Views</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O (Average of Measures)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMQ</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OOB Rate (10^‚àí3)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORD</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORD</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Out-of-domain</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall IoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@10%</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@30%</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA-MPJPE</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PA-MPVPE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PARENT</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCK</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCK3D (CA)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCK3D (CS)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCK@0.1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCK@0.2</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCK@0.3</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCK@0.4</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCK@0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PCKh</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCKh-0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCKh@0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCP3D</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PESQ</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PO</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPV (VEB)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PQ</th>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PQst</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PQth</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR-AUC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PSNR</th>\n",
       "      <th>neg</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSNR (Raw)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSNR (sRGB)</th>\n",
       "      <th>pos</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">PSNR-B</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parameters (M)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial MR^-2</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patch Matching</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patch Retrieval</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patch Verification</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Path Difference</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Path Length</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pearson Correlation</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Per-Class Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Per-class Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Per-pixel Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage Error</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage correct</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage error</th>\n",
       "      <th>neg</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Permuted Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Player Distance</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Point-to-surface distance (cm)</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos. F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pr@0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pr@0.7</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pr@0.9</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Precision</th>\n",
       "      <th>neg</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision@0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision@0.6</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision@0.7</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision@0.8</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision@0.9</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision@20</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quality</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Question Answering</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">R-Prec</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">R@1</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@10</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@100</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@16</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@2</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@32</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@4</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R@8</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RC</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RCL</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">RMSE</th>\n",
       "      <th>neg</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE (Subject-na√Øve)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE log</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ROUGE-2</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ROUGE-L</th>\n",
       "      <th>neg</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-SU4</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RRSE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank-1</th>\n",
       "      <th>pos</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank-1 Recognition Rate</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank-10</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Rank-5</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reasonable MR^-2</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reasonable Miss Rate</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Recall</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall@10</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall@50</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reference images</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region level (200 km)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relation F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurant (Acc)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurant (F1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Restaurant 2014 (F1)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurant 2015 (F1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurant 2016 (F1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Route Completion</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Runtime(s)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-Measure</th>\n",
       "      <th>pos</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAD</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SARI</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SPICE</th>\n",
       "      <th>neg</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SQ Rel</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SSIM</th>\n",
       "      <th>neg</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSIM (Raw)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SSIM (sRGB)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th>pos</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Search time (s)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sensitivity (VEB)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence Retrieval</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence-pair Classification</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shen F-1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Size (MB)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smatch</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smoothed BLEU-4</th>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spearman Correlation</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Speed  (FPS)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Speed (FPS)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Speed(ms/f)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sq Rel</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Step Change (10^‚àí3)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Street level (1 km)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Structured Prediction</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Success Rate 0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surface normal consistency</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAR @ FAR=0.0001</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAR @ FAR=0.001</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAR @ FAR=0.01</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTA</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target Binary F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Temporal awareness</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test AP</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Error</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test perplexity</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text-to-image R@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text-to-image R@10</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text-to-image R@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time (ms)</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top 1 Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top 1 Error</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Top 5 Accuracy</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-1</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-1 (%)</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-1 Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-1 Error Rate</th>\n",
       "      <th>neg</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-1 Localization Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-1 accuracy %</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-10 Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-3</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-5 (%)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Top-5 Accuracy</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top-5 Error</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UA</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UAS</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCC</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCS</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unpermuted Accuracy</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User Study Score</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V-Measure</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VI</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Val</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation mIoU</th>\n",
       "      <th>pos</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Verb@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vid acc@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vid acc@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video hit@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video hit@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Video-mAP 0.5</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Average F1-score</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted F-Measure</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Macro-F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted-F1</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word Error Rate (WER)</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute relative error</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amota</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg-mAP (0.1-0.5)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg-mAP (0.1-0.9)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg-mAP (0.3-0.7)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg. log MAE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_fp_quality</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_label</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_pairwise</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_spatial</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bits/dimension</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpd</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brier-minFDE (K=6)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classification score</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free-form mask l2 err</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inference time (ms)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAAE</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAOE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">mAP</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP (@0.1, Through-wall)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP (@0.1, Visible)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP (All-search &amp; Single-shot)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP (Val)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP @0.5:0.95</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.2</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.3</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.4</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">mAP IOU@0.6</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">mAP IOU@0.7</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.75</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.8</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.9</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP IOU@0.95</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP(V2T)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.1:0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.1:0.7</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.25</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.3</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.4</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.5</th>\n",
       "      <th>pos</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.50</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.50 (CS)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@0.50 (CV)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAP@AVG(0.1:0.9)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mASE</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mATE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mAVE</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mIOU</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">mIoU</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mIoU (13 classes)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mIoU (KMeans)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mPC [AP50]</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mPC [AP]</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mPrec</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mRec</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mask AP</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mask-IS</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mask-SSIM</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">max E-Measure</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">max E-measure</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">max F-Measure</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean Corruption Error (mCE)</th>\n",
       "      <th>neg</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean E-Measure</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean Recall @20</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanIOU</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minADE (K=1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minADE (K=6)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minFDE (K=1)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minFDE (K=6)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse (10^-3)</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nDCG@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nats</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pose</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rPC [%]</th>\n",
       "      <th>pos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank-1</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank-10</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank-5</th>\n",
       "      <th>pos</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank1</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank1(V2T)</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rect mask l1 error</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rect mask l2 err</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sMOTSA</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spl</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tOF</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">text-to-video Mean Rank</th>\n",
       "      <th>neg</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">text-to-video Median Rank</th>\n",
       "      <th>neg</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-to-video R@1</th>\n",
       "      <th>pos</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-to-video R@10</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-to-video R@5</th>\n",
       "      <th>pos</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-to-video R@50</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation mean average precision</th>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video-to-text Median Rank</th>\n",
       "      <th>pos</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">video-to-text R@1</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">video-to-text R@10</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">video-to-text R@5</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Œ¥1.25</th>\n",
       "      <th>neg</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  datasets\n",
       "metricName                              polarity          \n",
       " RMSE (Subject-exposed)                 neg              1\n",
       " three pixel error                      neg              1\n",
       "# of clusters (k)                       pos              1\n",
       "% Test Accuracy                         pos              1\n",
       "0..5sec                                 neg              1\n",
       "1-1                                     pos              2\n",
       "10 sec                                  pos              1\n",
       "10%                                     pos              1\n",
       "10-20% Mask PSNR                        pos              1\n",
       "14 gestures accuracy                    pos              3\n",
       "20-30% Mask PSNR                        pos              1\n",
       "28 gestures accuracy                    pos              2\n",
       "3 sec                                   neg              1\n",
       "3-fold Accuracy                         pos              3\n",
       "30 sec                                  pos              1\n",
       "30-40% Mask PSNR                        pos              1\n",
       "3DIoU                                   pos              6\n",
       "3DPCK                                   pos              4\n",
       "40-50% Mask PSNR                        pos              1\n",
       "5..20sec                                neg              1\n",
       "AAA                                     pos              1\n",
       "ABX-across                              neg              1\n",
       "ACC                                     pos              1\n",
       "ACC@1-100Clients                        neg              1\n",
       "                                        pos              1\n",
       "ACC@1-10Clients                         pos              2\n",
       "ACC@1-50Clients                         pos              3\n",
       "ACER                                    neg              1\n",
       "ADD                                     pos              1\n",
       "ADDS AUC                                pos              1\n",
       "AED                                     neg              3\n",
       "                                        pos              1\n",
       "AKD                                     neg              2\n",
       "                                        pos              2\n",
       "AMT                                     pos              2\n",
       "AMrTRE                                  neg              1\n",
       "AOP                                     pos              1\n",
       "AP                                      pos             40\n",
       "AP 0.5                                  pos              4\n",
       "AP Hard                                 pos              2\n",
       "AP Medium                               pos              1\n",
       "AP at 10' Elevation error               pos              1\n",
       "AP at 15' Azimuth error                 pos              1\n",
       "AP50                                    pos              6\n",
       "AP75                                    pos              6\n",
       "AP@0.15                                 pos              1\n",
       "AP@0.7                                  pos              1\n",
       "APH/L2                                  pos              3\n",
       "APL                                     pos              3\n",
       "APM                                     pos              2\n",
       "APc                                     pos              2\n",
       "APf                                     pos              2\n",
       "APr                                     pos              2\n",
       "AR                                      pos              3\n",
       "AR50                                    pos              1\n",
       "AR75                                    pos              1\n",
       "AR@100                                  pos              1\n",
       "ARI                                     pos              1\n",
       "ARL                                     pos              1\n",
       "ARM                                     pos              1\n",
       "AUC                                     pos             15\n",
       "AUC (ABPA)                              pos              1\n",
       "AUC (Aspergillus)                       neg              1\n",
       "AUC (Diabetes)                          pos              1\n",
       "AUC (E. Coli)                           pos              1\n",
       "AUC (I. Obstruction)                    neg              1\n",
       "AUC (K. Pneumonia)                      pos              1\n",
       "AUC (val)                               pos              1\n",
       "AUC-J                                   pos              1\n",
       "AUC-J&F                                 pos              1\n",
       "AUC-ROC                                 pos              1\n",
       "AUC0.08 private                         pos              1\n",
       "AUC@0.1 (all)                           pos              1\n",
       "AUROC                                   pos              2\n",
       "AVERAGE MAE                             neg              3\n",
       "                                        pos              1\n",
       "AVG                                     pos              2\n",
       "AVG-CDS                                 pos              1\n",
       "Abs Rel                                 neg              1\n",
       "                                        pos              1\n",
       "Acc                                     neg              1\n",
       "                                        pos             10\n",
       "Accuracy                                pos            195\n",
       "Accuracy                                pos              4\n",
       "Accuracy (%)                            neg              1\n",
       "                                        pos             20\n",
       "Accuracy (10-fold)                      pos              3\n",
       "Accuracy (5-fold)                       pos              3\n",
       "Accuracy (8 emotion)                    pos              1\n",
       "Accuracy (ADD)                          pos              2\n",
       "Accuracy (AV I)                         pos              1\n",
       "Accuracy (AV II)                        pos              1\n",
       "Accuracy (Body + Fingers + Face joints) neg              1\n",
       "Accuracy (Body + Fingers joints)        pos              1\n",
       "Accuracy (Body joints)                  neg              1\n",
       "Accuracy (CS)                           pos              4\n",
       "Accuracy (CV I)                         pos              1\n",
       "Accuracy (CV II)                        pos              1\n",
       "Accuracy (CV)                           pos              1\n",
       "Accuracy (Cross-Setup)                  neg              1\n",
       "                                        pos              1\n",
       "Accuracy (Cross-Subject)                pos              1\n",
       "Accuracy (Cross-View)                   pos              1\n",
       "Accuracy (Cross-View, Avg)              pos              1\n",
       "Accuracy (Inter-Patient)                pos              1\n",
       "Accuracy (RGB+pose)                     pos              1\n",
       "Accuracy (TEST-DB)                      pos              1\n",
       "Accuracy (TRAIN-DB)                     pos              1\n",
       "Accuracy (Test)                         pos              3\n",
       "Accuracy (easy)                         pos              2\n",
       "Accuracy (hard)                         pos              2\n",
       "Accuracy (median)                       pos              2\n",
       "Accuracy (val)                          pos              1\n",
       "Accuracy(on validation set)             pos              1\n",
       "Action@1                                pos              1\n",
       "Actions Top-1 (S1)                      pos              1\n",
       "Actions Top-1 (S2)                      neg              1\n",
       "Animals                                 pos              1\n",
       "Aspect                                  pos              1\n",
       "Average                                 neg              2\n",
       "Average 3D Error                        neg              4\n",
       "Average Accuracy                        pos             11\n",
       "Average Accuracy (10 times)             pos              3\n",
       "Average Class Accuracy                  pos              1\n",
       "Average F1                              pos              6\n",
       "Average MAE                             neg             15\n",
       "                                        pos              2\n",
       "Average MPJPE (mm)                      neg              6\n",
       "Average Orientation Similarity          pos              1\n",
       "Average Overlap                         pos              1\n",
       "Average PSNR                            pos              3\n",
       "Average Per-Class Accuracy              pos              1\n",
       "Average Precision                       pos              1\n",
       "Average Precision at 0.5 3D IoU         pos              1\n",
       "Average Recall                          pos              3\n",
       "Average Return (NoOp)                   pos              1\n",
       "Average Success Rate                    pos              1\n",
       "Average accuracy of 3 splits            pos              2\n",
       "Average-AP                              pos              1\n",
       "Average-mAP                             pos              2\n",
       "Avg                                     pos              1\n",
       "Avg F1                                  pos              2\n",
       "Avg.                                    pos              1\n",
       "B1                                      neg              1\n",
       "                                        pos              9\n",
       "B2                                      neg              1\n",
       "                                        pos              9\n",
       "B3                                      neg              2\n",
       "                                        pos              8\n",
       "B4                                      neg              4\n",
       "                                        pos              7\n",
       "BG#1-2                                  pos              1\n",
       "BLEU                                    neg              2\n",
       "                                        pos             21\n",
       "BLEU (EN-DE)                            pos              1\n",
       "BLEU score                              pos              1\n",
       "BLEU-1                                  pos              1\n",
       "BLEU-4                                  pos              1\n",
       "Backpack                                pos              1\n",
       "Balanced Error Rate                     neg              3\n",
       "Bare MR^-2                              pos              1\n",
       "Bits per dim                            neg              1\n",
       "CC                                      pos              1\n",
       "CD                                      neg              1\n",
       "CIDER                                   pos              1\n",
       "CIDEr                                   pos             13\n",
       "CIS                                     pos              1\n",
       "CL#1-2                                  pos              1\n",
       "CMC1                                    pos              2\n",
       "CMC10                                   pos              2\n",
       "CMC5                                    pos              2\n",
       "COL                                     neg              1\n",
       "CPU (sec)                               pos              1\n",
       "CR                                      pos              1\n",
       "CS                                      pos              1\n",
       "CSIM                                    pos              1\n",
       "Chamfer (cm)                            neg              2\n",
       "Chamfer Distance                        neg              2\n",
       "City level (25 km)                      pos              2\n",
       "Class Average IoU                       pos              1\n",
       "Class IOU                               pos              2\n",
       "Classification Accuracy                 pos              3\n",
       "Classification Error                    neg              2\n",
       "Conn                                    neg              1\n",
       "Continent level (2500 km)               pos              2\n",
       "Country level (750 km)                  pos              2\n",
       "D3R                                     neg              2\n",
       "DAC (K=6)                               pos              1\n",
       "DELETE                                  pos              1\n",
       "DFID                                    neg              2\n",
       "DLD                                     pos              2\n",
       "DSC                                     pos              1\n",
       "Decidability                            pos              2\n",
       "Dice                                    pos              4\n",
       "Dice (Average)                          pos              1\n",
       "Dice (SE)                               pos              1\n",
       "Dice Score                              pos             12\n",
       "Diversity                               pos              2\n",
       "Driving Score                           pos              1\n",
       "E-Measure                               pos              2\n",
       "EC                                      neg              1\n",
       "EER                                     neg              2\n",
       "                                        pos              2\n",
       "EM                                      neg              1\n",
       "                                        pos              9\n",
       "EM (Quasar-T)                           pos              1\n",
       "EMD                                     neg              1\n",
       "EO                                      neg              1\n",
       "Edit                                    neg              2\n",
       "                                        pos              1\n",
       "English-Wiki (open) F1                  pos              1\n",
       "Entity F1                               pos              1\n",
       "Equal Error Rate                        neg              1\n",
       "Error Rate                              neg              3\n",
       "                                        pos              1\n",
       "Error rate                              neg              2\n",
       "Exact Match                             pos              2\n",
       "Exact Span F1                           neg              1\n",
       "                                        pos              1\n",
       "Execution Accuracy                      pos              1\n",
       "Expected Average Overlap (EAO)          pos              4\n",
       "F                                       pos              1\n",
       "F-BC                                    pos              1\n",
       "F-Measure                               pos              2\n",
       "F-Measure (Seen)                        neg              1\n",
       "                                        pos              2\n",
       "F-Measure (Unseen)                      neg              1\n",
       "                                        pos              2\n",
       "F-Score                                 pos              2\n",
       "F-Score@1%                              pos              1\n",
       "F-measure                               pos              2\n",
       "F-measure (Decay)                       neg              1\n",
       "                                        pos              4\n",
       "F-measure (Mean)                        pos              6\n",
       "F-measure (Recall)                      pos              5\n",
       "F0.5                                    pos              3\n",
       "F1                                      neg              1\n",
       "                                        pos             71\n",
       "F1 (1dAVb)                              pos              1\n",
       "F1 (AF)                                 pos              1\n",
       "F1 (LBBB)                               pos              1\n",
       "F1 (Quasar-T)                           pos              1\n",
       "F1 (RBBB)                               pos              1\n",
       "F1 (SB)                                 pos              1\n",
       "F1 (ST)                                 pos              1\n",
       "F1 (Sequence)                           pos              1\n",
       "F1 (Set)                                pos              1\n",
       "F1 (micro)                              pos              1\n",
       "F1 (v2)                                 pos              1\n",
       "F1 - macro                              pos              2\n",
       "F1 Full                                 pos              2\n",
       "F1 Newswire                             pos              2\n",
       "F1 Score                                pos              4\n",
       "F1 score                                neg              2\n",
       "                                        pos              9\n",
       "F1-Score                                pos              4\n",
       "F1-score                                neg              1\n",
       "                                        pos              5\n",
       "F1-score (@IoU = 0.2)                   pos              2\n",
       "F1-score (@IoU = 0.3)                   pos              3\n",
       "F1-score (Augmented)                    neg              1\n",
       "                                        pos              2\n",
       "F1-score (Canonical)                    pos              4\n",
       "F1@10%                                  neg              2\n",
       "                                        pos              1\n",
       "F1@25%                                  neg              1\n",
       "                                        pos              2\n",
       "F1@50%                                  pos              3\n",
       "F1c (v2)                                pos              1\n",
       "FDE                                     neg              1\n",
       "FED                                     neg              1\n",
       "FID                                     neg             65\n",
       "                                        pos              1\n",
       "FID-0                                   neg              1\n",
       "FID-1                                   neg              1\n",
       "FID-10k-training-steps                  neg              1\n",
       "FID-2                                   neg              1\n",
       "FID-4                                   neg              1\n",
       "FID-50k                                 neg              1\n",
       "FID-5k-training-steps                   neg              1\n",
       "FID-8                                   neg              1\n",
       "FPR95                                   neg              1\n",
       "FPS                                     pos              2\n",
       "FPS on CPU                              neg              1\n",
       "FR@0.1(%, all)                          pos              1\n",
       "FSD                                     neg              1\n",
       "FSIM                                    pos              3\n",
       "FVD score                               neg              1\n",
       "F_NMI                                   pos              1\n",
       "Frame (fps)                             neg              1\n",
       "Frame-mAP                               pos              2\n",
       "Frames Needed                           pos              1\n",
       "Full MRP F1                             pos              1\n",
       "Full UCCA F1                            pos              1\n",
       "Fullset (public)                        neg              1\n",
       "G-Score (BLEU, Accuracy)                pos              1\n",
       "GAR @0.01% FAR Impersonation            pos              1\n",
       "GAR @0.01% FAR Obfuscation              pos              1\n",
       "GAR @0.01% FAR Overall                  pos              1\n",
       "GAR @0.01% FAR Plastic Surgery          pos              1\n",
       "GAR @0.1% FAR                           pos              1\n",
       "GAR @0.1% FAR Impersonation             pos              2\n",
       "GAR @0.1% FAR Obfuscation               neg              1\n",
       "                                        pos              1\n",
       "GAR @0.1% FAR Overall                   neg              1\n",
       "                                        pos              1\n",
       "GAR @0.1% FAR Plastic Surgery           pos              1\n",
       "GAR @1% FAR                             pos              1\n",
       "GAR @1% FAR Impersonation               neg              1\n",
       "GAR @1% FAR Obfuscation                 neg              1\n",
       "GAR @1% FAR Overall                     neg              1\n",
       "GFLOPs                                  neg              1\n",
       "Gender                                  pos              1\n",
       "Grad                                    neg              1\n",
       "Grad Det-Jac                            neg              1\n",
       "HR@20                                   neg              1\n",
       "                                        pos              3\n",
       "Hamming Loss                            pos              1\n",
       "Harmonic mean                           pos              1\n",
       "Hat                                     pos              1\n",
       "Hausdorff                               neg              1\n",
       "Hausdorff Distance (mm)                 neg              1\n",
       "Heavy MR^-2                             neg              1\n",
       "Hit@1                                   pos              1\n",
       "Hit@20                                  neg              1\n",
       "                                        pos              1\n",
       "Hits@1                                  pos              1\n",
       "Hits@10                                 pos              1\n",
       "Holder Binary F1                        pos              1\n",
       "Humans                                  pos              1\n",
       "I. Obstruction                          neg              1\n",
       "IDF1                                    pos              1\n",
       "IS                                      neg              1\n",
       "                                        pos              5\n",
       "Image-Level Recall                      pos              1\n",
       "Image-to-text R@1                       pos              2\n",
       "Image-to-text R@10                      pos              2\n",
       "Image-to-text R@5                       pos              2\n",
       "In-domain                               pos              3\n",
       "Inception Score                         neg              1\n",
       "                                        pos             10\n",
       "Inception score                         pos             11\n",
       "Instance Average IoU                    pos              1\n",
       "Interpolation Error                     neg              1\n",
       "Intra-FID                               neg              1\n",
       "IoU                                     pos              3\n",
       "IoU [256 distractors]                   pos              1\n",
       "IoU [32 distractors]                    pos              1\n",
       "IoU [4 distractors]                     neg              1\n",
       "IoU mean                                pos              2\n",
       "IoU overall                             neg              1\n",
       "                                        pos              1\n",
       "J&F                                     pos              6\n",
       "J&F 1st frame                           pos              1\n",
       "J&F@60s                                 pos              1\n",
       "J@60s                                   pos              1\n",
       "Jaccard (Decay)                         neg              2\n",
       "                                        pos              3\n",
       "Jaccard (Mean)                          pos              8\n",
       "Jaccard (Recall)                        neg              1\n",
       "                                        pos              4\n",
       "Jaccard (Seen)                          pos              2\n",
       "Jaccard (Unseen)                        neg              1\n",
       "                                        pos              1\n",
       "Joint                                   pos              2\n",
       "KEEP                                    pos              1\n",
       "KILT-EM                                 pos              3\n",
       "KILT-F1                                 pos              4\n",
       "KILT-RL                                 pos              1\n",
       "Kappa                                   pos              2\n",
       "Kendall's Tau                           pos              1\n",
       "Kernel Inception Distance               neg              3\n",
       "L1                                      neg              5\n",
       "                                        pos              1\n",
       "L2 Loss (10^-4)                         neg              1\n",
       "LAS                                     pos              1\n",
       "LCC                                     pos              1\n",
       "LCS                                     neg              1\n",
       "LLE                                     pos              1\n",
       "LPIPS                                   neg              5\n",
       "                                        pos              3\n",
       "LPP MRP F1                              pos              1\n",
       "LPP UCCA F1                             neg              1\n",
       "LSE-C                                   neg              2\n",
       "LSE-D                                   pos              3\n",
       "LT-ACC                                  pos              1\n",
       "Laptop (Acc)                            neg              1\n",
       "Laptop (F1)                             pos              2\n",
       "Laptop 2014 (F1)                        pos              2\n",
       "Local                                   pos              1\n",
       "Log-Spectral Distance                   neg              3\n",
       "Long-Tailed Accuracy                    neg              3\n",
       "MAE                                     neg             22\n",
       "                                        pos              2\n",
       "MAE (10% missing)                       neg              1\n",
       "MAE (10% of data as GT)                 neg              1\n",
       "MAE (Arousal)                           pos              1\n",
       "MAE (Expectancy)                        pos              1\n",
       "MAE (PM2.5)                             neg              1\n",
       "MAE (Power)                             neg              1\n",
       "MAE (Valence)                           neg              1\n",
       "MAE (trained with other data)           neg              1\n",
       "MAE [bpm, session-wise]                 neg              1\n",
       "                                        pos              1\n",
       "MAE for DBP [mmHg]                      neg              1\n",
       "MAE for SBP [mmHg]                      neg              1\n",
       "MAP                                     neg              3\n",
       "                                        pos             22\n",
       "MAP                                     pos              1\n",
       "MAR, walking, 1,000ms                   neg              1\n",
       "MAR, walking, 400ms                     neg              1\n",
       "MAX E-MEASURE                           neg              1\n",
       "                                        pos              1\n",
       "MAX F-MEASURE                           neg              1\n",
       "                                        pos              1\n",
       "ME (%, all)                             pos              1\n",
       "METEOR                                  neg              1\n",
       "                                        pos             18\n",
       "MIoU (13 classes)                       neg              1\n",
       "MIoU (16 classes)                       pos              1\n",
       "MJPE                                    neg              1\n",
       "MKR                                     neg              2\n",
       "                                        pos              1\n",
       "MMD                                     neg              2\n",
       "MMrTRE                                  pos              1\n",
       "MOTA                                    pos              9\n",
       "MOTP                                    pos              1\n",
       "MPE                                     neg              1\n",
       "MPJAE                                   neg              1\n",
       "MPJPE                                   neg              1\n",
       "                                        pos              2\n",
       "MPJPE (CA)                              neg              1\n",
       "MPJPE (CS)                              neg              1\n",
       "MPS                                     pos              1\n",
       "MPVPE                                   pos              1\n",
       "MR (K=1)                                pos              1\n",
       "MR (K=6)                                neg              1\n",
       "MRPE                                    neg              1\n",
       "MRR                                     pos              9\n",
       "MRR (x 100)                             pos              1\n",
       "MRR@20                                  pos              7\n",
       "MS-SSIM                                 pos              4\n",
       "MSE                                     neg              8\n",
       "MSE (10% missing)                       neg              1\n",
       "MSE (10^-2, 50% missing)                neg              1\n",
       "MSE (10^2, 50% missing)                 neg              1\n",
       "MSE stdev                               pos              1\n",
       "MSE(10^3)                               neg              1\n",
       "Macro F1                                pos              5\n",
       "Macro Precision                         pos              1\n",
       "Macro Recall                            pos              1\n",
       "Macro-F1                                pos              1\n",
       "Macro-F1 (20% training data)            pos              1\n",
       "Macro-F1 (60% training data)            pos              1\n",
       "Macro-F1 (80% training data)            pos              1\n",
       "Mean                                    neg              1\n",
       "Mean ADD                                pos              5\n",
       "Mean AP @ 0.5                           pos              1\n",
       "Mean Acc (Restaurant + Laptop)          pos              1\n",
       "Mean Accuracy                           pos              1\n",
       "Mean Error Rate                         neg              2\n",
       "Mean F-measure                          pos              1\n",
       "Mean F1 (WSJ)                           pos              1\n",
       "Mean IoU                                pos             14\n",
       "Mean IoU (class)                        pos              1\n",
       "Mean NME                                neg              1\n",
       "Mean NME                                neg              5\n",
       "Mean PCK                                pos              1\n",
       "Mean PCK@0.2                            pos              2\n",
       "Mean Rank                               neg              3\n",
       "Mean Reconstruction Error (mm)          neg              4\n",
       "Mean mAP                                pos              4\n",
       "Mean target overlap ratio               pos              1\n",
       "Medium Human-Normalized Score           pos              1\n",
       "Meteor (EN-DE)                          neg              1\n",
       "Micro F1                                pos              2\n",
       "Micro Precision                         pos              1\n",
       "Micro Recall                            pos              1\n",
       "Micro-F1                                pos              4\n",
       "Micro-F1 (20% training data)            pos              1\n",
       "Micro-F1 (80% training data)            pos              1\n",
       "MultiWOZ (Inform)                       neg              2\n",
       "MultiWOZ (Success)                      neg              2\n",
       "NDCG (x 100)                            pos              1\n",
       "NDS                                     pos              1\n",
       "NIQE                                    neg              3\n",
       "NIST                                    pos              2\n",
       "NLDA                                    neg              1\n",
       "NM#5-6                                  pos              1\n",
       "NME                                     neg              5\n",
       "NSS                                     pos              2\n",
       "Neg Jacob Det                           pos              1\n",
       "Neg. F1                                 neg              1\n",
       "NegLL                                   neg              1\n",
       "Normalized Pose Error                   neg              1\n",
       "Normalized Precision                    pos              1\n",
       "Normalized cPSNR                        neg              1\n",
       "Noun@1                                  pos              1\n",
       "Number of Frames Per View               pos              1\n",
       "Number of Views                         pos              1\n",
       "O (Average of Measures)                 pos              1\n",
       "OMQ                                     pos              1\n",
       "OOB Rate (10^‚àí3)                        neg              1\n",
       "ORD                                     neg              1\n",
       "ORD                                     neg              1\n",
       "Out-of-domain                           pos              3\n",
       "Overall                                 pos              1\n",
       "Overall Accuracy                        pos              5\n",
       "Overall IoU                             pos              6\n",
       "P                                       pos              1\n",
       "P@10%                                   pos              1\n",
       "P@30%                                   pos              1\n",
       "P@5                                     pos              4\n",
       "PA-MPJPE                                neg              2\n",
       "PA-MPVPE                                neg              1\n",
       "PARENT                                  pos              1\n",
       "PC                                      neg              1\n",
       "PCK                                     pos              4\n",
       "PCK3D (CA)                              pos              1\n",
       "PCK3D (CS)                              pos              1\n",
       "PCK@0.1                                 pos              1\n",
       "PCK@0.2                                 pos              2\n",
       "PCK@0.3                                 pos              2\n",
       "PCK@0.4                                 pos              1\n",
       "PCK@0.5                                 pos              1\n",
       "PCKh                                    neg              1\n",
       "                                        pos              1\n",
       "PCKh-0.5                                pos              1\n",
       "PCKh@0.5                                pos              1\n",
       "PCP3D                                   pos              2\n",
       "PESQ                                    pos              1\n",
       "PO                                      neg              1\n",
       "PPV (VEB)                               pos              1\n",
       "PQ                                      pos              7\n",
       "PQst                                    pos              3\n",
       "PQth                                    neg              1\n",
       "                                        pos              2\n",
       "PR-AUC                                  pos              1\n",
       "PRC                                     pos              1\n",
       "PSNR                                    neg              6\n",
       "                                        pos            108\n",
       "PSNR (Raw)                              pos              1\n",
       "PSNR (sRGB)                             pos              8\n",
       "PSNR-B                                  neg              1\n",
       "                                        pos              6\n",
       "Parameters (M)                          neg              1\n",
       "Partial MR^-2                           neg              1\n",
       "Patch Matching                          pos              1\n",
       "Patch Retrieval                         pos              1\n",
       "Patch Verification                      neg              1\n",
       "Path Difference                         pos              1\n",
       "Path Length                             neg              1\n",
       "Pearson Correlation                     pos              3\n",
       "Per-Class Accuracy                      pos              6\n",
       "Per-class Accuracy                      pos              3\n",
       "Per-pixel Accuracy                      pos              3\n",
       "Percentage Error                        neg              1\n",
       "Percentage correct                      pos              1\n",
       "Percentage error                        neg              8\n",
       "Permuted Accuracy                       pos              1\n",
       "Perplexity                              neg              2\n",
       "Player Distance                         pos              1\n",
       "Point-to-surface distance (cm)          neg              2\n",
       "Pos. F1                                 pos              1\n",
       "Pr@0.5                                  pos              1\n",
       "Pr@0.7                                  pos              1\n",
       "Pr@0.9                                  pos              1\n",
       "Precision                               neg              3\n",
       "                                        pos             11\n",
       "Precision@0.5                           pos              2\n",
       "Precision@0.6                           pos              2\n",
       "Precision@0.7                           pos              2\n",
       "Precision@0.8                           pos              2\n",
       "Precision@0.9                           pos              2\n",
       "Precision@20                            pos              1\n",
       "Quality                                 neg              2\n",
       "Question Answering                      neg              1\n",
       "R                                       pos              1\n",
       "R-Prec                                  neg              1\n",
       "                                        pos              3\n",
       "R@1                                     neg              1\n",
       "                                        pos              7\n",
       "R@10                                    pos              6\n",
       "R@100                                   pos              1\n",
       "R@16                                    pos              1\n",
       "R@2                                     pos              1\n",
       "R@32                                    pos              1\n",
       "R@4                                     pos              1\n",
       "R@5                                     pos              4\n",
       "R@8                                     pos              1\n",
       "RC                                      pos              2\n",
       "RCL                                     pos              1\n",
       "RMSE                                    neg              6\n",
       "                                        pos              1\n",
       "RMSE (Subject-na√Øve)                    neg              1\n",
       "RMSE log                                neg              1\n",
       "ROC AUC                                 pos              1\n",
       "ROC-AUC                                 pos              1\n",
       "ROUGE                                   neg              1\n",
       "ROUGE-1                                 pos              7\n",
       "ROUGE-2                                 neg              1\n",
       "                                        pos              5\n",
       "ROUGE-L                                 neg              4\n",
       "                                        pos             13\n",
       "ROUGE-SU4                               neg              1\n",
       "RRSE                                    neg              1\n",
       "Rank-1                                  pos             11\n",
       "Rank-1 Recognition Rate                 pos              3\n",
       "Rank-10                                 pos              3\n",
       "Rank-5                                  neg              1\n",
       "                                        pos              5\n",
       "Real                                    neg              1\n",
       "Reasonable MR^-2                        neg              1\n",
       "Reasonable Miss Rate                    neg              1\n",
       "Recall                                  neg              2\n",
       "                                        pos              8\n",
       "Recall@10                               pos              1\n",
       "Recall@5                                pos              4\n",
       "Recall@50                               pos              2\n",
       "Reference images                        pos              1\n",
       "Region level (200 km)                   pos              2\n",
       "Relation F1                             pos              1\n",
       "Restaurant (Acc)                        pos              2\n",
       "Restaurant (F1)                         pos              3\n",
       "Restaurant 2014 (F1)                    neg              1\n",
       "                                        pos              1\n",
       "Restaurant 2015 (F1)                    pos              2\n",
       "Restaurant 2016 (F1)                    pos              1\n",
       "Route Completion                        pos              1\n",
       "Runtime(s)                              pos              1\n",
       "S-Measure                               pos             25\n",
       "SAD                                     neg              1\n",
       "SARI                                    pos              1\n",
       "SPICE                                   neg              4\n",
       "                                        pos              7\n",
       "SQ Rel                                  neg              1\n",
       "SSIM                                    neg              7\n",
       "                                        pos             56\n",
       "SSIM (Raw)                              pos              1\n",
       "SSIM (sRGB)                             neg              1\n",
       "                                        pos              7\n",
       "Score                                   pos             64\n",
       "Search time (s)                         pos              2\n",
       "Sensitivity                             pos              1\n",
       "Sensitivity (VEB)                       neg              1\n",
       "Sentence Retrieval                      neg              1\n",
       "Sentence-pair Classification            neg              1\n",
       "Sentiment                               neg              1\n",
       "Shen F-1                                pos              1\n",
       "Size (MB)                               neg              1\n",
       "Smatch                                  pos              3\n",
       "Smoothed BLEU-4                         pos              7\n",
       "Spearman Correlation                    pos              1\n",
       "Speed  (FPS)                            neg              1\n",
       "                                        pos              1\n",
       "Speed (FPS)                             pos              2\n",
       "Speed(ms/f)                             pos              1\n",
       "Sq Rel                                  pos              1\n",
       "Step Change (10^‚àí3)                     neg              1\n",
       "Street level (1 km)                     pos              2\n",
       "Structured Prediction                   pos              1\n",
       "Success Rate 0.5                        pos              1\n",
       "Surface normal consistency              neg              2\n",
       "TAR @ FAR=0.0001                        pos              1\n",
       "TAR @ FAR=0.001                         pos              3\n",
       "TAR @ FAR=0.01                          pos              4\n",
       "TC                                      pos              1\n",
       "TIoU                                    pos              3\n",
       "TTA                                     pos              1\n",
       "Target Binary F1                        pos              1\n",
       "Temporal awareness                      pos              1\n",
       "Test AP                                 pos              1\n",
       "Test Error                              neg              2\n",
       "Test perplexity                         neg              1\n",
       "Text-to-image R@1                       pos              2\n",
       "Text-to-image R@10                      pos              2\n",
       "Text-to-image R@5                       pos              2\n",
       "Time (ms)                               neg              2\n",
       "Top 1 Accuracy                          pos             12\n",
       "Top 1 Error                             neg              1\n",
       "Top 5 Accuracy                          neg              1\n",
       "                                        pos              3\n",
       "Top-1                                   pos              2\n",
       "Top-1 (%)                               pos              2\n",
       "Top-1 Accuracy                          pos              7\n",
       "Top-1 Error Rate                        neg              6\n",
       "Top-1 Localization Accuracy             pos              1\n",
       "Top-1 accuracy %                        pos              1\n",
       "Top-10 Accuracy                         pos              1\n",
       "Top-3                                   pos              2\n",
       "Top-5 (%)                               pos              1\n",
       "Top-5 Accuracy                          neg              1\n",
       "                                        pos              4\n",
       "Top-5 Error                             neg              1\n",
       "UA                                      pos              2\n",
       "UAS                                     pos              2\n",
       "UCC                                     pos              1\n",
       "UCS                                     pos              1\n",
       "Unpermuted Accuracy                     pos              1\n",
       "User Study Score                        pos              3\n",
       "V-Measure                               pos              1\n",
       "VI                                      pos              1\n",
       "Val                                     pos              1\n",
       "Validation mIoU                         pos             15\n",
       "Verb@1                                  pos              1\n",
       "Vid acc@1                               pos              1\n",
       "Vid acc@5                               pos              1\n",
       "Video hit@1                             pos              1\n",
       "Video hit@5                             pos              1\n",
       "Video-mAP 0.5                           neg              1\n",
       "                                        pos              1\n",
       "Weighted Average F1-score               pos              4\n",
       "Weighted F-Measure                      pos              2\n",
       "Weighted F1                             pos              3\n",
       "Weighted Macro-F1                       pos              1\n",
       "Weighted-F1                             pos              2\n",
       "Word Error Rate (WER)                   neg              2\n",
       "absolute relative error                 neg              2\n",
       "amota                                   pos              1\n",
       "avg-mAP (0.1-0.5)                       pos              1\n",
       "avg-mAP (0.1-0.9)                       pos              1\n",
       "avg-mAP (0.3-0.7)                       pos              1\n",
       "avg. log MAE                            neg              1\n",
       "avg_fp_quality                          pos              1\n",
       "avg_label                               pos              1\n",
       "avg_pairwise                            neg              1\n",
       "avg_spatial                             neg              1\n",
       "bits/dimension                          neg              1\n",
       "                                        pos              1\n",
       "bpd                                     neg              2\n",
       "brier-minFDE (K=6)                      neg              1\n",
       "classification score                    pos              1\n",
       "count                                   pos              2\n",
       "free-form mask l2 err                   neg              1\n",
       "inference time (ms)                     neg              1\n",
       "mAAE                                    pos              1\n",
       "mAOE                                    neg              1\n",
       "mAP                                     neg              2\n",
       "                                        pos             23\n",
       "mAP (@0.1, Through-wall)                pos              1\n",
       "mAP (@0.1, Visible)                     neg              1\n",
       "mAP (All-search & Single-shot)          pos              1\n",
       "mAP (Val)                               pos              1\n",
       "mAP @0.5:0.95                           pos              2\n",
       "mAP IOU@0.1                             pos              1\n",
       "mAP IOU@0.2                             pos              1\n",
       "mAP IOU@0.3                             pos              2\n",
       "mAP IOU@0.4                             pos              2\n",
       "mAP IOU@0.5                             pos              3\n",
       "mAP IOU@0.6                             neg              1\n",
       "                                        pos              1\n",
       "mAP IOU@0.7                             neg              1\n",
       "                                        pos              1\n",
       "mAP IOU@0.75                            neg              1\n",
       "mAP IOU@0.8                             neg              1\n",
       "mAP IOU@0.9                             neg              1\n",
       "mAP IOU@0.95                            pos              1\n",
       "mAP(V2T)                                pos              1\n",
       "mAP@0.1:0.5                             pos              1\n",
       "mAP@0.1:0.7                             pos              1\n",
       "mAP@0.25                                pos              3\n",
       "mAP@0.3                                 pos              1\n",
       "mAP@0.4                                 pos              1\n",
       "mAP@0.5                                 pos             10\n",
       "mAP@0.50                                pos              3\n",
       "mAP@0.50 (CS)                           pos              1\n",
       "mAP@0.50 (CV)                           pos              1\n",
       "mAP@AVG(0.1:0.9)                        pos              1\n",
       "mASE                                    pos              1\n",
       "mATE                                    neg              1\n",
       "mAVE                                    neg              1\n",
       "mIOU                                    pos              1\n",
       "mIoU                                    neg              1\n",
       "                                        pos             18\n",
       "mIoU (13 classes)                       pos              1\n",
       "mIoU (KMeans)                           pos              1\n",
       "mPC [AP50]                              pos              1\n",
       "mPC [AP]                                pos              2\n",
       "mPrec                                   pos              1\n",
       "mRec                                    neg              1\n",
       "mask AP                                 pos              1\n",
       "mask-IS                                 neg              1\n",
       "mask-SSIM                               neg              1\n",
       "max E-Measure                           neg              2\n",
       "                                        pos              5\n",
       "max E-measure                           neg              1\n",
       "                                        pos              5\n",
       "max F-Measure                           neg              1\n",
       "                                        pos              8\n",
       "mean Corruption Error (mCE)             neg              2\n",
       "mean E-Measure                          pos              2\n",
       "mean Recall @20                         neg              1\n",
       "meanIOU                                 pos              1\n",
       "minADE (K=1)                            pos              1\n",
       "minADE (K=6)                            neg              1\n",
       "minFDE (K=1)                            pos              1\n",
       "minFDE (K=6)                            pos              1\n",
       "mse (10^-3)                             neg              1\n",
       "nDCG@5                                  pos              1\n",
       "nats                                    neg              1\n",
       "pose                                    neg              1\n",
       "rPC [%]                                 pos              3\n",
       "rank-1                                  pos              4\n",
       "rank-10                                 pos              4\n",
       "rank-5                                  pos              4\n",
       "rank1                                   pos              1\n",
       "rank1(V2T)                              pos              1\n",
       "rect mask l1 error                      pos              1\n",
       "rect mask l2 err                        neg              1\n",
       "sMOTSA                                  pos              1\n",
       "spl                                     pos              1\n",
       "tOF                                     neg              1\n",
       "text-to-video Mean Rank                 neg              4\n",
       "                                        pos              1\n",
       "text-to-video Median Rank               neg              5\n",
       "                                        pos              2\n",
       "text-to-video R@1                       pos              7\n",
       "text-to-video R@10                      pos              6\n",
       "text-to-video R@5                       pos              6\n",
       "text-to-video R@50                      pos              1\n",
       "validation mean average precision       pos              1\n",
       "video-to-text Median Rank               pos              2\n",
       "video-to-text R@1                       neg              1\n",
       "                                        pos              1\n",
       "video-to-text R@10                      neg              1\n",
       "                                        pos              1\n",
       "video-to-text R@5                       neg              1\n",
       "                                        pos              1\n",
       "Œ¥1.25                                   neg              1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the summary of neg and pos counts for every metric:\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#summarise\n",
    "polarity_df_report = pd.DataFrame(polarity_df.groupby(['metricName', 'polarity'])['datasets'].count())\n",
    "#order\n",
    "polarity_df_report = polarity_df_report.sort_index(ascending=True)\n",
    "#display\n",
    "polarity_df_report\n",
    "#Note: this is basically the number of times every metric was positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c7c001f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['COL', 'FDE', 'absolute relative error', 'D3R', 'ORD', 'RMSE',\n",
       "       'Œ¥1.25', 'ORD ', 'Abs Rel', 'RMSE log', 'SQ Rel',\n",
       "       ' three pixel error', 'SSIM', 'NIQE', 'FID', 'PSNR', 'FED',\n",
       "       'LPIPS', 'Fullset (public)', 'Mean Error Rate', 'Mean NME ',\n",
       "       'Mean NME', 'Error rate', 'pose', 'NME',\n",
       "       'Mean Reconstruction Error (mm)', 'Equal Error Rate', 'MAE',\n",
       "       'NLDA', 'GAR @0.1% FAR Obfuscation', 'GAR @0.1% FAR Overall',\n",
       "       'GAR @1% FAR Impersonation', 'GAR @1% FAR Obfuscation',\n",
       "       'GAR @1% FAR Overall', 'Reasonable Miss Rate', 'Heavy MR^-2',\n",
       "       'Partial MR^-2', 'Reasonable MR^-2', 'LCS', 'MIoU (13 classes)',\n",
       "       'Quality', 'Kernel Inception Distance', 'Top 1 Error',\n",
       "       'mean Corruption Error (mCE)', 'Top-1 Error Rate',\n",
       "       'Word Error Rate (WER)', 'Percentage error', 'BLEU', 'Mean Rank',\n",
       "       'R@1', 'Mean', 'MultiWOZ (Inform)', 'MultiWOZ (Success)',\n",
       "       'Average MPJPE (mm)', 'MSE', 'MRPE', 'OOB Rate (10^‚àí3) ',\n",
       "       'Path Length', 'Step Change (10^‚àí3)', 'MAE (PM2.5)',\n",
       "       'MAE (10% missing)', 'MSE (10^2, 50% missing)', 'L2 Loss (10^-4)',\n",
       "       'MSE (10% missing)', 'MAE (10% of data as GT)', 'Meteor (EN-DE)',\n",
       "       'F1 score', 'Hausdorff', 'DFID', 'Classification Error',\n",
       "       'free-form mask l2 err', 'rect mask l2 err', 'Real', 'IS',\n",
       "       'mask-IS', 'mask-SSIM', 'PCKh', 'Inception Score',\n",
       "       'AUC (Aspergillus)', 'AUC (I. Obstruction)', 'I. Obstruction',\n",
       "       'mAP', 'Test Error', 'FVD score', 'text-to-video Mean Rank',\n",
       "       'text-to-video Median Rank', 'video-to-text R@1',\n",
       "       'video-to-text R@10', 'video-to-text R@5', 'tOF',\n",
       "       'Interpolation Error', 'F1-score (Augmented)', 'FPS on CPU',\n",
       "       'Top 5 Accuracy', 'mAP IOU@0.8', 'mAP IOU@0.9', 'EER',\n",
       "       'Top-5 Accuracy', 'Video-mAP 0.5', 'mAP IOU@0.7', 'mAP IOU@0.6',\n",
       "       'mAP IOU@0.75', 'R-Prec', 'EM', 'F1', 'L1',\n",
       "       'Accuracy (Cross-Setup)',\n",
       "       'Accuracy (Body + Fingers + Face joints)',\n",
       "       'Accuracy (Body joints)', 'MSE (10^-2, 50% missing)', 'NegLL',\n",
       "       'mse (10^-3)', 'RRSE', 'avg_pairwise', 'avg_spatial',\n",
       "       'mean Recall @20', 'Recall', 'ROUGE-2', 'ROUGE-SU4', 'ROUGE',\n",
       "       'Precision', 'Actions Top-1 (S2)', 'MMD', 'FSD', 'Average MAE',\n",
       "       '0..5sec', '5..20sec', 'Average', '3 sec', 'EC', 'EO', 'PC', 'PO',\n",
       "       ' RMSE (Subject-exposed)', 'RMSE (Subject-na√Øve)', 'Perplexity',\n",
       "       'F1-score', 'PQth', 'Time (ms)', 'IoU [4 distractors]',\n",
       "       'Top-5 Error', 'MAE (Power)', 'MAE (Valence)', 'Edit', 'F1@10%',\n",
       "       'Acc', 'F1@25%', 'Average 3D Error', 'LPP UCCA F1', 'MJPE',\n",
       "       'MPJPE', 'PA-MPJPE', 'MPJAE', 'MPJPE (CA)', 'MPJPE (CS)',\n",
       "       'MAE (trained with other data)', 'MAR, walking, 1,000ms',\n",
       "       'MAR, walking, 400ms', 'mAP (@0.1, Visible)', 'Sentiment',\n",
       "       'Laptop (Acc)', 'Restaurant 2014 (F1)', 'Exact Span F1',\n",
       "       'Sensitivity (VEB)', 'F-Measure (Seen)', 'Chamfer Distance',\n",
       "       'PSNR-B', 'Size (MB)', 'SSIM (sRGB)', 'ROUGE-L', 'LSE-C', 'Hit@20',\n",
       "       'HR@20', 'PA-MPVPE', 'Normalized Pose Error',\n",
       "       'inference time (ms)', 'mAOE', 'mATE', 'mAVE',\n",
       "       'Balanced Error Rate', 'MAP', 'max E-Measure', 'max F-Measure',\n",
       "       'Test perplexity', 'MAE [bpm, session-wise]', 'MAE for DBP [mmHg]',\n",
       "       'MAE for SBP [mmHg]', 'Log-Spectral Distance', 'Accuracy (%)',\n",
       "       'Grad Det-Jac', 'Hausdorff Distance (mm)', 'AMrTRE',\n",
       "       'Patch Verification', 'FPR95', 'ACER', 'CD', 'EMD', 'AED', 'AKD',\n",
       "       'MKR', 'GFLOPs', 'Percentage Error', 'Rank-5', 'MR (K=6)',\n",
       "       'brier-minFDE (K=6)', 'minADE (K=6)', 'mRec', 'IoU overall',\n",
       "       'Frame (fps)', 'Speed  (FPS)', 'Jaccard (Decay)',\n",
       "       'F-measure (Decay)', 'Jaccard (Recall)', 'AVERAGE MAE',\n",
       "       'max E-measure', 'MAX E-MEASURE', 'MAX F-MEASURE', 'Conn', 'Grad',\n",
       "       'MSE(10^3)', 'SAD', 'ABX-across', 'nats', 'Bits per dim',\n",
       "       'FID-5k-training-steps', 'bits/dimension', 'Intra-FID', 'bpd',\n",
       "       'FID-10k-training-steps', 'FID-50k', 'Parameters (M)', 'mIoU',\n",
       "       'Normalized cPSNR', 'Chamfer (cm)',\n",
       "       'Point-to-surface distance (cm)', 'Surface normal consistency',\n",
       "       'avg. log MAE', 'MPE', 'Question Answering', 'Sentence Retrieval',\n",
       "       'Sentence-pair Classification', 'F-Measure (Unseen)',\n",
       "       'Jaccard (Unseen)', 'ACC@1-100Clients', 'Error Rate',\n",
       "       'Long-Tailed Accuracy', 'B4', 'B1', 'B2', 'B3', 'METEOR', 'SPICE',\n",
       "       'Neg. F1', 'FID-0', 'FID-1', 'FID-2', 'FID-4', 'FID-8'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#metrics with negative polarity    \n",
    "polarity_df[polarity_df[\"polarity\"]==\"neg\"][\"metricName\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76af82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function\n",
    "def get_all_negative_metrics():\n",
    "    df_metric_all = pd.read_csv(\"df_metric_all.csv\")\n",
    "\n",
    "    df_metric_all = df_metric_all[df_metric_all[\"ranking\"]<=2]\n",
    "    df_metric_all = df_metric_all[df_metric_all[\"ranking\"]>0]\n",
    "    df_metric_all = df_metric_all[df_metric_all[\"metrics\"]!=\"No. parameters\"]\n",
    "\n",
    "    #calculate polarity \n",
    "    #report df\n",
    "    polarity_df = pd.DataFrame(columns=['task','datasets','metricName','rank_1', 'rank_2', 'polarity'])\n",
    "\n",
    "    #iterate over tasks\n",
    "    for task in df_metric_all[\"task\"].unique():\n",
    "        #iterate over datasets\n",
    "        for dataset in df_metric_all[df_metric_all[\"task\"]==task][\"datasets\"].unique():\n",
    "            #iterate over metrics\n",
    "            for metric in df_metric_all[df_metric_all[\"task\"]==task][\"metrics\"].unique():\n",
    "\n",
    "                #get ranks 1 and 2, compare and set polarity\n",
    "                try:\n",
    "                    rank_1 = df_metric_all[(df_metric_all[\"task\"] == task) & (df_metric_all[\"datasets\"] == dataset) & (df_metric_all[\"metrics\"] == metric ) & (df_metric_all[\"ranking\"] == 1 )]\n",
    "                    rank_2 = df_metric_all[(df_metric_all[\"task\"] == task) & (df_metric_all[\"datasets\"] == dataset) & (df_metric_all[\"metrics\"] == metric ) & (df_metric_all[\"ranking\"] == 2 )]\n",
    "                    polarity = float(rank_1.value.iloc[0]) - float(rank_2.value.iloc[0])\n",
    "                    if(polarity >=0):\n",
    "                        polarity = \"pos\"\n",
    "                    else:\n",
    "                        polarity = \"neg\"\n",
    "\n",
    "                    #print(\"metricName:\"+metric + \"\\trank_1:\"+rank_1[\"value\"].iloc[0]+\"\\trank_2:\"+rank_2[\"value\"].iloc[0]+\"\\tpolarity:\"+polarity)\n",
    "\n",
    "                    #save to report df\n",
    "                    polarity_df = polarity_df.append({'task':task,\n",
    "                                                      'datasets':dataset,\n",
    "                                                      'metricName':metric,\n",
    "                                                      'rank_1':rank_1[\"value\"].iloc[0],\n",
    "                                                      'rank_2':rank_2[\"value\"].iloc[0],\n",
    "                                                      'polarity':polarity},\n",
    "                                                      ignore_index=True)\n",
    "                #skip if metric is not present for such dataset    \n",
    "                except:\n",
    "                    next\n",
    "\n",
    "\n",
    "    metricName_neg = polarity_df[polarity_df[\"polarity\"]==\"neg\"][\"metricName\"].unique()\n",
    "    \n",
    "    return(metricName_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf9747f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['COL', 'FDE', 'absolute relative error', 'D3R', 'ORD', 'RMSE',\n",
       "       'Œ¥1.25', 'ORD ', 'Abs Rel', 'RMSE log', 'SQ Rel',\n",
       "       ' three pixel error', 'SSIM', 'NIQE', 'FID', 'PSNR', 'FED',\n",
       "       'LPIPS', 'Fullset (public)', 'Mean Error Rate', 'Mean NME ',\n",
       "       'Mean NME', 'Error rate', 'pose', 'NME',\n",
       "       'Mean Reconstruction Error (mm)', 'Equal Error Rate', 'MAE',\n",
       "       'NLDA', 'GAR @0.1% FAR Obfuscation', 'GAR @0.1% FAR Overall',\n",
       "       'GAR @1% FAR Impersonation', 'GAR @1% FAR Obfuscation',\n",
       "       'GAR @1% FAR Overall', 'Reasonable Miss Rate', 'Heavy MR^-2',\n",
       "       'Partial MR^-2', 'Reasonable MR^-2', 'LCS', 'MIoU (13 classes)',\n",
       "       'Quality', 'Kernel Inception Distance', 'Top 1 Error',\n",
       "       'mean Corruption Error (mCE)', 'Top-1 Error Rate',\n",
       "       'Word Error Rate (WER)', 'Percentage error', 'BLEU', 'Mean Rank',\n",
       "       'R@1', 'Mean', 'MultiWOZ (Inform)', 'MultiWOZ (Success)',\n",
       "       'Average MPJPE (mm)', 'MSE', 'MRPE', 'OOB Rate (10^‚àí3) ',\n",
       "       'Path Length', 'Step Change (10^‚àí3)', 'MAE (PM2.5)',\n",
       "       'MAE (10% missing)', 'MSE (10^2, 50% missing)', 'L2 Loss (10^-4)',\n",
       "       'MSE (10% missing)', 'MAE (10% of data as GT)', 'Meteor (EN-DE)',\n",
       "       'F1 score', 'Hausdorff', 'DFID', 'Classification Error',\n",
       "       'free-form mask l2 err', 'rect mask l2 err', 'Real', 'IS',\n",
       "       'mask-IS', 'mask-SSIM', 'PCKh', 'Inception Score',\n",
       "       'AUC (Aspergillus)', 'AUC (I. Obstruction)', 'I. Obstruction',\n",
       "       'mAP', 'Test Error', 'FVD score', 'text-to-video Mean Rank',\n",
       "       'text-to-video Median Rank', 'video-to-text R@1',\n",
       "       'video-to-text R@10', 'video-to-text R@5', 'tOF',\n",
       "       'Interpolation Error', 'F1-score (Augmented)', 'FPS on CPU',\n",
       "       'Top 5 Accuracy', 'mAP IOU@0.8', 'mAP IOU@0.9', 'EER',\n",
       "       'Top-5 Accuracy', 'Video-mAP 0.5', 'mAP IOU@0.7', 'mAP IOU@0.6',\n",
       "       'mAP IOU@0.75', 'R-Prec', 'EM', 'F1', 'L1',\n",
       "       'Accuracy (Cross-Setup)',\n",
       "       'Accuracy (Body + Fingers + Face joints)',\n",
       "       'Accuracy (Body joints)', 'MSE (10^-2, 50% missing)', 'NegLL',\n",
       "       'mse (10^-3)', 'RRSE', 'avg_pairwise', 'avg_spatial',\n",
       "       'mean Recall @20', 'Recall', 'ROUGE-2', 'ROUGE-SU4', 'ROUGE',\n",
       "       'Precision', 'Actions Top-1 (S2)', 'MMD', 'FSD', 'Average MAE',\n",
       "       '0..5sec', '5..20sec', 'Average', '3 sec', 'EC', 'EO', 'PC', 'PO',\n",
       "       ' RMSE (Subject-exposed)', 'RMSE (Subject-na√Øve)', 'Perplexity',\n",
       "       'F1-score', 'PQth', 'Time (ms)', 'IoU [4 distractors]',\n",
       "       'Top-5 Error', 'MAE (Power)', 'MAE (Valence)', 'Edit', 'F1@10%',\n",
       "       'Acc', 'F1@25%', 'Average 3D Error', 'LPP UCCA F1', 'MJPE',\n",
       "       'MPJPE', 'PA-MPJPE', 'MPJAE', 'MPJPE (CA)', 'MPJPE (CS)',\n",
       "       'MAE (trained with other data)', 'MAR, walking, 1,000ms',\n",
       "       'MAR, walking, 400ms', 'mAP (@0.1, Visible)', 'Sentiment',\n",
       "       'Laptop (Acc)', 'Restaurant 2014 (F1)', 'Exact Span F1',\n",
       "       'Sensitivity (VEB)', 'F-Measure (Seen)', 'Chamfer Distance',\n",
       "       'PSNR-B', 'Size (MB)', 'SSIM (sRGB)', 'ROUGE-L', 'LSE-C', 'Hit@20',\n",
       "       'HR@20', 'PA-MPVPE', 'Normalized Pose Error',\n",
       "       'inference time (ms)', 'mAOE', 'mATE', 'mAVE',\n",
       "       'Balanced Error Rate', 'MAP', 'max E-Measure', 'max F-Measure',\n",
       "       'Test perplexity', 'MAE [bpm, session-wise]', 'MAE for DBP [mmHg]',\n",
       "       'MAE for SBP [mmHg]', 'Log-Spectral Distance', 'Accuracy (%)',\n",
       "       'Grad Det-Jac', 'Hausdorff Distance (mm)', 'AMrTRE',\n",
       "       'Patch Verification', 'FPR95', 'ACER', 'CD', 'EMD', 'AED', 'AKD',\n",
       "       'MKR', 'GFLOPs', 'Percentage Error', 'Rank-5', 'MR (K=6)',\n",
       "       'brier-minFDE (K=6)', 'minADE (K=6)', 'mRec', 'IoU overall',\n",
       "       'Frame (fps)', 'Speed  (FPS)', 'Jaccard (Decay)',\n",
       "       'F-measure (Decay)', 'Jaccard (Recall)', 'AVERAGE MAE',\n",
       "       'max E-measure', 'MAX E-MEASURE', 'MAX F-MEASURE', 'Conn', 'Grad',\n",
       "       'MSE(10^3)', 'SAD', 'ABX-across', 'nats', 'Bits per dim',\n",
       "       'FID-5k-training-steps', 'bits/dimension', 'Intra-FID', 'bpd',\n",
       "       'FID-10k-training-steps', 'FID-50k', 'Parameters (M)', 'mIoU',\n",
       "       'Normalized cPSNR', 'Chamfer (cm)',\n",
       "       'Point-to-surface distance (cm)', 'Surface normal consistency',\n",
       "       'avg. log MAE', 'MPE', 'Question Answering', 'Sentence Retrieval',\n",
       "       'Sentence-pair Classification', 'F-Measure (Unseen)',\n",
       "       'Jaccard (Unseen)', 'ACC@1-100Clients', 'Error Rate',\n",
       "       'Long-Tailed Accuracy', 'B4', 'B1', 'B2', 'B3', 'METEOR', 'SPICE',\n",
       "       'Neg. F1', 'FID-0', 'FID-1', 'FID-2', 'FID-4', 'FID-8'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Show and export all the negative polarities.\n",
    "metricName_negative = polarity_df[polarity_df[\"polarity\"]==\"neg\"][\"metricName\"].unique()\n",
    "\n",
    "#metricName_negative.tofile('metricName_neg_pol.csv', sep=\",\")\n",
    "\n",
    "metricName_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659719c",
   "metadata": {},
   "source": [
    "Note: Some of the metrics display two polarities. This should be manually checked. For example see Top-5 Accuracy below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2034dc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>datasets</th>\n",
       "      <th>metricName</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Action Classification</td>\n",
       "      <td>Kinetics-600</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>97.3</td>\n",
       "      <td>96.5</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>Something-Something V2</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>92.7</td>\n",
       "      <td>92.70</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>Few-Shot Image Classification</td>\n",
       "      <td>ImageNet (1-shot)</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>58.2</td>\n",
       "      <td>59.2</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>Scene Graph Generation</td>\n",
       "      <td>3R-Scan</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.66</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>3D Object Classification</td>\n",
       "      <td>3R-Scan</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.68</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               task                datasets      metricName  \\\n",
       "547           Action Classification            Kinetics-600  Top-5 Accuracy   \n",
       "642              Action Recognition  Something-Something V2  Top-5 Accuracy   \n",
       "716   Few-Shot Image Classification       ImageNet (1-shot)  Top-5 Accuracy   \n",
       "862          Scene Graph Generation                 3R-Scan  Top-5 Accuracy   \n",
       "1874       3D Object Classification                 3R-Scan  Top-5 Accuracy   \n",
       "\n",
       "     rank_1 rank_2 polarity  \n",
       "547    97.3   96.5      pos  \n",
       "642    92.7  92.70      pos  \n",
       "716    58.2   59.2      neg  \n",
       "862    0.87   0.66      pos  \n",
       "1874    0.7   0.68      pos  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This example shows that on the polarity_df dataframe, the polarity of \"Top-5 Accuracy\" is wrong \n",
    "#on PWC.\n",
    "polarity_df[polarity_df[\"metricName\"]==\"Top-5 Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f708a183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>task</th>\n",
       "      <th>datasets</th>\n",
       "      <th>metrics</th>\n",
       "      <th>ranking</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Classification</td>\n",
       "      <td>Kinetics-600</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>97.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Classification</td>\n",
       "      <td>Kinetics-600</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>96.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Video Classification</td>\n",
       "      <td>Something-Something V2</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Video Classification</td>\n",
       "      <td>Something-Something V1</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>Something-Something V2</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>Something-Something V2</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>92.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>EgoGesture</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>99.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>Computer Vision', 'Methodology</td>\n",
       "      <td>Few-Shot Image Classification</td>\n",
       "      <td>ImageNet (1-shot)</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>58.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>Computer Vision', 'Methodology</td>\n",
       "      <td>Few-Shot Image Classification</td>\n",
       "      <td>ImageNet (1-shot)</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>59.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Scene Graph Generation</td>\n",
       "      <td>3R-Scan</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Scene Graph Generation</td>\n",
       "      <td>3R-Scan</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>Something-Something V2</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>Something-Something V2</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>92.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>EgoGesture</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>99.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5038</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>Something-Something V2</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5040</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>Something-Something V2</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>92.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5071</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Action Recognition</td>\n",
       "      <td>EgoGesture</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>99.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Few-Shot Image Classification</td>\n",
       "      <td>ImageNet (1-shot)</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>58.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5654</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>Few-Shot Image Classification</td>\n",
       "      <td>ImageNet (1-shot)</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>59.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6093</th>\n",
       "      <td>Robots', 'Computer Vision</td>\n",
       "      <td>3D Object Classification</td>\n",
       "      <td>3R-Scan</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6095</th>\n",
       "      <td>Robots', 'Computer Vision</td>\n",
       "      <td>3D Object Classification</td>\n",
       "      <td>3R-Scan</td>\n",
       "      <td>Top-5 Accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          categories                           task  \\\n",
       "1535                 Computer Vision          Action Classification   \n",
       "1537                 Computer Vision          Action Classification   \n",
       "1575                 Computer Vision           Video Classification   \n",
       "1576                 Computer Vision           Video Classification   \n",
       "1823                 Computer Vision             Action Recognition   \n",
       "1825                 Computer Vision             Action Recognition   \n",
       "1856                 Computer Vision             Action Recognition   \n",
       "2032  Computer Vision', 'Methodology  Few-Shot Image Classification   \n",
       "2033  Computer Vision', 'Methodology  Few-Shot Image Classification   \n",
       "2440                 Computer Vision         Scene Graph Generation   \n",
       "2441                 Computer Vision         Scene Graph Generation   \n",
       "2617                 Computer Vision             Action Recognition   \n",
       "2619                 Computer Vision             Action Recognition   \n",
       "2650                 Computer Vision             Action Recognition   \n",
       "5038                 Computer Vision             Action Recognition   \n",
       "5040                 Computer Vision             Action Recognition   \n",
       "5071                 Computer Vision             Action Recognition   \n",
       "5653                 Computer Vision  Few-Shot Image Classification   \n",
       "5654                 Computer Vision  Few-Shot Image Classification   \n",
       "6093       Robots', 'Computer Vision       3D Object Classification   \n",
       "6095       Robots', 'Computer Vision       3D Object Classification   \n",
       "\n",
       "                    datasets         metrics  ranking  value  \n",
       "1535            Kinetics-600  Top-5 Accuracy        1   97.3  \n",
       "1537            Kinetics-600  Top-5 Accuracy        2   96.5  \n",
       "1575  Something-Something V2  Top-5 Accuracy        1     91  \n",
       "1576  Something-Something V1  Top-5 Accuracy        1     84  \n",
       "1823  Something-Something V2  Top-5 Accuracy        1   92.7  \n",
       "1825  Something-Something V2  Top-5 Accuracy        2  92.70  \n",
       "1856              EgoGesture  Top-5 Accuracy        1   99.2  \n",
       "2032       ImageNet (1-shot)  Top-5 Accuracy        1   58.2  \n",
       "2033       ImageNet (1-shot)  Top-5 Accuracy        2   59.2  \n",
       "2440                 3R-Scan  Top-5 Accuracy        1   0.87  \n",
       "2441                 3R-Scan  Top-5 Accuracy        2   0.66  \n",
       "2617  Something-Something V2  Top-5 Accuracy        1   92.7  \n",
       "2619  Something-Something V2  Top-5 Accuracy        2  92.70  \n",
       "2650              EgoGesture  Top-5 Accuracy        1   99.2  \n",
       "5038  Something-Something V2  Top-5 Accuracy        1   92.7  \n",
       "5040  Something-Something V2  Top-5 Accuracy        2  92.70  \n",
       "5071              EgoGesture  Top-5 Accuracy        1   99.2  \n",
       "5653       ImageNet (1-shot)  Top-5 Accuracy        1   58.2  \n",
       "5654       ImageNet (1-shot)  Top-5 Accuracy        2   59.2  \n",
       "6093                 3R-Scan  Top-5 Accuracy        1    0.7  \n",
       "6095                 3R-Scan  Top-5 Accuracy        2   0.68  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This filter shows where this metric is used.\n",
    "df_metric_all[df_metric_all.metrics==\"Top-5 Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "031260d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many metrics are assigned to positive and negative polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b36835af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_df_report_2 = polarity_df_report.copy()\n",
    "\n",
    "polarity_df_report_2.reset_index(inplace=True)  \n",
    "\n",
    "polarity_df_report_2.head(5)\n",
    "\n",
    "metrics_polarity_counts =  pd.DataFrame(polarity_df_report_2.groupby(['metricName'])['polarity'].count())\n",
    "\n",
    "#count how many metrics have 2 different polarities\n",
    "len(metrics_polarity_counts[metrics_polarity_counts[\"polarity\"]==2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "565e1d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ACC@1-100Clients', 'AED', 'AKD', 'AVERAGE MAE', 'Abs Rel', 'Acc',\n",
       "       'Accuracy (%)', 'Accuracy (Cross-Setup)', 'Average MAE', 'B1', 'B2',\n",
       "       'B3', 'B4', 'BLEU', 'EER', 'EM', 'Edit', 'Error Rate', 'Exact Span F1',\n",
       "       'F-Measure (Seen)', 'F-Measure (Unseen)', 'F-measure (Decay)', 'F1',\n",
       "       'F1 score', 'F1-score', 'F1-score (Augmented)', 'F1@10%', 'F1@25%',\n",
       "       'FID', 'GAR @0.1% FAR Obfuscation', 'GAR @0.1% FAR Overall', 'HR@20',\n",
       "       'Hit@20', 'IS', 'Inception Score', 'IoU overall', 'Jaccard (Decay)',\n",
       "       'Jaccard (Recall)', 'Jaccard (Unseen)', 'L1', 'LPIPS', 'MAE',\n",
       "       'MAE [bpm, session-wise]', 'MAP', 'MAX E-MEASURE', 'MAX F-MEASURE',\n",
       "       'METEOR', 'MKR', 'MPJPE', 'PCKh', 'PQth', 'PSNR', 'PSNR-B', 'Precision',\n",
       "       'R-Prec', 'R@1', 'RMSE', 'ROUGE-2', 'ROUGE-L', 'Rank-5', 'Recall',\n",
       "       'Restaurant 2014 (F1)', 'SPICE', 'SSIM', 'SSIM (sRGB)', 'Speed  (FPS)',\n",
       "       'Top 5 Accuracy', 'Top-5 Accuracy', 'Video-mAP 0.5', 'bits/dimension',\n",
       "       'mAP', 'mAP IOU@0.6', 'mAP IOU@0.7', 'mIoU', 'max E-Measure',\n",
       "       'max E-measure', 'max F-Measure', 'text-to-video Mean Rank',\n",
       "       'text-to-video Median Rank', 'video-to-text R@1', 'video-to-text R@10',\n",
       "       'video-to-text R@5'],\n",
       "      dtype='object', name='metricName')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print these metrics\n",
    "metrics_polarity_counts[metrics_polarity_counts[\"polarity\"]==2].index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
